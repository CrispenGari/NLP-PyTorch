{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "\n",
        "**project**: `Symptoms to Disease Classifier (S2DC)`\n",
        "\n",
        "**date**: `2023-04-13`\n",
        "\n",
        "**decription`**: `In this notebook I'm going to create a simple AI classification model using NLP techniquies that will accurately identify the disease based on the description of the symptoms.`\n",
        "\n",
        "**main**: `Natural Language Processing (NLP) pytorch`\n",
        "\n",
        "**programmer**: `crispengari`\n",
        "\n",
        "**architecture**: `BiDirectional Long Short Term Memory [BiLSTM] (torchtext)`\n",
        "\n",
        "**language:** `python` \n",
        "\n",
        "____\n",
        "\n",
        "\n",
        "### Problem Statement\n",
        "\"Most people have certain symptoms and they fail to indentify wether they are sick or not. Sometimes others even ignore the symptoms because they lack knowledge. Using Deep Learning techniquies we want to implement an AI tool that will be able to identify the possible disease based on description of the symptoms by someone who's feeling sick.\"\n",
        "\n",
        "In this project I will create a simple classifiation model that will be able to classify the `disease` based on symptoms from text.\n",
        "\n",
        "### Data\n",
        "The dataset that we going to use in this notebook we be comming from [kaggle](https://www.kaggle.com/datasets/niyarrbarman/symptom2disease). \n",
        "\n",
        "### Model Architecture\n",
        "We are going to use `BiLSTM` in doing `multi-class` classifications of diseases based on the textual description that we are going to get from anyone using the model. We are going to use the following notebook as reference:\n",
        "\n",
        "> [10_MENTAL_HEALTH_CONVERSATION_BOT](https://github.com/CrispenGari/nlp-pytorch/blob/main/10_MENTAL_HEALTH_CONVERSATION_BOT/01_MENTAL_HEALTH_CONVERSATION_BOT.ipynb)\n",
        "\n",
        "### Installing Helper Packages\n",
        "In the following code cell we are going to install the package called `helperfns` that provide us with some usefull helper functions for machine learning."
      ],
      "metadata": {
        "id": "_e0eOlNyp0Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install helperfns -q"
      ],
      "metadata": {
        "id": "AZEmuYO1pSCx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports\n",
        "\n",
        "In  the following code cell we are going to import all the packages that we are going to use throughout this `notebook`"
      ],
      "metadata": {
        "id": "-P5QiqDJtK37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "import torchtext\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "from torchtext import data\n",
        "from collections import Counter\n",
        "from torchtext import vocab\n",
        "\n",
        "from helperfns.tables import tabulate_data\n",
        "from helperfns.visualization import plot_complicated_confusion_matrix, plot_simple_confusion_matrix\n",
        "from helperfns.torch import models\n",
        "from helperfns.utils import hms_string\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive, files\n",
        "\n",
        "torchtext.__version__, torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXoDyT2RpR-0",
        "outputId": "a036d8ab-d2a2-4086-cfec-04382b54c750"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('0.15.1+cpu', '2.0.0+cu118')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seed\n",
        "In the following code cell we are going to set the seed to all random operations for reproducivity."
      ],
      "metadata": {
        "id": "cVqGu8UDTkOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "4g7FAa9ETrDA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Device\n",
        "In the following code cell we are going to get `gpu` device if possible"
      ],
      "metadata": {
        "id": "fx6BtsYvTrtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4mXa6L0Tx4O",
        "outputId": "b82f33ca-28c0-4d75-bb8d-e7946ec9cb6f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "Our dataset that we are going to use will be comming from [`kaggle`](https://www.kaggle.com/datasets/niyarrbarman/symptom2disease) and will be loaded from google drive  where i uploaded it in a folder called `S2DC`. So in the following code cell we are going to mount our google drive to this colab instance."
      ],
      "metadata": {
        "id": "eDiHxRiMtxgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2XqK2PNlpR8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48046c62-9eb3-4db1-a8e4-06510a60b6fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Path to the dataset.\n",
        "Now we can define the path as a variable to the location where our dataset file called `Symptom2Disease.csv` is located."
      ],
      "metadata": {
        "id": "ciiieVigOGCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/drive/My Drive/NLP Data/S2DC\"\n",
        "\n",
        "assert os.path.exists(base_dir), f\"The path '{base_dir}' does not exists.\"\n",
        "\n",
        "data_path = os.path.join(base_dir, 'Symptom2Disease.csv')\n",
        "\n",
        "assert os.path.exists(data_path), f\"The path '{data_path}' does not exists.\""
      ],
      "metadata": {
        "id": "CEQ9QayBpR0L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we need to read the `Symptom2Disease.csv` file and create a classification dataset from it. For that we are going to use pandas and read the giant file into a dataframe as follows:"
      ],
      "metadata": {
        "id": "uhN_RheqQfGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pd.read_csv(data_path)\n",
        "dataframe.head(2)"
      ],
      "metadata": {
        "id": "TmEiCSKDpRxq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "83d3b204-62c2-4e5b-8d10-7bbf4d8c7713"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0      label                                               text\n",
              "0           0  Psoriasis  I have been experiencing a skin rash on my arm...\n",
              "1           1  Psoriasis  My skin has been peeling, especially on my kne..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ee73b88-aaac-400e-ae8d-164085f30aae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Psoriasis</td>\n",
              "      <td>I have been experiencing a skin rash on my arm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Psoriasis</td>\n",
              "      <td>My skin has been peeling, especially on my kne...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ee73b88-aaac-400e-ae8d-164085f30aae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1ee73b88-aaac-400e-ae8d-164085f30aae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1ee73b88-aaac-400e-ae8d-164085f30aae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this dataframe we only care about two field which are `label` and `text`. First let's check the classes or unique labels that we are going to have in this dataset."
      ],
      "metadata": {
        "id": "z2rGLv9-Q637"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diseases = dataframe.label.unique()\n",
        "diseases, len(diseases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cFZZssqyrGf",
        "outputId": "0f61f6b3-8c18-4530-d4ad-ed906c93b45d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['Psoriasis', 'Varicose Veins', 'Typhoid', 'Chicken pox',\n",
              "        'Impetigo', 'Dengue', 'Fungal infection', 'Common Cold',\n",
              "        'Pneumonia', 'Dimorphic Hemorrhoids', 'Arthritis', 'Acne',\n",
              "        'Bronchial Asthma', 'Hypertension', 'Migraine',\n",
              "        'Cervical spondylosis', 'Jaundice', 'Malaria',\n",
              "        'urinary tract infection', 'allergy',\n",
              "        'gastroesophageal reflux disease', 'drug reaction',\n",
              "        'peptic ulcer disease', 'diabetes'], dtype=object),\n",
              " 24)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we have `24` different kind of diseases that can be classified, based on the user text. In the following code cell we are going to extract `features` and `labels` from this giant dataframe so that we can create, different sets from it."
      ],
      "metadata": {
        "id": "VP9B9PHuyqri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels, features = [label.lower() for label in dataframe.label.values], [feature.lower() for feature in dataframe.text.values]\n",
        "\n",
        "labels[:2], features[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx-FLiuczwpN",
        "outputId": "877291b3-19fe-4583-b939-9dddd1ad64b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['psoriasis', 'psoriasis'],\n",
              " ['i have been experiencing a skin rash on my arms, legs, and torso for the past few weeks. it is red, itchy, and covered in dry, scaly patches.',\n",
              "  'my skin has been peeling, especially on my knees, elbows, and scalp. this peeling is often accompanied by a burning or stinging sensation.'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have converted all features and labels into list of values with common case we can create a giant dataset from this in the following code cell."
      ],
      "metadata": {
        "id": "WfW61qEW0fSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = list()\n",
        "for feature, label in zip(features, labels):\n",
        "   dataset.append((feature, label))\n",
        "print(\"Dataset size: {}\".format(len(dataset)))"
      ],
      "metadata": {
        "id": "OzuqHHnzpRuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b08c7d40-9525-45dc-85d4-bb9c527021fe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 1200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a small dataset that contains `1200` examples, let's check the first `10` examples in the dataset:"
      ],
      "metadata": {
        "id": "ZmXJtTEsTAA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[:10]"
      ],
      "metadata": {
        "id": "mYtE32C2pRr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c8bca9e-48bd-464d-a510-d09a8459dd98"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i have been experiencing a skin rash on my arms, legs, and torso for the past few weeks. it is red, itchy, and covered in dry, scaly patches.',\n",
              "  'psoriasis'),\n",
              " ('my skin has been peeling, especially on my knees, elbows, and scalp. this peeling is often accompanied by a burning or stinging sensation.',\n",
              "  'psoriasis'),\n",
              " ('i have been experiencing joint pain in my fingers, wrists, and knees. the pain is often achy and throbbing, and it gets worse when i move my joints.',\n",
              "  'psoriasis'),\n",
              " ('there is a silver like dusting on my skin, especially on my lower back and scalp. this dusting is made up of small scales that flake off easily when i scratch them.',\n",
              "  'psoriasis'),\n",
              " ('my nails have small dents or pits in them, and they often feel inflammatory and tender to the touch. even there are minor rashes on my arms.',\n",
              "  'psoriasis'),\n",
              " ('the skin on my palms and soles is thickened and has deep cracks. these cracks are painful and bleed easily.',\n",
              "  'psoriasis'),\n",
              " ('the skin around my mouth, nose, and eyes is red and inflamed. it is often itchy and uncomfortable. there is a noticeable inflammation in my nails.',\n",
              "  'psoriasis'),\n",
              " ('my skin is very sensitive and reacts easily to changes in temperature or humidity. i often have to be careful about what products i use on my skin.',\n",
              "  'psoriasis'),\n",
              " ('i have noticed a sudden peeling of skin at different parts of my body, mainly arms, legs and back. also, i face severe joint pain and skin rashes.',\n",
              "  'psoriasis'),\n",
              " ('the skin on my genitals is red and inflamed. it is often itchy, burning, and uncomfortable. there are rashes on different parts of the body too.',\n",
              "  'psoriasis')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we are going to use the `random` module to shuffle our dataset and then check again the size first `10` examples before creating dataframes."
      ],
      "metadata": {
        "id": "miWuxr3nTTxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(dataset)"
      ],
      "metadata": {
        "id": "JjUTIfDIpRpE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OvbVPUrrnWRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd3e902-620c-44a1-a9e3-63c485921cd6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"i've recently been suffering with chills, lethargy, a cough, a high temperature, and difficulties breathing. i've been sweating profusely and generally feeling ill and weak. i've also had some quite thick and red phlegm.\",\n",
              "  'pneumonia'),\n",
              " ('i have been having severe itching, vomiting, and fatigue. i have also lost weight and have a high fever. my skin has turned yellow and my urine is dark. i am also experiencing abdominal pain.',\n",
              "  'jaundice'),\n",
              " (\"i'm having a lot of trouble with my bowel movements lately. it's hard to go and it hurts when i do. my anus is really sore and it's been bleeding when i go. it's really painful and i'm really uncomfortable.\",\n",
              "  'dimorphic hemorrhoids'),\n",
              " (\"i can't seem to catch my breath and i'm sweating a lot. i feel really sick and have a lot of phlegm in my throat. my chest hurts and my heart is racing. the mucus i'm coughing up is brownish and stringy.\",\n",
              "  'pneumonia'),\n",
              " (\"along with a phlegmy cough and muscle weakness, i've been dealing with excruciating back pain. in addition to feeling woozy and shaky on my feet, my neck has been hurting.\",\n",
              "  'cervical spondylosis'),\n",
              " (\"the pain in my calves is constant and becomes worse when i stand or walk for long periods of time. i am getting constant cramps and can't run for longer periods of time.\",\n",
              "  'varicose veins'),\n",
              " ('i have a runny nose and i am sneezing all the time. my eyes are itchy and often watery, and i am coughing all the time. my head hurts and all the time',\n",
              "  'allergy'),\n",
              " (\"constipation and soreness with bowel motions have been bothering me lately. when i go, my anus bleeds and is really uncomfortable. i'm in a lot of discomfort and it hurts extremely bad.\",\n",
              "  'dimorphic hemorrhoids'),\n",
              " (\"i'm dripping with perspiration and can't seem to catch my breath. my throat is filled with a lot of phlegm, and i feel awful. my heart is pounding, and my chest aches. i'm coughing up brownish mucous.\",\n",
              "  'pneumonia'),\n",
              " (\"i have a rash all over my body, and i can't stop scratching because my skin is itchy. my skin also has a few spots where the hue is altered and some lumps and knot-like pimples.\",\n",
              "  'fungal infection')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "dataset[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So for our train data we are going to take all the examples in the `dataset` and then for the `validation` and `testing` set we are going to take a fraction of `40%` and `60%` from the dataset respectively."
      ],
      "metadata": {
        "id": "qn6nF_4JUVMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame(dataset, columns=[\"text\", \"label\" ])\n",
        "\n",
        "TEST_EXAMPLES = int(.6 * len(dataset))\n",
        "\n",
        "random.shuffle(dataset)\n",
        "test_df = pd.DataFrame(dataset[:TEST_EXAMPLES], columns=[\"text\", \"label\" ])\n",
        "val_df = pd.DataFrame(dataset[TEST_EXAMPLES: ], columns=[\"text\", \"label\" ])"
      ],
      "metadata": {
        "id": "fbwMyFBzT8y_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking our dataframes.\n",
        "\n",
        "\n",
        "1. train dataframe"
      ],
      "metadata": {
        "id": "ShVBIiWjU9t_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vtBv4dnjU7tQ",
        "outputId": "4b815faa-9a5c-4706-c19b-1b72ba838b0e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text                  label\n",
              "0  i've recently been suffering with chills, leth...              pneumonia\n",
              "1  i have been having severe itching, vomiting, a...               jaundice\n",
              "2  i'm having a lot of trouble with my bowel move...  dimorphic hemorrhoids\n",
              "3  i can't seem to catch my breath and i'm sweati...              pneumonia\n",
              "4  along with a phlegmy cough and muscle weakness...   cervical spondylosis"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f66828a4-e5c5-40ee-896e-175dd675ae44\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i've recently been suffering with chills, leth...</td>\n",
              "      <td>pneumonia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i have been having severe itching, vomiting, a...</td>\n",
              "      <td>jaundice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i'm having a lot of trouble with my bowel move...</td>\n",
              "      <td>dimorphic hemorrhoids</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i can't seem to catch my breath and i'm sweati...</td>\n",
              "      <td>pneumonia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>along with a phlegmy cough and muscle weakness...</td>\n",
              "      <td>cervical spondylosis</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f66828a4-e5c5-40ee-896e-175dd675ae44')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f66828a4-e5c5-40ee-896e-175dd675ae44 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f66828a4-e5c5-40ee-896e-175dd675ae44');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. test dataframe"
      ],
      "metadata": {
        "id": "X1ktJffeVIBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZwWqjTLcVHHU",
        "outputId": "b4538e30-7ef9-47a2-a2e8-5e04342ac714"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text                    label\n",
              "0  i have a plugged nose and nasal congestion. i ...                  allergy\n",
              "1  the itching is making it hard for me to sleep ...              chicken pox\n",
              "2  recently, my calves have been cramping up freq...           varicose veins\n",
              "3  i've had back pain, a persistent cough, and we...     cervical spondylosis\n",
              "4  i have pain near my pelvic region and vomit a ...  urinary tract infection"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3adb6c8b-a80c-406c-baf3-ffa20f4e72e8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i have a plugged nose and nasal congestion. i ...</td>\n",
              "      <td>allergy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the itching is making it hard for me to sleep ...</td>\n",
              "      <td>chicken pox</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>recently, my calves have been cramping up freq...</td>\n",
              "      <td>varicose veins</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i've had back pain, a persistent cough, and we...</td>\n",
              "      <td>cervical spondylosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i have pain near my pelvic region and vomit a ...</td>\n",
              "      <td>urinary tract infection</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3adb6c8b-a80c-406c-baf3-ffa20f4e72e8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3adb6c8b-a80c-406c-baf3-ffa20f4e72e8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3adb6c8b-a80c-406c-baf3-ffa20f4e72e8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. validatation dataframe"
      ],
      "metadata": {
        "id": "UkqLes8-VNPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "DzNJKAAVVEeL",
        "outputId": "bedcfee3-50c9-4edc-9ded-bfa94e826ea5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text                  label\n",
              "0  i've been feeling really cold and tired lately...              pneumonia\n",
              "1  constipation, discomfort with bowel motions, a...  dimorphic hemorrhoids\n",
              "2  i have been feeling itchy and have been vomiti...               jaundice\n",
              "3  i am experiencing skin rashes with burning sor...               impetigo\n",
              "4  along with excessive appetite, a stiff neck, d...               migraine"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1e49f9f6-c2c3-408c-b95e-05c0bf72ddf5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i've been feeling really cold and tired lately...</td>\n",
              "      <td>pneumonia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>constipation, discomfort with bowel motions, a...</td>\n",
              "      <td>dimorphic hemorrhoids</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i have been feeling itchy and have been vomiti...</td>\n",
              "      <td>jaundice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i am experiencing skin rashes with burning sor...</td>\n",
              "      <td>impetigo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>along with excessive appetite, a stiff neck, d...</td>\n",
              "      <td>migraine</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e49f9f6-c2c3-408c-b95e-05c0bf72ddf5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1e49f9f6-c2c3-408c-b95e-05c0bf72ddf5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1e49f9f6-c2c3-408c-b95e-05c0bf72ddf5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have text matched to labels, we can go ahead and save the `csv` files for these 3 different sets of data"
      ],
      "metadata": {
        "id": "cvjr1em9VYNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv(os.path.join(base_dir, \"train.csv\"),  index = False, header = True)\n",
        "test_df.to_csv(os.path.join(base_dir, \"test.csv\"),  index = False, header = True)\n",
        "val_df.to_csv(os.path.join(base_dir, \"val.csv\"),  index = False, header = True)\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQlpvRx4VWum",
        "outputId": "eb667c8b-7ec4-4a96-d3f9-3ab9abc0c012"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code cell we are going to count the examples that are in each set of our whole dataset."
      ],
      "metadata": {
        "id": "6_0O8AsVV0ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"Set\", \"Example(s)\"]\n",
        "\n",
        "examples = [\n",
        "    ['training', len(train_df)],\n",
        "    ['validation', len(val_df)],\n",
        "    ['testing', len(test_df)],\n",
        "    ['total', len(train_df) +  len(test_df) + len(val_df)],\n",
        "]\n",
        "\n",
        "tabulate_data(columns, examples, \"Exmples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXeApTx7Vp-m",
        "outputId": "8c3f7498-e17b-43cf-eb35-82fd34bb427a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+\n",
            "| Set        | Example(s) |\n",
            "+------------+------------+\n",
            "| training   |       1200 |\n",
            "| validation |        480 |\n",
            "| testing    |        720 |\n",
            "| total      |       2400 |\n",
            "+------------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features and Labels\n",
        "Our fetures are the actual `text` in the dataframes which is the column named `text` and our labels will come from the column called `label`. In the following code cell we are going to read features and labels in a numpy arrays for each set."
      ],
      "metadata": {
        "id": "f5MGCJeBWMr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "train_texts = train_df.text.values\n",
        "train_labels = train_df.label.values\n",
        "\n",
        "# test\n",
        "test_texts = test_df.text.values\n",
        "test_labels = test_df.label.values\n",
        "\n",
        "# val\n",
        "val_texts = val_df.text.values\n",
        "val_labels = val_df.label.values"
      ],
      "metadata": {
        "id": "0OtJMI3tWDxP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Preprocessing\n",
        "In our text processing pipeline we need to do the following steps:\n",
        "\n",
        "1. tokenize sentences\n",
        "* this is the process of converting a sentence or text into senquence of word. For this process we are going to use a pre-trained model from spacy language model. You can read more about other tokenizers that you can use at [pytorch](https://pytorch.org/text/stable/data_utils.html).org.\n",
        "\n",
        "2. vocabulary\n",
        "We will to create a vocabulary based on our sentences that are in the train dataset. A `vocabulary` is esentially a `word` to `index` mapping that allows us to reference the word with their integer representation, since machine leaning models does not understand words. This vocabulary will be used during model training and also can be used at model inference.\n",
        "\n",
        "### Tokenizer\n",
        "In the following code cell we are going to geta a tokenier object that will convert a sentence into a sequence of word using the `spacy-en` language model. The reason we are using the english langauge model it's because our intents are in english."
      ],
      "metadata": {
        "id": "zn4H4nQPWh6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = data.utils.get_tokenizer('spacy', 'en')\n",
        "tokenizer(\"This is a boy.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKzRB6ERWb-Z",
        "outputId": "12957ef6-0a2b-4797-c338-5cc2c74236cf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'boy', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary\n",
        "In the following code cell we are going to create a `vocabulary` object from torchtext. So we are going to use the `Counter` module from `collections` to generate these counts from our train features.\n",
        "\n",
        "We are going to specify the `min_freq` to `2` meaning that the words that does not appear at least 2 times will be converted to unknown. We are also going to specify the special tokens during creation of the vocabulary object."
      ],
      "metadata": {
        "id": "6eg16gaXXYXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter = Counter()\n",
        "for line in train_texts:\n",
        "    counter.update(tokenizer(line))\n",
        "#  our special tokens are (unknown, padding, start of sentence, end of sentence)\n",
        "vocabulary = vocab.vocab(counter, min_freq=2, specials=('<unk>', '<pad>', '<sos>', '<eos>'))"
      ],
      "metadata": {
        "id": "TGI6C9-8XETl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STOI - String To Integer\n",
        "This will be a dictionary that contains a string to integer mapping which will be our actual vocabulary. In the following code cell we are going to create object called `stoi` which is essentially a dictionary of word to index mapping. This dictionary will be used during training as well as during model inference."
      ],
      "metadata": {
        "id": "cTx0dEomYPXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = vocabulary.get_stoi()"
      ],
      "metadata": {
        "id": "P9qDNOS_XX-K"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Pipeline\n",
        "After our text has been tokenized we need a way of converting those words into numbers because machine leaning models understand numbers not words. That's where we the `text_pipeline` function comes into play. So this function takes in a sentence and tokenize it then converts each word to a number. Note that the word that does not exists in the vocabulay (`stoi`) will be converted to  an unkown `('<unk>')` token (0)."
      ],
      "metadata": {
        "id": "ERWg6ii7ZBR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_pipeline(x: str):\n",
        "  values = list()\n",
        "  tokens = tokenizer(x.lower()) # convert to lower case.\n",
        "  for token in tokens:\n",
        "    try:\n",
        "      v = stoi[token]\n",
        "    except KeyError as e:\n",
        "      v = stoi['<unk>']\n",
        "    values.append(v)\n",
        "  return values"
      ],
      "metadata": {
        "id": "76wRr0h0XX5U"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label pipeline\n",
        "Our labels for now are just textual. We also need to convert these labels into numbers. This is very simple what we need to do is to get all the uniqe labels and then create a `labels_vocab` which is a label to integer representation. \n",
        "\n",
        "> As you have noticed we have `24` labels which are diseases that we need to  predict given a sequence of words or symptoms description.\n",
        "\n",
        "The `label_pipeline` function will then takes in the label and then returns us an integer representation of that label."
      ],
      "metadata": {
        "id": "7T5oM8aIZqdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_dict = {k: v for v, k in enumerate([d.lower() for d in diseases])}"
      ],
      "metadata": {
        "id": "-xdB9m-cXX0y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_pipeline = lambda x: labels_dict[x]"
      ],
      "metadata": {
        "id": "j8Ub0Wq4XXxd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our vocabularies for labels `labels_dict` and  features `stoi` we can then save thes files as they will be used suring model inference. We are going to save these files as `.json` files."
      ],
      "metadata": {
        "id": "0KSxxSUwbMLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(base_dir, \"vocab.json\"), 'w') as f:\n",
        "  f.write(json.dumps(stoi, indent=2))\n",
        "\n",
        "with open(os.path.join(base_dir, \"labels_dict.json\"), 'w') as f:\n",
        "  f.write(json.dumps(labels_dict, indent=2))\n",
        "\n",
        "print(\"Saved!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kpBXmdMbhOe",
        "outputId": "90f26046-1187-4152-d61d-fea9f865c0a4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained vectors\n",
        "In the following code cell we are going to download the predtrained word vectors. We are going to use the `GloVe.6B.100d`. These are pretrained vectors that were trained with about `~6B` words and have a vector representation of a word in `100` dimension for each word."
      ],
      "metadata": {
        "id": "D-s0S-pza939"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 100\n",
        "glove_vectors = vocab.GloVe('6B', dim=EMBEDDING_DIM)"
      ],
      "metadata": {
        "id": "Mtrb3ctoXXsW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Embedding matrix\n",
        "Now that we have our glove vectors we need to costomize them so that they fit our use case. We are going to create an embedding matrix that suits the our vocabulary. So essentially this embedding matrix will be the word to vector mapping for all the words that arein our vocabulary."
      ],
      "metadata": {
        "id": "Kc_GWFnQcYmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(stoi)\n",
        "EMBEDDING_MATRIX= torch.zeros([VOCAB_SIZE, EMBEDDING_DIM])\n",
        "for i, word in enumerate(vocabulary.get_itos()):\n",
        "  EMBEDDING_MATRIX[i] = glove_vectors[word]"
      ],
      "metadata": {
        "id": "BeVKb-33a9dH"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the followig code cell we are going to check the embedding matrix for the word `\"the\"`."
      ],
      "metadata": {
        "id": "y9Tr8ACuciDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_MATRIX[stoi['rashes']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84DLGctra9aF",
        "outputId": "ea23ff3e-20e8-4351-912d-99c67611ff87"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0940, -0.4780,  0.1963,  0.1708, -0.3933,  0.1521,  0.0457,  0.1012,\n",
              "         0.1560, -0.7529, -0.4220, -0.7771,  0.9255,  0.4668,  0.7455, -0.0863,\n",
              "        -0.9490, -0.9518,  0.1909, -0.6723, -0.5767,  0.6063, -0.7237,  0.8700,\n",
              "         0.6882,  0.8048,  0.6606, -0.0345,  0.3939, -0.9757,  0.3382,  0.2372,\n",
              "        -0.1619, -0.1441, -0.7168,  0.2400,  0.5837, -0.3739, -0.0716,  0.5532,\n",
              "        -0.1439,  0.6237, -0.7219, -1.1488, -0.8956,  1.3508,  0.3170,  0.6591,\n",
              "         0.2489, -0.3259, -0.2373, -0.2869, -0.8275,  0.2285,  0.3482,  0.0633,\n",
              "         0.5941,  0.2802, -0.9896, -0.2571,  0.3315,  1.4114,  0.5243,  0.2785,\n",
              "        -0.2324, -0.0899,  0.5105, -0.7090,  0.6590, -1.2017, -0.3753, -0.1030,\n",
              "         0.4379, -0.0864, -0.0114,  0.9335, -0.3950,  0.5125, -0.0697,  0.8756,\n",
              "         0.6625, -0.2473,  0.1869,  0.6091, -0.7198, -0.4607,  0.5697,  0.1913,\n",
              "        -0.9225,  0.4444, -0.1880, -0.2680, -0.1879,  0.2891,  0.4514,  1.0086,\n",
              "        -0.5549, -0.5044,  0.7773, -0.9821])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Dataset for Training\n",
        "\n",
        "In the following code cell we are going to create a dataset class called `S2DCDataset`. This dataset will takes in the labels and the text of a set."
      ],
      "metadata": {
        "id": "qzFJ0SZZcrhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class S2DCDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, labels, text):\n",
        "    super(S2DCDataset, self).__init__()\n",
        "    self.labels = labels\n",
        "    self.text = text\n",
        "      \n",
        "  def __getitem__(self, index):\n",
        "    return self.labels[index], self.text[index]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.labels)"
      ],
      "metadata": {
        "id": "g3e35_Aza9W6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### collate_fn\n",
        "We are going to create a collate function called `tokenize_batch`. This function actually takes in a `batch` and does the preprocessing of the text and labels. This function will be passed to the `DataLoader` class to do the preprocessing of features and labels.\n",
        "\n",
        "`tokenize_batch` function:\n",
        "\n",
        "* this function takes in a batch in each set and convert the features and labels to integer representation. It goes ahead and `pad` and `truncate` the sequence to the same `length` and returns `labels` and `features`."
      ],
      "metadata": {
        "id": "AP49l5SIc8YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_batch(batch, max_len=100, padding=\"pre\"):\n",
        "  assert padding==\"pre\" or padding==\"post\", \"the padding can be either pre or post\"\n",
        "  labels_list, text_list = [], []\n",
        "  for _label, _text in batch:\n",
        "    labels_list.append(label_pipeline(_label))\n",
        "    text_holder = torch.zeros(max_len, dtype=torch.int32)\n",
        "    processed_text = torch.tensor(text_pipeline(_text.lower()), dtype=torch.int32)\n",
        "    pos = min(max_len, len(processed_text))\n",
        "    if padding == \"pre\":\n",
        "      text_holder[:pos] = processed_text[:pos]\n",
        "    else:\n",
        "      text_holder[-pos:] = processed_text[-pos:]\n",
        "    text_list.append(text_holder.unsqueeze(dim=0))\n",
        "  #  the labels will be torch long tensors since it is a multi-class classification.\n",
        "  return torch.LongTensor(labels_list), torch.cat(text_list, dim=0)"
      ],
      "metadata": {
        "id": "YzOXdOcka9UA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datasets\n",
        "In the following code cell we are going to create the datasets for all our three sets using the `MHCBataset` class."
      ],
      "metadata": {
        "id": "BftA18_edart"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = S2DCDataset(train_labels, train_texts)\n",
        "test_dataset = S2DCDataset(test_labels, test_texts)\n",
        "val_dataset = S2DCDataset(val_labels, val_texts)"
      ],
      "metadata": {
        "id": "tPaNeJOVdVrt"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iterators\n",
        "In the following code cell we are going to create loaders using the `DataLoader` class from `torch.utils.data` for our `3` sets. We are going to use the `batch_size` of `128` and our `collate_function` is `tokenize_batch`. For the validation and testing dataset we are going to set the shuffle to `False` because there's no need fo us to shuffle these examples."
      ],
      "metadata": {
        "id": "jLrPPmq2dpKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=tokenize_batch)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=tokenize_batch)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=tokenize_batch)"
      ],
      "metadata": {
        "id": "aFhmjJ0qdiXk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking a single Batch Data"
      ],
      "metadata": {
        "id": "khiKQFrJd58l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lbl, txt = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "Pfq_L5KNd-GT"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Labels in the first batch."
      ],
      "metadata": {
        "id": "6EZVZH0hd-C0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lbl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-6-3P76d9_s",
        "outputId": "67173de4-065d-40dd-e4a6-abed127d4347"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2,  0,  4, 10,  9,  5,  0,  4,  2, 22, 14, 16,  5, 22,  2,  0, 16, 13,\n",
              "         6,  3,  5, 10, 11,  8, 13,  7, 14, 23, 10, 19, 16,  7, 22,  5,  2,  9,\n",
              "        17,  3,  2,  5, 19,  9,  5,  4, 20, 14,  0, 14, 18, 13, 15, 12, 19,  6,\n",
              "        22, 14,  0, 10, 14, 19, 21, 21, 16, 17,  4,  8,  4, 12, 13,  1, 21,  2,\n",
              "        15, 16, 18, 22,  9,  0,  5,  0,  0, 17,  1, 16,  2, 12,  0,  8, 21,  6,\n",
              "        12, 15, 14,  2,  8, 23,  4,  4,  1,  8, 10, 13, 22, 22,  4, 14,  8, 19,\n",
              "        22, 23,  5, 15, 22, 17, 17, 22, 11, 17, 11,  1, 12, 14, 18, 11,  2,  3,\n",
              "         4, 14])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first sentence in the batch."
      ],
      "metadata": {
        "id": "i-ho13LdeNC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12CNt4ffeM-b",
        "outputId": "d0f5ee20-9f59-45ea-f135-da9be2ad1866"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  4,  34,   7,  24,  71, 475,  17,  26,  11,  17,   4,  76,  77,  78,\n",
              "         65, 189, 644,  57,  62,  20,   4,  34,  13, 336,  42,  17,  13, 678,\n",
              "         54,  83,  43,  53, 503,  20,   4,  76,  77, 940, 297,  49, 916,  20,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Creation\n",
        "Now that we have our loaders we can now create a model. The model that we are going to create is called `S2DCModel`.  As mentioned we are going to use `BiDirectional Long Short Term Memory (BiLSTM)` to build this model."
      ],
      "metadata": {
        "id": "WlFHj0dBeYlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class S2DCModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_size, hidden_size, output_size, num_layers\n",
        "               , bidirectional, dropout, pad_idx):\n",
        "    super(S2DCModel, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Sequential(\n",
        "        nn.Embedding(vocab_size, embedding_dim=embedding_size, padding_idx=pad_idx),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "    self.lstm = nn.Sequential(\n",
        "        nn.LSTM(\n",
        "          embedding_size, \n",
        "          hidden_size=hidden_size, \n",
        "          bidirectional=bidirectional, \n",
        "          num_layers=num_layers,\n",
        "          dropout=dropout\n",
        "        )\n",
        "    )\n",
        "    self.out = nn.Sequential(\n",
        "        nn.Linear(hidden_size * 2, out_features=128),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(128, out_features=output_size),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, text, text_lengths):\n",
        "    embedded = self.embedding(text)\n",
        "    # set batch_first=true since input shape has batch_size first and text_lengths to the device.\n",
        "    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False, batch_first=True)\n",
        "    packed_output, (h_0, c_0) = self.lstm(packed_embedded)\n",
        "    output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "    output = torch.cat((h_0[-2,:,:], h_0[-1,:,:]), dim = 1)\n",
        "    return self.out(output)"
      ],
      "metadata": {
        "id": "KUWtbBIceM7d"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Instance\n",
        "In the following code cell we are going to create a model instance."
      ],
      "metadata": {
        "id": "z509c1kIe4b5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(stoi) \n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = len(labels_dict)\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = stoi['<pad>'] \n",
        "s2dc_model = S2DCModel(\n",
        "              INPUT_DIM, \n",
        "              EMBEDDING_DIM, \n",
        "              HIDDEN_DIM, \n",
        "              OUTPUT_DIM, \n",
        "              N_LAYERS, \n",
        "              BIDIRECTIONAL, \n",
        "              DROPOUT, \n",
        "              PAD_IDX\n",
        ").to(device)\n",
        "s2dc_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F0hcAj4eMtn",
        "outputId": "137f712e-3aeb-449f-c706-de2b748494a0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "S2DCModel(\n",
              "  (embedding): Sequential(\n",
              "    (0): Embedding(1127, 100, padding_idx=1)\n",
              "    (1): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (lstm): Sequential(\n",
              "    (0): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  )\n",
              "  (out): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
              "    (1): Dropout(p=0.5, inplace=False)\n",
              "    (2): Linear(in_features=128, out_features=24, bias=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting Model Parameters\n",
        "In the following code cell we are going to count the model parameters."
      ],
      "metadata": {
        "id": "5Q3sHK_rfLch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models.model_params(s2dc_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6aaOSVbd98d",
        "outputId": "f5f53c32-9f63-42fd-ab4b-000d687fd3c0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOTAL MODEL PARAMETERS: \t2,491,604\n",
            "TOTAL TRAINABLE PARAMETERS: \t2,491,604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Embedding Vectors\n",
        "In the following code cell we are going to load the pretained custom vectors in our embedding layer. We are going to load the embedding vectors tha suits our data using the `mhcb_model.embedding[0].weight.data.copy_(EMBEDDING_MATRIX)` as follows:"
      ],
      "metadata": {
        "id": "ixhvbZrnfa1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s2dc_model.embedding[0].weight.data.copy_(EMBEDDING_MATRIX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWmuwFOCd959",
        "outputId": "13aaff03-f887-411c-9b2d-7b4946fcb0d9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        ...,\n",
              "        [-0.8353,  0.5702,  0.1922,  ..., -0.2583,  1.1413, -0.2057],\n",
              "        [ 0.5105, -0.6065,  0.5856,  ...,  0.0143,  0.7046,  0.2131],\n",
              "        [ 0.0220, -0.1008,  0.2294,  ..., -0.9317,  0.8893,  0.3623]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer and Criterion\n",
        "\n",
        "In the following code cell we are going to define the `optimizer` and `criterion`. For the `optimizer` we are going to use the `Adam` optimizer with default parameters and for the criterion we are going to use the `CrossEntropyLoss()` function since this is a `multi-class` classification."
      ],
      "metadata": {
        "id": "61W2JkF3f0V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(s2dc_model.parameters())\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "id": "bHJL3sJyd93Z"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code cell we are going to create our `categorical_accuracy` function, which is a function that calulates the the catecorical accuracy between the predicted labels and real labels."
      ],
      "metadata": {
        "id": "ifivhpn5gITZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "  top_pred = preds.argmax(1, keepdim = True)\n",
        "  correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "  acc = correct.float() / y.shape[0]\n",
        "  return acc"
      ],
      "metadata": {
        "id": "7LAsdm-DgEbF"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluate functions\n",
        "In the following code cell we are going to create our `train` and `evalute` functions:"
      ],
      "metadata": {
        "id": "JQZ49pfrgSPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "  epoch_loss,epoch_acc = 0, 0\n",
        "  model.train()\n",
        "  for batch in iterator:\n",
        "    y, X = batch\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    lengths = torch.tensor([len(i) for i in X])\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions = model(X, lengths).squeeze(1)\n",
        "    loss = criterion(predictions, y)\n",
        "    acc = categorical_accuracy(predictions, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "  epoch_loss,epoch_acc = 0, 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch in iterator:\n",
        "      y, X = batch\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      lengths = torch.tensor([len(i) for i in X])\n",
        "      predictions = model(X, lengths).squeeze(1)\n",
        "      loss = criterion(predictions, y)\n",
        "      acc = categorical_accuracy(predictions, y)\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "lkZUeIWJgPBx"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop\n",
        "In the following code cell we are going to run the training loop. We are going to save the model when the loss decreased."
      ],
      "metadata": {
        "id": "uSIas6DcgfMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 200\n",
        "MODEL_NAME = 's2dc_model.pt'\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start = time.time()\n",
        "  train_loss, train_acc = train(s2dc_model, train_loader, optimizer, criterion)\n",
        "  valid_loss, valid_acc = evaluate(s2dc_model, val_loader, criterion)\n",
        "  title = f\"EPOCH: {epoch+1:02}/{N_EPOCHS:02} {'saving best model...' if valid_loss < best_valid_loss else 'not saving...'}\"\n",
        "  if valid_loss < best_valid_loss:\n",
        "      best_valid_loss = valid_loss\n",
        "      torch.save(s2dc_model.state_dict(), MODEL_NAME)\n",
        "  end = time.time()\n",
        "  data = [\n",
        "       [\"Training\", f'{train_loss:.3f}', f'{train_acc:.3f}', f\"{hms_string(end - start)}\" ],\n",
        "       [\"Validation\", f'{valid_loss:.3f}', f'{valid_acc:.3f}', \"\" ],       \n",
        "   ]\n",
        "  columns = [\"CATEGORY\", \"LOSS\", \"ACCURACY\", \"ETA\"]\n",
        "  print(title)\n",
        "  tabulate_data(columns, data, title)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DLKNbVmgba4",
        "outputId": "f25c378f-fd86-4011-8125-7507b0f70ec4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 01/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.181 |    0.044 | 0:00:01.78 |\n",
            "| Validation | 3.171 |    0.052 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 02/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.174 |    0.065 | 0:00:00.97 |\n",
            "| Validation | 3.151 |    0.141 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 03/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.111 |    0.101 | 0:00:00.86 |\n",
            "| Validation | 2.913 |    0.160 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 04/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.057 |    0.113 | 0:00:00.85 |\n",
            "| Validation | 2.918 |    0.186 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 05/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.916 |    0.163 | 0:00:00.96 |\n",
            "| Validation | 2.675 |    0.209 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 06/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.852 |    0.160 | 0:00:01.00 |\n",
            "| Validation | 2.606 |    0.338 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 07/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.728 |    0.218 | 0:00:00.98 |\n",
            "| Validation | 2.329 |    0.367 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 08/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.685 |    0.216 | 0:00:00.95 |\n",
            "| Validation | 2.248 |    0.419 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 09/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.625 |    0.233 | 0:00:00.88 |\n",
            "| Validation | 2.207 |    0.448 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 10/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.522 |    0.270 | 0:00:00.87 |\n",
            "| Validation | 2.024 |    0.445 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 11/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.470 |    0.288 | 0:00:00.89 |\n",
            "| Validation | 1.958 |    0.524 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 12/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.417 |    0.291 | 0:00:00.87 |\n",
            "| Validation | 1.771 |    0.561 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 13/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.366 |    0.315 | 0:00:00.89 |\n",
            "| Validation | 1.728 |    0.600 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 14/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.315 |    0.343 | 0:00:00.88 |\n",
            "| Validation | 1.616 |    0.606 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 15/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.238 |    0.343 | 0:00:00.90 |\n",
            "| Validation | 1.539 |    0.635 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 16/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.169 |    0.361 | 0:00:00.87 |\n",
            "| Validation | 1.351 |    0.686 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 17/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.074 |    0.386 | 0:00:00.89 |\n",
            "| Validation | 1.193 |    0.757 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 18/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.042 |    0.391 | 0:00:00.90 |\n",
            "| Validation | 1.167 |    0.743 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 19/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.041 |    0.380 | 0:00:00.89 |\n",
            "| Validation | 1.147 |    0.733 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 20/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.054 |    0.377 | 0:00:00.99 |\n",
            "| Validation | 1.137 |    0.782 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 21/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.024 |    0.385 | 0:00:00.98 |\n",
            "| Validation | 1.007 |    0.837 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 22/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.905 |    0.415 | 0:00:01.01 |\n",
            "| Validation | 0.995 |    0.801 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 23/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.927 |    0.402 | 0:00:00.95 |\n",
            "| Validation | 0.834 |    0.843 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 24/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.849 |    0.430 | 0:00:00.91 |\n",
            "| Validation | 0.745 |    0.861 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 25/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.793 |    0.436 | 0:00:00.91 |\n",
            "| Validation | 0.733 |    0.854 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 26/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.774 |    0.445 | 0:00:00.92 |\n",
            "| Validation | 0.623 |    0.888 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 27/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.680 |    0.472 | 0:00:00.90 |\n",
            "| Validation | 0.572 |    0.900 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 28/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.782 |    0.434 | 0:00:00.90 |\n",
            "| Validation | 0.680 |    0.846 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 29/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.728 |    0.451 | 0:00:00.91 |\n",
            "| Validation | 0.521 |    0.917 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 30/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.630 |    0.473 | 0:00:00.90 |\n",
            "| Validation | 0.539 |    0.907 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 31/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.722 |    0.447 | 0:00:00.89 |\n",
            "| Validation | 0.585 |    0.878 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 32/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.687 |    0.459 | 0:00:00.92 |\n",
            "| Validation | 0.503 |    0.923 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 33/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.603 |    0.487 | 0:00:00.92 |\n",
            "| Validation | 0.402 |    0.927 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 34/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.734 |    0.439 | 0:00:00.94 |\n",
            "| Validation | 0.423 |    0.912 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 35/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.616 |    0.484 | 0:00:01.01 |\n",
            "| Validation | 0.339 |    0.960 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 36/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.495 |    0.515 | 0:00:00.98 |\n",
            "| Validation | 0.344 |    0.939 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 37/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.584 |    0.481 | 0:00:00.97 |\n",
            "| Validation | 0.294 |    0.959 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 38/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.476 |    0.515 | 0:00:01.01 |\n",
            "| Validation | 0.265 |    0.971 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 39/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.553 |    0.476 | 0:00:00.91 |\n",
            "| Validation | 0.229 |    0.981 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 40/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.458 |    0.517 | 0:00:00.89 |\n",
            "| Validation | 0.244 |    0.961 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 41/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.506 |    0.497 | 0:00:00.91 |\n",
            "| Validation | 0.199 |    0.980 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 42/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.476 |    0.508 | 0:00:00.89 |\n",
            "| Validation | 0.255 |    0.950 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 43/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.508 |    0.504 | 0:00:00.91 |\n",
            "| Validation | 0.172 |    0.982 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 44/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.472 |    0.511 | 0:00:00.90 |\n",
            "| Validation | 0.212 |    0.954 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 45/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.575 |    0.470 | 0:00:00.89 |\n",
            "| Validation | 0.191 |    0.965 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 46/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.468 |    0.518 | 0:00:00.89 |\n",
            "| Validation | 0.199 |    0.964 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 47/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.501 |    0.490 | 0:00:00.92 |\n",
            "| Validation | 0.169 |    0.976 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 48/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.474 |    0.493 | 0:00:00.95 |\n",
            "| Validation | 0.169 |    0.971 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 49/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.407 |    0.523 | 0:00:01.01 |\n",
            "| Validation | 0.135 |    0.977 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 50/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.363 |    0.537 | 0:00:01.01 |\n",
            "| Validation | 0.126 |    0.976 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 51/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.393 |    0.544 | 0:00:01.00 |\n",
            "| Validation | 0.085 |    0.996 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 52/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.480 |    0.490 | 0:00:00.91 |\n",
            "| Validation | 0.105 |    0.986 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 53/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.365 |    0.533 | 0:00:00.87 |\n",
            "| Validation | 0.092 |    0.990 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 54/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.443 |    0.498 | 0:00:00.89 |\n",
            "| Validation | 0.062 |    0.993 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 55/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.396 |    0.514 | 0:00:00.90 |\n",
            "| Validation | 0.060 |    0.995 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 56/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.343 |    0.552 | 0:00:00.88 |\n",
            "| Validation | 0.109 |    0.979 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 57/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.462 |    0.509 | 0:00:00.87 |\n",
            "| Validation | 0.110 |    0.976 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 58/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.466 |    0.510 | 0:00:00.89 |\n",
            "| Validation | 0.068 |    0.996 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 59/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.450 |    0.498 | 0:00:00.87 |\n",
            "| Validation | 0.082 |    0.992 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 60/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.446 |    0.505 | 0:00:00.87 |\n",
            "| Validation | 0.074 |    0.988 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 61/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.344 |    0.536 | 0:00:00.89 |\n",
            "| Validation | 0.059 |    0.997 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 62/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.417 |    0.515 | 0:00:00.90 |\n",
            "| Validation | 0.058 |    0.994 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 63/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.375 |    0.532 | 0:00:00.94 |\n",
            "| Validation | 0.065 |    0.994 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 64/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.347 |    0.532 | 0:00:01.00 |\n",
            "| Validation | 0.057 |    0.996 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 65/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.259 |    0.557 | 0:00:01.00 |\n",
            "| Validation | 0.041 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 66/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.321 |    0.534 | 0:00:00.94 |\n",
            "| Validation | 0.045 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 67/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.367 |    0.518 | 0:00:00.88 |\n",
            "| Validation | 0.034 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 68/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.380 |    0.507 | 0:00:00.87 |\n",
            "| Validation | 0.045 |    0.994 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 69/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.311 |    0.550 | 0:00:00.88 |\n",
            "| Validation | 0.024 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 70/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.340 |    0.535 | 0:00:00.87 |\n",
            "| Validation | 0.069 |    0.992 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 71/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.458 |    0.492 | 0:00:00.87 |\n",
            "| Validation | 0.044 |    0.992 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 72/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.373 |    0.511 | 0:00:01.08 |\n",
            "| Validation | 0.026 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 73/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.319 |    0.533 | 0:00:01.04 |\n",
            "| Validation | 0.021 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 74/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.302 |    0.539 | 0:00:01.02 |\n",
            "| Validation | 0.022 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 75/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.349 |    0.529 | 0:00:00.88 |\n",
            "| Validation | 0.017 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 76/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.284 |    0.548 | 0:00:00.88 |\n",
            "| Validation | 0.015 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 77/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.356 |    0.517 | 0:00:00.92 |\n",
            "| Validation | 0.016 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 78/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.268 |    0.553 | 0:00:00.96 |\n",
            "| Validation | 0.017 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 79/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.335 |    0.535 | 0:00:00.95 |\n",
            "| Validation | 0.032 |    0.995 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 80/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.306 |    0.536 | 0:00:00.97 |\n",
            "| Validation | 0.017 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 81/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.328 |    0.530 | 0:00:00.87 |\n",
            "| Validation | 0.028 |    0.996 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 82/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.349 |    0.517 | 0:00:00.90 |\n",
            "| Validation | 0.013 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 83/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.345 |    0.534 | 0:00:00.87 |\n",
            "| Validation | 0.017 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 84/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.339 |    0.522 | 0:00:00.85 |\n",
            "| Validation | 0.025 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 85/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.312 |    0.534 | 0:00:00.86 |\n",
            "| Validation | 0.018 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 86/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.320 |    0.539 | 0:00:00.86 |\n",
            "| Validation | 0.016 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 87/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.368 |    0.520 | 0:00:00.87 |\n",
            "| Validation | 0.018 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 88/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.353 |    0.520 | 0:00:00.86 |\n",
            "| Validation | 0.015 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 89/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.297 |    0.535 | 0:00:00.88 |\n",
            "| Validation | 0.013 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 90/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.328 |    0.537 | 0:00:00.90 |\n",
            "| Validation | 0.011 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 91/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.268 |    0.542 | 0:00:00.87 |\n",
            "| Validation | 0.012 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 92/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.280 |    0.542 | 0:00:00.92 |\n",
            "| Validation | 0.014 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 93/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.242 |    0.558 | 0:00:01.00 |\n",
            "| Validation | 0.009 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 94/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.300 |    0.528 | 0:00:00.96 |\n",
            "| Validation | 0.009 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 95/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.265 |    0.545 | 0:00:00.95 |\n",
            "| Validation | 0.010 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 96/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.314 |    0.535 | 0:00:00.86 |\n",
            "| Validation | 0.009 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 97/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.246 |    0.549 | 0:00:00.87 |\n",
            "| Validation | 0.006 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 98/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.266 |    0.549 | 0:00:00.87 |\n",
            "| Validation | 0.008 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 99/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.319 |    0.529 | 0:00:00.86 |\n",
            "| Validation | 0.009 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 100/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.304 |    0.536 | 0:00:00.87 |\n",
            "| Validation | 0.007 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 101/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.290 |    0.533 | 0:00:00.85 |\n",
            "| Validation | 0.007 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 102/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.309 |    0.543 | 0:00:00.87 |\n",
            "| Validation | 0.011 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 103/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.317 |    0.531 | 0:00:00.87 |\n",
            "| Validation | 0.009 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 104/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.289 |    0.540 | 0:00:00.86 |\n",
            "| Validation | 0.011 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 105/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.291 |    0.540 | 0:00:00.87 |\n",
            "| Validation | 0.007 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 106/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.271 |    0.545 | 0:00:00.86 |\n",
            "| Validation | 0.021 |    0.993 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 107/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.276 |    0.543 | 0:00:00.94 |\n",
            "| Validation | 0.007 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 108/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.310 |    0.531 | 0:00:00.98 |\n",
            "| Validation | 0.006 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 109/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.377 |    0.509 | 0:00:01.00 |\n",
            "| Validation | 0.005 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 110/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.261 |    0.543 | 0:00:00.94 |\n",
            "| Validation | 0.004 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 111/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.260 |    0.542 | 0:00:00.88 |\n",
            "| Validation | 0.004 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 112/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.342 |    0.523 | 0:00:00.87 |\n",
            "| Validation | 0.016 |    0.995 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 113/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.282 |    0.544 | 0:00:00.87 |\n",
            "| Validation | 0.007 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 114/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.271 |    0.538 | 0:00:00.88 |\n",
            "| Validation | 0.004 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 115/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.296 |    0.546 | 0:00:00.88 |\n",
            "| Validation | 0.005 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 116/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.253 |    0.541 | 0:00:00.87 |\n",
            "| Validation | 0.005 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 117/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.314 |    0.519 | 0:00:00.90 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 118/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.396 |    0.498 | 0:00:00.87 |\n",
            "| Validation | 0.004 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 119/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.231 |    0.553 | 0:00:00.90 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 120/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.339 |    0.514 | 0:00:00.88 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 121/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.219 |    0.565 | 0:00:00.87 |\n",
            "| Validation | 0.006 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 122/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.301 |    0.528 | 0:00:00.95 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 123/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.268 |    0.540 | 0:00:00.99 |\n",
            "| Validation | 0.002 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 124/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.283 |    0.540 | 0:00:00.95 |\n",
            "| Validation | 0.006 |    0.997 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 125/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.226 |    0.563 | 0:00:00.93 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 126/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.222 |    0.557 | 0:00:00.88 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 127/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.310 |    0.529 | 0:00:00.87 |\n",
            "| Validation | 0.004 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 128/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.300 |    0.538 | 0:00:00.88 |\n",
            "| Validation | 0.005 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 129/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.262 |    0.551 | 0:00:00.99 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 130/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.248 |    0.543 | 0:00:00.87 |\n",
            "| Validation | 0.053 |    0.993 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 131/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.254 |    0.548 | 0:00:00.87 |\n",
            "| Validation | 0.004 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 132/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.244 |    0.558 | 0:00:00.88 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 133/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.191 |    0.566 | 0:00:00.87 |\n",
            "| Validation | 0.002 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 134/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.317 |    0.530 | 0:00:00.89 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 135/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.352 |    0.515 | 0:00:00.87 |\n",
            "| Validation | 0.002 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 136/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.210 |    0.562 | 0:00:00.93 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 137/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.238 |    0.551 | 0:00:00.96 |\n",
            "| Validation | 0.002 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 138/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.349 |    0.515 | 0:00:00.97 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 139/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.340 |    0.526 | 0:00:00.96 |\n",
            "| Validation | 0.008 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 140/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.313 |    0.530 | 0:00:00.88 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 141/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.290 |    0.532 | 0:00:00.87 |\n",
            "| Validation | 0.002 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 142/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.240 |    0.550 | 0:00:00.87 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 143/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.301 |    0.539 | 0:00:00.87 |\n",
            "| Validation | 0.002 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 144/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.268 |    0.534 | 0:00:00.87 |\n",
            "| Validation | 0.002 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 145/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.270 |    0.538 | 0:00:00.87 |\n",
            "| Validation | 0.002 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 146/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.188 |    0.573 | 0:00:00.89 |\n",
            "| Validation | 0.021 |    0.996 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 147/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.250 |    0.551 | 0:00:00.87 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 148/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.275 |    0.535 | 0:00:00.87 |\n",
            "| Validation | 0.007 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 149/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.315 |    0.522 | 0:00:00.87 |\n",
            "| Validation | 0.005 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 150/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.293 |    0.527 | 0:00:00.87 |\n",
            "| Validation | 0.005 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 151/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.302 |    0.529 | 0:00:00.94 |\n",
            "| Validation | 0.006 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 152/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.285 |    0.539 | 0:00:00.98 |\n",
            "| Validation | 0.002 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 153/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.264 |    0.545 | 0:00:00.95 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 154/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.228 |    0.551 | 0:00:00.97 |\n",
            "| Validation | 0.011 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 155/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.274 |    0.545 | 0:00:00.88 |\n",
            "| Validation | 0.002 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 156/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.298 |    0.540 | 0:00:00.87 |\n",
            "| Validation | 0.005 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 157/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.260 |    0.555 | 0:00:00.88 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 158/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.312 |    0.526 | 0:00:00.86 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 159/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.314 |    0.531 | 0:00:00.88 |\n",
            "| Validation | 0.002 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 160/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.247 |    0.547 | 0:00:00.87 |\n",
            "| Validation | 0.002 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 161/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.288 |    0.533 | 0:00:00.88 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 162/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.257 |    0.549 | 0:00:01.20 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 163/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.268 |    0.541 | 0:00:01.13 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 164/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.344 |    0.506 | 0:00:01.29 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 165/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.251 |    0.542 | 0:00:01.36 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 166/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.287 |    0.538 | 0:00:01.90 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 167/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.230 |    0.552 | 0:00:01.39 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 168/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.312 |    0.519 | 0:00:00.99 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 169/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.283 |    0.535 | 0:00:00.97 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 170/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.194 |    0.570 | 0:00:00.88 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 171/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.309 |    0.526 | 0:00:00.86 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 172/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.237 |    0.551 | 0:00:00.87 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 173/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.251 |    0.540 | 0:00:00.89 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 174/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.305 |    0.522 | 0:00:00.86 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 175/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.224 |    0.548 | 0:00:00.88 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 176/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.208 |    0.567 | 0:00:00.88 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 177/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.314 |    0.527 | 0:00:00.87 |\n",
            "| Validation | 0.002 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 178/200 saving best model...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.297 |    0.534 | 0:00:00.97 |\n",
            "| Validation | 0.000 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 179/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.272 |    0.539 | 0:00:00.97 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 180/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.282 |    0.530 | 0:00:00.95 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 181/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.226 |    0.561 | 0:00:00.94 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 182/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.265 |    0.536 | 0:00:00.86 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 183/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.245 |    0.542 | 0:00:00.87 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 184/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.261 |    0.534 | 0:00:00.87 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 185/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.224 |    0.551 | 0:00:00.87 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 186/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.268 |    0.533 | 0:00:00.87 |\n",
            "| Validation | 0.000 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 187/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.211 |    0.562 | 0:00:00.87 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 188/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.238 |    0.555 | 0:00:00.87 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 189/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.280 |    0.529 | 0:00:00.87 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 190/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.250 |    0.539 | 0:00:00.88 |\n",
            "| Validation | 0.000 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 191/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.234 |    0.550 | 0:00:00.89 |\n",
            "| Validation | 0.000 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 192/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.233 |    0.554 | 0:00:00.87 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 193/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.273 |    0.549 | 0:00:00.95 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 194/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.295 |    0.522 | 0:00:00.96 |\n",
            "| Validation | 0.012 |    0.996 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 195/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.251 |    0.560 | 0:00:00.97 |\n",
            "| Validation | 0.013 |    0.993 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 196/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.277 |    0.549 | 0:00:00.93 |\n",
            "| Validation | 0.007 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 197/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.310 |    0.536 | 0:00:00.88 |\n",
            "| Validation | 0.011 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 198/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.281 |    0.529 | 0:00:00.87 |\n",
            "| Validation | 0.006 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 199/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.231 |    0.556 | 0:00:00.87 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "EPOCH: 200/200 not saving...\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.215 |    0.562 | 0:00:00.87 |\n",
            "| Validation | 0.001 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the best model.\n",
        "In the following code cell we are going to evaluate the best model using on the `test` data as follows:"
      ],
      "metadata": {
        "id": "Tt6w6iSNhBsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = [\"Set\", \"Loss\", \"Accuracy\", \"ETA (time)\"]\n",
        "s2dc_model.load_state_dict(torch.load(MODEL_NAME))\n",
        "test_loss, test_acc = evaluate(s2dc_model, test_loader, criterion)\n",
        "title = \"Model Evaluation Summary\"\n",
        "data_rows = [[\"Test\", f'{test_loss:.3f}', f'{test_acc * 100:.2f}%', \"\"]]\n",
        "\n",
        "tabulate_data(column_names, data_rows, title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjI79Ryhg0oz",
        "outputId": "be50ba93-7cc9-484d-88d4-9e59347b6d43"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+----------+------------+\n",
            "| Set  |  Loss | Accuracy | ETA (time) |\n",
            "+------+-------+----------+------------+\n",
            "| Test | 0.001 |  100.00% |            |\n",
            "+------+-------+----------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Inference\n",
        "In the following code cell we are going to make predictions with the best model. We will have the function called `inference_preprocess_text` which is a function that process the text for inference."
      ],
      "metadata": {
        "id": "KwHVSqWqhRDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_preprocess_text(text, max_len=100, padding=\"pre\"):\n",
        "  assert padding==\"pre\" or padding==\"post\", \"the padding can be either pre or post\"\n",
        "  text_holder = torch.zeros(max_len, dtype=torch.int32) # fixed size tensor of max_len with  = 0\n",
        "  processed_text = torch.tensor(text_pipeline(text), dtype=torch.int32)\n",
        "  pos = min(max_len, len(processed_text))\n",
        "  if padding == \"pre\":\n",
        "    text_holder[:pos] = processed_text[:pos]\n",
        "  else:\n",
        "    text_holder[-pos:] = processed_text[-pos:]\n",
        "  text_list= text_holder.unsqueeze(dim=0)\n",
        "  return text_list"
      ],
      "metadata": {
        "id": "b92dGb5-hI_I"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting Diseases\n",
        "In the following code cell we are going to create a function that predicts the `diseases` given a certain `sickness symptoms` called `predict_disease`."
      ],
      "metadata": {
        "id": "zmvxwzw8hp35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Prediction:\n",
        "  def __init__(self, pattern: str, disease: str, diseaseId: int, confidence: float):\n",
        "    self.pattern = pattern\n",
        "    self.disease = disease\n",
        "    self.diseaseId = diseaseId\n",
        "    self.confidence = confidence\n",
        "\n",
        "  def __repr__(self) -> str:\n",
        "    return f\"<S2DC Preciction: {self.disease}>\"\n",
        "\n",
        "  def __str__(self) -> str:\n",
        "    return f\"<S2DC Preciction: {self.disease}>\"\n",
        "\n",
        "  def to_json(self):\n",
        "    return {\n",
        "        'pattern':  self.pattern,\n",
        "        'disease':  self.disease,\n",
        "        'diseaseId':  self.diseaseId,\n",
        "        'confidence':  self.confidence,\n",
        "    }"
      ],
      "metadata": {
        "id": "INOb_R2XhozJ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_disease(model, sentence, device): \n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    tensor = inference_preprocess_text(sentence.lower()).to(device)\n",
        "    length = torch.tensor([len(t) for t in tensor])\n",
        "    probabilities = torch.softmax(model(tensor, length).squeeze(0), dim=0)\n",
        "    prediction = torch.argmax(probabilities)\n",
        "    prediction = prediction.detach().cpu().item()\n",
        "    tags = {v:k for k, v in labels_dict.items()}\n",
        "    tag = tags[prediction]\n",
        "   \n",
        "    return Prediction(\n",
        "        sentence.lower(), tag, int(prediction), float(round(probabilities[prediction].item(), 2))\n",
        "    )"
      ],
      "metadata": {
        "id": "O3Nk45oOijTX"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_disease(s2dc_model, \"i've recently been suffering with chills, lethargy, a cough, a high temperature, and difficulties breathing. i've been sweating profusely and generally feeling ill and weak. i've also had some quite thick and red phlegm.\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FotmLIEkqmE",
        "outputId": "cbf98238-84c7-44a3-cc84-811e25de00ce"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<S2DC Preciction: pneumonia>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the model.\n",
        "We are going to download the model"
      ],
      "metadata": {
        "id": "hZ1w-3vTlkXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "S0Yn_GM3llLp",
        "outputId": "c7e5acf4-3dcb-4f42-ff52-e738d5963cee"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_33b1a2d0-59f2-4b91-a7bc-71c0006b813e\", \"s2dc_model.pt\", 9970159)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}