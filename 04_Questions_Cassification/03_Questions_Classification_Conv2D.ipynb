{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Questions_Classification_Conv2D.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwFkL0ZK1L-X"
      },
      "source": [
        "### Questions Classification Custom dataset Conv2D.\n",
        "\n",
        "In this notebook we are going to use the previous notebook as the base notebook for this Conv2D notebook on question classification. In the previous notebbok we have leant how to implement the `FastText` model that was able to get a reasonable accuracy of `100%` and loss of `0` on the train, validation and test sets.\n",
        "\n",
        "\n",
        "### First let's mount our drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5bfB7PC1HRJ",
        "outputId": "4cc402d0-bceb-4676-aa62-dbe0578b8ab5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpcZ-TG63VID"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qpE80WdY3TgV",
        "outputId": "c63ec1bc-87c1-44d8-91c3-a7942447ff4f"
      },
      "source": [
        "import time\n",
        "from prettytable import PrettyTable\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import torch, os, random\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.9.0+cu102'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAkhI0Ns3wiU"
      },
      "source": [
        "### Setting up the seeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwYV_hrz3sSB"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deteministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoTx8u2J344-"
      },
      "source": [
        "### Loading files.\n",
        "\n",
        "Now we have 3 files for three sets that were created which are:\n",
        "```\n",
        "train.csv\n",
        "test.csv\n",
        "val.csv\n",
        "```\n",
        "\n",
        "We are going to use torchtext to load these files.\n",
        "\n",
        "**Note:** In the previous notebooks we loaded these our files as json files. This time around we are going to load `csv` files instead. The procedure is the same.\n",
        "\n",
        "### Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdKKgohm34DD"
      },
      "source": [
        "files_path = '/content/drive/MyDrive/NLP Data/questions-classification/pytorch'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBBoZ8MqGsGR"
      },
      "source": [
        "train_path = 'train.csv'\n",
        "test_path = 'test.csv'\n",
        "val_path = 'val.csv'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3bh0FGhXv3u"
      },
      "source": [
        "### Conv2D\n",
        "\n",
        "Conv Nets are not the best for processing sequence data. But in this notebook we are going to  use them beacuase. **Why not?**\n",
        "\n",
        "Unlike from the previous notebook we created a function called `generate_bigrams` that was passed on our `Text` field as a preprocessing function. That was because that's what the fasttext paper says. In this notebook we are nog going to do that. All we have to do is to pass `batch_first` argument to true, because Conv Nets expect input to have batchsize as the first dim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7YMbs8a43jD"
      },
      "source": [
        "### Creating the Fields.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_xUMByB5tGa"
      },
      "source": [
        "from torchtext.legacy import data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RaqW-b-4lJJ"
      },
      "source": [
        "TEXT = data.Field(\n",
        "   tokenize=\"spacy\",\n",
        "  batch_first=True,\n",
        "  tokenizer_language = 'en_core_web_sm',\n",
        ")\n",
        "LABEL = data.LabelField()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OugU18S_7tAS"
      },
      "source": [
        "fields = {\n",
        "  \"Questions\": ('text', TEXT),\n",
        "  \"Category1\": ('label', LABEL)\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiNOBqFO8pyo"
      },
      "source": [
        "### Creating the dataset.\n",
        "\n",
        "We ar going to use the `TabularDataset.split()` to create the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAgiWOoh8mWO"
      },
      "source": [
        "train_data, val_data, test_data = data.TabularDataset.splits(\n",
        "   files_path,\n",
        "   train=train_path,\n",
        "   test= train_path,\n",
        "   validation= train_path,\n",
        "   format = \"csv\",\n",
        "   fields=fields,\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f4YEu-fQ1WK",
        "outputId": "d30c0daf-a3a8-4db8-e6bc-2e5cdda4c529"
      },
      "source": [
        "len(train_data), len(test_data), len(val_data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5179, 5179, 5179)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ1oWIQW9KWn",
        "outputId": "a1242db6-c392-47b9-cc20-6d6a771b8630"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['What', 'is', 'the', 'name', 'of', 'Miss', 'India', '1994', '?'], 'label': 'HUM'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t-0JC1U-l73"
      },
      "source": [
        "### Building the Vocabulary and Loading the `pretrained` word vectors.\n",
        "\n",
        "We are going to use the `glove.6B.100d` word vectors which was trained with 6 billion words and each word is a 100 dimesional vector.\n",
        "\n",
        "**Note** We should only build the vocabulary on the `train` dataset only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4p3m1_--WYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbe7662f-2329-481b-fe10-86a2b0e7105c"
      },
      "source": [
        "MAX_VOCAB_SIZE = 100_000_000\n",
        "\n",
        "TEXT.build_vocab(\n",
        "    train_data,\n",
        "     max_size = MAX_VOCAB_SIZE,\n",
        "    vectors = \"glove.6B.100d\",\n",
        "    unk_init = torch.Tensor.normal_\n",
        ")\n",
        "LABEL.build_vocab(train_data)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:39, 5.39MB/s]                          \n",
            "100%|█████████▉| 399278/400000 [00:15<00:00, 25431.66it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFukx38W_aNv"
      },
      "source": [
        "### Device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYPGTIZ7_Zet",
        "outputId": "bb1aa406-6445-48ee-cf58-32aa4af73dd8"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKkUJMz6K0WH",
        "outputId": "8637a353-aff0-4b5a-caf1-b02563617892"
      },
      "source": [
        "LABEL.vocab.stoi"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(None,\n",
              "            {'ABBR': 5, 'DESC': 2, 'ENTY': 0, 'HUM': 1, 'LOC': 4, 'NUM': 3})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaCMz4yZ_n5W"
      },
      "source": [
        "### Creating iterators.\n",
        "\n",
        "We are going to use our favorite iterator known as the `BucketIterator` to create iterators for all the sets that we have.\n",
        "\n",
        "For the `batch_size` this time around we want to test a huge batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPDrPFKQ_mf1"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    device = device,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.text),\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spZX8zodA91F"
      },
      "source": [
        "### Creating the  Con2D Model.\n",
        "\n",
        "* In this notebook I'm not going to explain how convnets works but If you want to understand more about convnets. I recommend [this](https://github.com/CrispenGari/pytorch-python/blob/main/09_TorchText/02_Sentiment_Analyisis_Series/04_CNN_Sentiment_Analyisis.ipynb) notebook. Which has a clear explanation of how conv nets work on sequential data.\n",
        "\n",
        "\n",
        "* We are going to create a generic `ConvNet` model that accepts a number of parameters and all the magic will hapen behind the scene. **Really!!**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReThGRspA7uO"
      },
      "source": [
        "class QuestionsConv2DNet(nn.Module):\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               embedding_size,\n",
        "               n_filters,\n",
        "               filter_sizes,\n",
        "               output_size,\n",
        "               pad_idx,\n",
        "               dropout=.5\n",
        "               ):\n",
        "    super(QuestionsConv2DNet, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(\n",
        "        vocab_size, embedding_size, padding_idx=pad_idx\n",
        "    )\n",
        "    self.convs = nn.ModuleList([\n",
        "        nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels = n_filters,\n",
        "            kernel_size=(fs, embedding_size)\n",
        "        ) for fs in filter_sizes \n",
        "    ])\n",
        "\n",
        "    self.fc = nn.Linear(len(filter_sizes) * n_filters, output_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, text):\n",
        "    embedded = self.embedding(text).unsqueeze(1)\n",
        "    conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "    pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2)\n",
        "            for conv in conved\n",
        "            ]\n",
        "    cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "    return self.fc(cat)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2IVtv8EDhJd"
      },
      "source": [
        "### Creating the model instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbJdW058DgTl",
        "outputId": "58438e97-60cf-4b3b-93ef-1fee19746c76"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5]\n",
        "OUTPUT_DIM =  6\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] \n",
        "DROPOUT = 0.5\n",
        "\n",
        "questions_model = QuestionsConv2DNet(\n",
        "            INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            N_FILTERS,\n",
        "            FILTER_SIZES,\n",
        "            OUTPUT_DIM, \n",
        "            pad_idx = PAD_IDX,\n",
        "            dropout=DROPOUT\n",
        "            ).to(device)\n",
        "questions_model"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuestionsConv2DNet(\n",
              "  (embedding): Embedding(9053, 100, padding_idx=1)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
              "    (1): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
              "    (2): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
              "    (3): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
              "    (4): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
              "    (5): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
              "    (6): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
              "    (7): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
              "    (8): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
              "    (9): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
              "    (10): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=1100, out_features=6, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acQD-B6zEE72"
      },
      "source": [
        "### Model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQaztp4ID-HI",
        "outputId": "07a584d4-126f-4ec3-a93f-6fc2ef87f994"
      },
      "source": [
        "def count_trainable_params(model):\n",
        "  return sum(p.numel() for p in model.parameters()), sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "n_params, trainable_params = count_trainable_params(questions_model)\n",
        "print(f\"Total number of paramaters: {n_params:,}\\nTotal tainable parameters: {trainable_params:,}\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of paramaters: 1,343,006\n",
            "Total tainable parameters: 1,343,006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8gjXny7EUNb"
      },
      "source": [
        "### Loading pretrained vectors to the embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULe__PiKEQS8"
      },
      "source": [
        "pretrained_embeddings  = TEXT.vocab.vectors"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeXa9ycREfuV",
        "outputId": "39e07052-90fc-4dc6-b194-640d03330ffc"
      },
      "source": [
        "questions_model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.1233,  0.3499,  0.6173],\n",
              "        [ 0.7262,  0.0912, -0.3891,  ...,  0.0821,  0.4440, -0.7240],\n",
              "        [ 0.1638,  0.6046,  1.0789,  ..., -0.3140,  0.1844,  0.3624],\n",
              "        ...,\n",
              "        [ 0.0091,  0.2810,  0.7356,  ..., -0.7508,  0.8967, -0.7631],\n",
              "        [ 0.2906,  0.3217,  0.2419,  ..., -0.9444, -0.3790,  0.6196],\n",
              "        [-0.3898, -0.5949,  0.2729,  ..., -1.0948,  0.8617, -0.4429]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q27hKnrpEoKF"
      },
      "source": [
        "### Zeroing the `<pad>` and `<unk>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_qvO2teEmvh",
        "outputId": "389dd266-e229-4a58-f407-13fdfd4464d0"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token] or TEXT.vocab.stoi[\"<unk>\"]\n",
        "questions_model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "questions_model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "questions_model.embedding.weight.data"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.1638,  0.6046,  1.0789,  ..., -0.3140,  0.1844,  0.3624],\n",
              "        ...,\n",
              "        [ 0.0091,  0.2810,  0.7356,  ..., -0.7508,  0.8967, -0.7631],\n",
              "        [ 0.2906,  0.3217,  0.2419,  ..., -0.9444, -0.3790,  0.6196],\n",
              "        [-0.3898, -0.5949,  0.2729,  ..., -1.0948,  0.8617, -0.4429]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm2RklTLE-Dv"
      },
      "source": [
        "### Loss and optimizer.\n",
        "We are going to use the Adam as our optimizer with the default leaning rate. We are also going to use `CrossEntropyLoss()` as our loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7UCyAlYE1nA"
      },
      "source": [
        "optimizer = torch.optim.Adam(questions_model.parameters())\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi2tk-kSFTVq"
      },
      "source": [
        "### Accuracy function.\n",
        "We are going to create the `categorical_accuracy()` function that will calculate the categorical accuracy for predicted labels and actual labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRCfQZvmFPBS"
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "  top_pred = preds.argmax(1, keepdim = True)\n",
        "  correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "  return correct.float() / y.shape[0]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gf_nyXVFvOG"
      },
      "source": [
        "### Training and Evaluation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80QEfQeaFuVZ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss ,epoch_acc = 0, 0\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text = batch.text\n",
        "        predictions = model(text).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss , epoch_acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text = batch.text\n",
        "            predictions = model(text)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdtkgFFJG1WS"
      },
      "source": [
        "### Training loop.\n",
        "We are going to create helper functions that will help us to visualize our training.\n",
        "\n",
        "1. Time to string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMH6iA1AG0tj"
      },
      "source": [
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp5OjGiPHR6h"
      },
      "source": [
        "2. tabulate training epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtCTm84HHQb5"
      },
      "source": [
        "def visualize_training(start, end, train_loss, train_accuracy, val_loss, val_accuracy, title):\n",
        "  data = [\n",
        "       [\"Training\", f'{train_loss:.3f}', f'{train_accuracy:.3f}', f\"{hms_string(end - start)}\" ],\n",
        "       [\"Validation\", f'{val_loss:.3f}', f'{val_accuracy:.3f}', \"\" ],       \n",
        "  ]\n",
        "  table = PrettyTable([\"CATEGORY\", \"LOSS\", \"ACCURACY\", \"ETA\"])\n",
        "  table.align[\"CATEGORY\"] = 'l'\n",
        "  table.align[\"LOSS\"] = 'r'\n",
        "  table.align[\"ACCURACY\"] = 'r'\n",
        "  table.align[\"ETA\"] = 'r'\n",
        "  table.title = title\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcu_WMc2HXqb",
        "outputId": "ea3e601c-2332-40f0-ebe6-94ea60650861"
      },
      "source": [
        "N_EPOCHS = 30\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start = time.time()\n",
        "    train_loss, train_acc = train(questions_model, train_iter, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(questions_model, val_iter, criterion)\n",
        "    title = f\"EPOCH: {epoch+1:02}/{N_EPOCHS:02} {'saving best model...' if valid_loss < best_valid_loss else 'not saving...'}\"\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(questions_model.state_dict(), 'best-model.pt')\n",
        "    end = time.time()\n",
        "    visualize_training(start, end, train_loss, train_acc, valid_loss, valid_acc, title)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------+\n",
            "|     EPOCH: 01/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.589 |    0.799 | 0:00:32.42 |\n",
            "| Validation | 0.413 |    0.880 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 02/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.411 |    0.869 | 0:00:32.61 |\n",
            "| Validation | 0.274 |    0.925 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 03/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.273 |    0.921 | 0:00:32.77 |\n",
            "| Validation | 0.179 |    0.964 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 04/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.201 |    0.942 | 0:00:32.87 |\n",
            "| Validation | 0.122 |    0.976 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 05/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.135 |    0.967 | 0:00:32.96 |\n",
            "| Validation | 0.084 |    0.987 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 06/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.093 |    0.982 | 0:00:33.04 |\n",
            "| Validation | 0.058 |    0.992 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 07/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.065 |    0.987 | 0:00:33.13 |\n",
            "| Validation | 0.043 |    0.994 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 08/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.050 |    0.991 | 0:00:33.21 |\n",
            "| Validation | 0.033 |    0.997 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 09/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.037 |    0.994 | 0:00:33.21 |\n",
            "| Validation | 0.028 |    0.997 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 10/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.026 |    0.998 | 0:00:33.21 |\n",
            "| Validation | 0.021 |    0.998 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 11/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.020 |    0.998 | 0:00:33.21 |\n",
            "| Validation | 0.014 |    0.999 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 12/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.016 |    0.998 | 0:00:33.21 |\n",
            "| Validation | 0.013 |    0.999 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 13/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.014 |    0.999 | 0:00:33.20 |\n",
            "| Validation | 0.011 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 14/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.012 |    1.000 | 0:00:33.21 |\n",
            "| Validation | 0.009 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 15/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.009 |    1.000 | 0:00:33.21 |\n",
            "| Validation | 0.008 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 16/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.008 |    1.000 | 0:00:33.20 |\n",
            "| Validation | 0.007 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 17/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.006 |    1.000 | 0:00:33.21 |\n",
            "| Validation | 0.006 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 18/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.007 |    1.000 | 0:00:33.20 |\n",
            "| Validation | 0.005 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 19/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.006 |    1.000 | 0:00:33.19 |\n",
            "| Validation | 0.005 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 20/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.006 |    1.000 | 0:00:33.20 |\n",
            "| Validation | 0.004 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 21/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.005 |    1.000 | 0:00:33.20 |\n",
            "| Validation | 0.004 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 22/30 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.004 |    1.000 | 0:00:33.19 |\n",
            "| Validation | 0.004 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 23/30 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.003 |    1.000 | 0:00:33.19 |\n",
            "| Validation | 0.004 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 24/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.004 |    1.000 | 0:00:33.16 |\n",
            "| Validation | 0.004 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 25/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.004 |    0.999 | 0:00:33.16 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 26/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.003 |    1.000 | 0:00:33.15 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 27/30 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.003 |    1.000 | 0:00:33.14 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 28/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.003 |    1.000 | 0:00:33.17 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 29/30 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.004 |    1.000 | 0:00:33.17 |\n",
            "| Validation | 0.002 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 30/30 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.003 |    1.000 | 0:00:33.16 |\n",
            "| Validation | 0.003 |    1.000 |            |\n",
            "+------------+-------+----------+------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZNkikbDIB2z"
      },
      "source": [
        "### Model Evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8fWTBruHw1p",
        "outputId": "c6a4186f-148f-4002-ee01-bf464c729727"
      },
      "source": [
        "questions_model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(questions_model, test_iter, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.002 | Test Acc: 100.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4zEBLYsLki5"
      },
      "source": [
        "### Model Inference.\n",
        "\n",
        "We are now ready to make predictions with our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyNLifHaJBKS"
      },
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clNBiJ9NPkRJ",
        "outputId": "2c69eaef-c1cc-4fa6-eefb-6ff241ac39ea"
      },
      "source": [
        "reversed_labels = dict([(v, k) for (k, v) in LABEL.vocab.stoi.items()])\n",
        "reversed_labels"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'ENTY', 1: 'HUM', 2: 'DESC', 3: 'NUM', 4: 'LOC', 5: 'ABBR'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9A6ZGTILzWD"
      },
      "source": [
        "def tabulate(column_names, data, title=\"QUESTIONS PREDICTIONS TABLE\"):\n",
        "  table = PrettyTable(column_names)\n",
        "  table.align[column_names[0]] = \"l\"\n",
        "  table.align[column_names[1]] = \"l\"\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)\n",
        "\n",
        "def predict_question_type(model, sentence, min_len = 5, actual_class=0):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "     \n",
        "      if len(tokenized) < min_len:\n",
        "          tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
        "      indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "      tensor = torch.LongTensor(indexed).to(device).unsqueeze(0)\n",
        "      probabilities = model(tensor)\n",
        "      prediction = torch.argmax(probabilities, dim=1)\n",
        "      prediction = prediction.item()\n",
        "    \n",
        "      table_headers =[\"KEY\", \"VALUE\"]\n",
        "      table_data = [\n",
        "          [\"PREDICTED CLASS\",  prediction],\n",
        "          [\"ACTUAL CLASS\", actual_class],\n",
        "          [\"PREDICTED CLASS NAME\",  reversed_labels[prediction]],    \n",
        "      ]\n",
        "      tabulate(table_headers, table_data)\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm5h2CDLUECt",
        "outputId": "04f0d4cb-6e59-4ce3-f233-7cbd83166de4"
      },
      "source": [
        "reversed_labels"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'ENTY', 1: 'HUM', 2: 'DESC', 3: 'NUM', 4: 'LOC', 5: 'ABBR'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W93zzSlQMO4p"
      },
      "source": [
        "### Location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcuHlaPwML4Y",
        "outputId": "750946ba-323e-4faa-bd30-afad3d8396a0"
      },
      "source": [
        "predict_question_type(questions_model, \"What are the largest libraries in the US ?\", actual_class=4)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------+-------+\n",
            "| KEY                  | VALUE |\n",
            "+----------------------+-------+\n",
            "| PREDICTED CLASS      | 4     |\n",
            "| ACTUAL CLASS         | 4     |\n",
            "| PREDICTED CLASS NAME | LOC   |\n",
            "+----------------------+-------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5Eqjo0MMP8L"
      },
      "source": [
        "### Human"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnhFchDiMQQn",
        "outputId": "50e4cb7d-eba7-4cfb-8232-5081bbfdb301"
      },
      "source": [
        "predict_question_type(questions_model, \"Who is John Macarthur , 1767-1834 ?\", actual_class=1)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------+-------+\n",
            "| KEY                  | VALUE |\n",
            "+----------------------+-------+\n",
            "| PREDICTED CLASS      | 1     |\n",
            "| ACTUAL CLASS         | 1     |\n",
            "| PREDICTED CLASS NAME | HUM   |\n",
            "+----------------------+-------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WvswKe5MQ3D"
      },
      "source": [
        "### DESCRIPTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3jeZM6xMRVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10243d5-f9c8-476d-c8c6-a1686add068c"
      },
      "source": [
        "predict_question_type(questions_model, \"What is the root of all evil ? \", actual_class=2)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------+-------+\n",
            "| KEY                  | VALUE |\n",
            "+----------------------+-------+\n",
            "| PREDICTED CLASS      | 2     |\n",
            "| ACTUAL CLASS         | 2     |\n",
            "| PREDICTED CLASS NAME | DESC  |\n",
            "+----------------------+-------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeGjIT9NMRxA"
      },
      "source": [
        "### Numeric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GxH8LKiMSOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f2be336-29ee-447d-ce3f-7a738302d285"
      },
      "source": [
        "predict_question_type(questions_model, \"How many watts make a kilowatt ?\", actual_class=3)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------+-------+\n",
            "| KEY                  | VALUE |\n",
            "+----------------------+-------+\n",
            "| PREDICTED CLASS      | 3     |\n",
            "| ACTUAL CLASS         | 3     |\n",
            "| PREDICTED CLASS NAME | NUM   |\n",
            "+----------------------+-------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-7beS0jPLW7"
      },
      "source": [
        "### ENTITY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvQq7ul9PHFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83051967-6aa5-4abe-f30f-355e5e704b97"
      },
      "source": [
        "\n",
        "predict_question_type(questions_model, \"What films featured the character Popeye Doyle ?\", actual_class=0)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------+-------+\n",
            "| KEY                  | VALUE |\n",
            "+----------------------+-------+\n",
            "| PREDICTED CLASS      | 0     |\n",
            "| ACTUAL CLASS         | 0     |\n",
            "| PREDICTED CLASS NAME | ENTY  |\n",
            "+----------------------+-------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ0cQrfFQ9QI"
      },
      "source": [
        "### ABBREVIATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvzy3XHRQZQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac530e16-2823-479d-a06a-d600306d7f42"
      },
      "source": [
        "predict_question_type(questions_model, \"What does NECROSIS stands for ?\", actual_class=5)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------+-------+\n",
            "| KEY                  | VALUE |\n",
            "+----------------------+-------+\n",
            "| PREDICTED CLASS      | 5     |\n",
            "| ACTUAL CLASS         | 5     |\n",
            "| PREDICTED CLASS NAME | ABBR  |\n",
            "+----------------------+-------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Sa3_lq4VHSn"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "We were able to create our model and get a loss of `0` and `100%` accuracy on the validation and test data.\n",
        "\n",
        "### Next Step\n",
        "* In the next Notebook we are going to use `Conv1D` to perform sentiment analyisis on this dataset."
      ]
    }
  ]
}