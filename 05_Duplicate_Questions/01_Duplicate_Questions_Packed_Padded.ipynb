{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Duplicate_Questions_Packed_Padded.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7Jsp1cXo42F"
      },
      "source": [
        "### Question Pairs\n",
        "In this notebook we are going to learn how we can classify wether questions are dulicates/simmilar or not using the dataset downloaded from [Kaggle](https://www.kaggle.com/quora/question-pairs-dataset).\n",
        "\n",
        " As usual I'm going to uzip the file and upload it to my google drive so that it can be easly loaded here in google colab.\n",
        "\n",
        "### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7CjjHG0zomH9",
        "outputId": "82029651-198a-4447-9991-5ed6e224d7c1"
      },
      "source": [
        "import time, os, torch, random, math\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import torch, os, random\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.9.0+cu102'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GP7IDeFrG_y"
      },
      "source": [
        "### SEEDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFDVrd3arD1-"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deteministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l8XBaShrNPp"
      },
      "source": [
        "### Device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0nGBaM2rKqn",
        "outputId": "962737fe-bc08-4060-9760-0b690bb99f7a"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TtGHvnUsML-"
      },
      "source": [
        "### Mounting the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHCa7e5NsR3L",
        "outputId": "5075fa83-fc24-4c9a-c0fc-accc3c63b1ae"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLVZWBcorbTh"
      },
      "source": [
        "### Paths to data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0KKQSDjrXhr",
        "outputId": "8ac56edb-8004-4be5-81f6-c0783e4895b2"
      },
      "source": [
        "base_path = '/content/drive/MyDrive/NLP Data/duplicates-questions'\n",
        "train_path = 'train.csv'\n",
        "val_path = 'val.csv'\n",
        "test_path = 'test.csv'\n",
        "\n",
        "os.path.exists(base_path)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_gtMwjPsjxa"
      },
      "source": [
        "### Data Loading\n",
        "This is a binary classification task where we are going to predict weather questions are duplicates or not. We are going to have 2 inputs which is two differant questions that will map to one label, is_duplicate(1) or is_not_duplicate (0). We are going to create the fields of our data. We are going to use [this notebook](https://github.com/CrispenGari/pytorch-python/blob/main/09_TorchText/02_Sentiment_Analyisis_Series/02_Updated_Sentiment_Analysis.ipynb) together with [this](https://github.com/CrispenGari/nlp-pytorch/blob/main/04_Questions_Cassification/01_Questions_Classification.ipynb) as our guieds for this task.\n",
        "\n",
        "### What do we have?\n",
        "We are having three `csv` files for each set whih makes it easy to create the dataset for this task.\n",
        "\n",
        "Since we are going to use the `packed_padded_sequence` for this classification task, during our Text field creation we need to pass `include_lengths=True`. And also since this is a binary classificatio task we are going to create a label field with a float data type. This is what i did in [this](https://github.com/CrispenGari/pytorch-python/blob/main/09_TorchText/02_Sentiment_Analyisis_Series/02_Updated_Sentiment_Analysis.ipynb) notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbyLjuhrsDES"
      },
      "source": [
        "from torchtext.legacy import data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwE4hbQAvMRp"
      },
      "source": [
        "### Fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSxCs527vK5G"
      },
      "source": [
        "TEXT = data.Field(\n",
        "      tokenize = 'spacy',\n",
        "      tokenizer_language = 'en_core_web_sm',\n",
        "      include_lengths=True\n",
        "    )\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDB7h4t8wNc0"
      },
      "source": [
        "fields = {\n",
        "    \"question1\": (\"qn1\", TEXT),\n",
        "    \"question2\": (\"qn2\", TEXT), \n",
        "    \"is_duplicate\": (\"label\", LABEL),\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LsuY9EXwsm-"
      },
      "source": [
        "Next we will create our dataset using our favourate class fro  torchtext `TabularDataset`. We are going to load the data that is in `csv` format as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVu-sAxPwryA"
      },
      "source": [
        "train_data, val_data, test_data = data.TabularDataset.splits(\n",
        "   base_path,\n",
        "   train=train_path,\n",
        "   test= test_path,\n",
        "   validation= val_path,\n",
        "   format = \"csv\",\n",
        "   fields=fields\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWjZPcAZxGHS",
        "outputId": "fdcdc6ee-efb4-4617-9c57-a957ff4f3fd0"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'qn1': ['Is', 'it', 'right', 'for', 'a', 'woman', 'to', 'date', 'someone', '2', '-', '3', 'years', 'younger', 'than', 'her', '?'], 'qn2': ['Is', 'it', 'strange', 'to', 'have', 'a', 'crush', 'on', 'someone', 'say', '17', 'years', 'younger', 'than', 'me', '?'], 'label': '0'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlNAFiSnxNhd"
      },
      "source": [
        "#### Next we will build the Vocabulary.\n",
        "\n",
        "We are going to use the pretrained word vectors `glove.6B.100d` which was trained on about 6 billion english words.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfx6__Xxx3xN"
      },
      "source": [
        "\n",
        "MAX_VOCAB_SIZE = 100_000\n",
        "\n",
        "TEXT.build_vocab(\n",
        "     train_data,\n",
        "     max_size = MAX_VOCAB_SIZE,\n",
        "     vectors = \"glove.6B.100d\",\n",
        "    unk_init = torch.Tensor.normal_\n",
        ")\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jx5d2T-0NXe",
        "outputId": "956c3d2c-6bff-477d-9556-0ea1032c83a6"
      },
      "source": [
        "LABEL.vocab.stoi"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(None, {'0': 0, '1': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG3-CxkOz50d"
      },
      "source": [
        "### Creating iterators\n",
        "\n",
        "We are going to use the `BucketIterator` to create iterators for all these sets that we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pzygmoay7jG"
      },
      "source": [
        "sort_key = lambda x: len(x.qn1)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    device = device,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = sort_key,\n",
        "    sort_within_batch=True\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUxdqysr0bK-"
      },
      "source": [
        "### Next we are going to create the model.\n",
        "\n",
        "We are going to have two inputs which will be Question1 and Question2.\n",
        "* Each question will be passed through it's own embedding layer.\n",
        "* Then each embedding layer will have an LSTM layer following it that will learn each question seperately and then we will concatenate the learned layers and using the  `torch.cat()` in the forward pass.\n",
        "* We will then concatate the layers and pass down into the fully connected layer where we will learn the parameters before passing it down to the output layerr."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOe3SQ6b0WMz"
      },
      "source": [
        "class DuplicateQuestions(nn.Module):\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               embedding_size,\n",
        "               hidden_size,\n",
        "               output_size,\n",
        "               num_layers,\n",
        "               pad_index,\n",
        "               bidirectional = True,\n",
        "               dropout=.5\n",
        "               ):\n",
        "    super(DuplicateQuestions, self).__init__()\n",
        "    self.embedding_1 = nn.Embedding(\n",
        "        vocab_size,\n",
        "        embedding_size,\n",
        "        padding_idx = pad_index\n",
        "    )\n",
        "    self.embedding_2 = nn.Embedding(\n",
        "        vocab_size,\n",
        "        embedding_size,\n",
        "        padding_idx = pad_index\n",
        "    )\n",
        "    self.lstm = nn.LSTM(\n",
        "        embedding_size,\n",
        "        hidden_size  = hidden_size,\n",
        "        bidirectional = bidirectional,\n",
        "        num_layers = num_layers,\n",
        "        dropout = dropout\n",
        "    )\n",
        "    self.fc_1 = nn.Linear(\n",
        "        hidden_size * 2 if bidirectional else hidden_size  ,\n",
        "        out_features = 512\n",
        "    )\n",
        "    self.fc_2 = nn.Linear(\n",
        "        512 * 2,\n",
        "        out_features = 256\n",
        "    )\n",
        "    \"\"\"\n",
        "    Why 512 * 2?\n",
        "    We are going to concatenate the two learned outputs from the first \n",
        "    feature(qn1) and the second feature(qn2).\n",
        "    \"\"\"\n",
        "    self.out = nn.Linear(\n",
        "        256,\n",
        "        out_features = output_size\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self,\n",
        "              question1, \n",
        "              question1_lengths,\n",
        "              question2, \n",
        "              question2_lengths,\n",
        "              ):\n",
        "    embedded_1 = self.dropout(self.embedding_1(question1))\n",
        "    embedded_2 = self.dropout(self.embedding_2(question2))\n",
        "\n",
        "    packed_embedded_1 = nn.utils.rnn.pack_padded_sequence(\n",
        "        embedded_1, question1_lengths.to('cpu'), enforce_sorted=False\n",
        "    )\n",
        "    packed_embedded_2 = nn.utils.rnn.pack_padded_sequence(\n",
        "        embedded_2, question2_lengths.to('cpu'), enforce_sorted=False\n",
        "    )\n",
        "    packed_output_1, (h_0_1, c_0_1) = self.lstm(packed_embedded_1)\n",
        "    packed_output_2, (h_0_2, c_0_2) = self.lstm(packed_embedded_2)\n",
        "\n",
        "\n",
        "    output_1, output_lengths_1 = nn.utils.rnn.pad_packed_sequence(packed_output_1)\n",
        "    output_2, output_lengths_2 = nn.utils.rnn.pad_packed_sequence(packed_output_2)\n",
        "\n",
        "    h_0_1 = self.dropout(torch.cat((h_0_1[-2,:,:], h_0_1[-1,:,:]), dim = 1))\n",
        "    h_0_2 = self.dropout(torch.cat((h_0_2[-2,:,:], h_0_2[-1,:,:]), dim = 1))\n",
        "\n",
        "    out_1 = self.dropout(self.fc_1(h_0_1))\n",
        "    out_2 = self.dropout(self.fc_1(h_0_1))\n",
        "    concatenated = self.dropout(torch.cat((out_1, out_2), dim=1))\n",
        "    out = self.dropout(self.fc_2(concatenated))\n",
        "    return self.out(out)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPZB4wPJ8MH5"
      },
      "source": [
        "### Creating the model instance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jemm7xH8LKE",
        "outputId": "c03e4cc8-6ce0-4246-ba57-2f8d1ab40ed1"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM =  1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] \n",
        "\n",
        "duplicate_questions_model = DuplicateQuestions(\n",
        "            INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            N_LAYERS, \n",
        "            bidirectional = BIDIRECTIONAL, \n",
        "            dropout = DROPOUT, \n",
        "            pad_index = PAD_IDX\n",
        "            ).to(device)\n",
        "duplicate_questions_model"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DuplicateQuestions(\n",
              "  (embedding_1): Embedding(100002, 100, padding_idx=1)\n",
              "  (embedding_2): Embedding(100002, 100, padding_idx=1)\n",
              "  (lstm): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc_1): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (fc_2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "  (out): Linear(in_features=256, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF84BeAV8yIc"
      },
      "source": [
        "### Model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8ahZtoC8iMX",
        "outputId": "1c0600a0-5a72-4caf-aaa5-2e59de522815"
      },
      "source": [
        "\n",
        "def count_trainable_params(model):\n",
        "  return sum(p.numel() for p in model.parameters()), sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "n_params, trainable_params = count_trainable_params(duplicate_questions_model)\n",
        "print(f\"Total number of paramaters: {n_params:,}\\nTotal tainable parameters: {trainable_params:,}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of paramaters: 22,835,857\n",
            "Total tainable parameters: 22,835,857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIWPKwjl88OA"
      },
      "source": [
        "### Loading pretrained vectors to the `embedding` layers.\n",
        "* Now we have two embedding layers in the model, so we need to add the word vectors to each embedding layer as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTfwoZlo849_"
      },
      "source": [
        "pretrained_embeddings  = TEXT.vocab.vectors"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr9kpInY9DmE",
        "outputId": "166f8355-ef36-467c-f652-2b1a08a1979a"
      },
      "source": [
        "duplicate_questions_model.embedding_1.weight.data.copy_(\n",
        "    pretrained_embeddings\n",
        "    )\n",
        "duplicate_questions_model.embedding_2.weight.data.copy_(\n",
        "    pretrained_embeddings\n",
        "    )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.1233,  0.3499,  0.6173],\n",
              "        [ 0.7262,  0.0912, -0.3891,  ...,  0.0821,  0.4440, -0.7240],\n",
              "        [ 0.1638,  0.6046,  1.0789,  ..., -0.3140,  0.1844,  0.3624],\n",
              "        ...,\n",
              "        [ 0.1434,  0.8499,  1.2881,  ...,  2.0599, -1.1911,  0.5823],\n",
              "        [ 0.6803, -0.4131,  1.0441,  ...,  0.8616,  0.4945,  0.1403],\n",
              "        [ 0.2489, -0.2695, -0.0509,  ..., -0.3666, -0.1306,  0.8195]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzAbLd5P9iZ6"
      },
      "source": [
        "### Zeroing the `<pad>` and the `<unk>` tokens.\n",
        "\n",
        "These tokens are not acually necessary for the model trainning that's the reason we are zeroing them. We will do this for all our emmbedding layers in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEvzZ5yG9fII",
        "outputId": "851042bf-8e7d-4bcc-bf5f-42344dfb5cb9"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token] or TEXT.vocab.stoi[\"<unk>\"]\n",
        "\n",
        "duplicate_questions_model.embedding_1.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "duplicate_questions_model.embedding_1.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "duplicate_questions_model.embedding_2.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "duplicate_questions_model.embedding_2.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "\n",
        "duplicate_questions_model.embedding_1.weight.data"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.1638,  0.6046,  1.0789,  ..., -0.3140,  0.1844,  0.3624],\n",
              "        ...,\n",
              "        [ 0.1434,  0.8499,  1.2881,  ...,  2.0599, -1.1911,  0.5823],\n",
              "        [ 0.6803, -0.4131,  1.0441,  ...,  0.8616,  0.4945,  0.1403],\n",
              "        [ 0.2489, -0.2695, -0.0509,  ..., -0.3666, -0.1306,  0.8195]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB0sVZ2T-Xiz"
      },
      "source": [
        "### Loss and optimizer\n",
        "For the optimizer we are going to use `Adam()` with default paramaters and for the loss function we are going to use the `BCEWithLogitsLoss()` since we are doing a binary classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDNamG-K976I"
      },
      "source": [
        "optimizer = torch.optim.Adam(duplicate_questions_model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdWa5_qp-_4K"
      },
      "source": [
        "### Accuracy function.\n",
        "For the accuracy we are going to create a `binary_accuracy` function that will take predicted labels and accual labels to return the accuracy as a probability value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF1P4NyX-XQp"
      },
      "source": [
        "def binary_accuracy(y_preds, y_true):\n",
        "  rounded_preds = torch.round(torch.sigmoid(y_preds))\n",
        "  correct = (rounded_preds == y_true).float()\n",
        "  return correct.sum() / len(correct)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEiypTM7_iF9"
      },
      "source": [
        "### Train and evaluation function.\n",
        "This time around we have two features which is our two text labels. The model except 4 positional args whic are:\n",
        "```\n",
        "  question1, \n",
        "  question1_lengths\n",
        "  question2\n",
        "  question2_lengths\n",
        "```\n",
        "### Where are we going to get them?\n",
        "\n",
        "Well our iterator contains all this information so we dont have o worry much about that. Let's create a train and evaluation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8TMKyT5-XM_"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "  epoch_loss,epoch_acc = 0, 0\n",
        "  model.train()\n",
        "  for batch in iterator:\n",
        "    optimizer.zero_grad()\n",
        "    qn1, qn1_lengths = batch.qn1\n",
        "    qn2, qn2_lengths = batch.qn2\n",
        "    try:\n",
        "      predictions = model(qn1, qn1_lengths,\n",
        "                          qn2, qn2_lengths ).squeeze(1)\n",
        "      loss = criterion(predictions, batch.label)\n",
        "      acc = binary_accuracy(predictions, batch.label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "    except:\n",
        "      pass\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "  epoch_loss,epoch_acc = 0, 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch in iterator:\n",
        "      qn1, qn1_lengths = batch.qn1\n",
        "      qn2, qn2_lengths = batch.qn2\n",
        "      predictions = model(qn1, qn1_lengths,\n",
        "                          qn2, qn2_lengths ).squeeze(1)\n",
        "      loss = criterion(predictions, batch.label)\n",
        "      acc = binary_accuracy(predictions, batch.label)\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_Ddb4rDBpeH"
      },
      "source": [
        "### Train Loop\n",
        "\n",
        "We are going to create some helper functions that will help us to visualize every epoch during training.\n",
        "\n",
        "Time to string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81icrKOD-XKU"
      },
      "source": [
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewfk-aIcB8BW"
      },
      "source": [
        "Tabulate training epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSnir_zT-XGi"
      },
      "source": [
        "def visualize_training(start, end, train_loss, train_accuracy, val_loss, val_accuracy, title):\n",
        "  data = [\n",
        "       [\"Training\", f'{train_loss:.3f}', f'{train_accuracy:.3f}', f\"{hms_string(end - start)}\" ],\n",
        "       [\"Validation\", f'{val_loss:.3f}', f'{val_accuracy:.3f}', \"\" ],       \n",
        "  ]\n",
        "  table = PrettyTable([\"CATEGORY\", \"LOSS\", \"ACCURACY\", \"ETA\"])\n",
        "  table.align[\"CATEGORY\"] = 'l'\n",
        "  table.align[\"LOSS\"] = 'r'\n",
        "  table.align[\"ACCURACY\"] = 'r'\n",
        "  table.align[\"ETA\"] = 'r'\n",
        "  table.title = title\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)\n",
        "  "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JdSJnqp-XDO",
        "outputId": "d6fa0c54-21f5-473c-f20b-587852f2c971"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start = time.time()\n",
        "  train_loss, train_acc = train(duplicate_questions_model, train_iter, optimizer, criterion)\n",
        "  valid_loss, valid_acc = evaluate(duplicate_questions_model, val_iter, criterion)\n",
        "  title = f\"EPOCH: {epoch+1:02}/{N_EPOCHS:02} {'saving best model...' if valid_loss < best_valid_loss else 'not saving...'}\"\n",
        "  if valid_loss < best_valid_loss:\n",
        "      best_valid_loss = valid_loss\n",
        "      torch.save(duplicate_questions_model.state_dict(), 'best-model.pt')\n",
        "  end = time.time()\n",
        "  visualize_training(start, end, train_loss, train_acc, valid_loss, valid_acc, title)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------+\n",
            "|     EPOCH: 01/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.570 |    0.707 | 0:03:30.48 |\n",
            "| Validation | 0.522 |    0.747 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 02/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.523 |    0.741 | 0:03:31.43 |\n",
            "| Validation | 0.508 |    0.758 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 03/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.502 |    0.754 | 0:03:31.98 |\n",
            "| Validation | 0.507 |    0.765 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 04/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.486 |    0.764 | 0:03:28.54 |\n",
            "| Validation | 0.490 |    0.768 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 05/10 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.473 |    0.772 | 0:03:27.09 |\n",
            "| Validation | 0.492 |    0.762 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 06/10 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.462 |    0.778 | 0:03:27.70 |\n",
            "| Validation | 0.497 |    0.769 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 07/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.450 |    0.785 | 0:03:27.02 |\n",
            "| Validation | 0.488 |    0.763 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 08/10 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.441 |    0.791 | 0:03:27.03 |\n",
            "| Validation | 0.494 |    0.772 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 09/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.431 |    0.796 | 0:03:26.29 |\n",
            "| Validation | 0.486 |    0.770 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 10/10 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.423 |    0.800 | 0:03:26.51 |\n",
            "| Validation | 0.497 |    0.771 |            |\n",
            "+------------+-------+----------+------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IMcTLBMq-T5"
      },
      "source": [
        "### Evaluating the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOjGeUSMrGMK",
        "outputId": "fe0d0b6e-3be4-41ca-a738-f92c12f43182"
      },
      "source": [
        "duplicate_questions_model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(duplicate_questions_model, test_iter, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.486 | Test Acc: 76.97%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QJBuD65ioTa"
      },
      "source": [
        "### Model Inference\n",
        "\n",
        "Our predict sentiment function will:\n",
        "\n",
        "* get `two` question pairs, tokenize them and convert them to sequences.\n",
        "* We will then get the lengths of each sentence and convert them to tensors.\n",
        "* pass the model the, questions and their lenghts.\n",
        "* Apply the sigmoid to get the accual label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URonyAkn-W8J"
      },
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "def predict_sentiment(model, q1, q2):\n",
        "  model.eval()\n",
        "  tokenized_q1 = [tok.text for tok in nlp.tokenizer(q1)]\n",
        "  tokenized_q2 = [tok.text for tok in nlp.tokenizer(q2)]\n",
        "\n",
        "  indexed_1 = [TEXT.vocab.stoi[t] for t in tokenized_q1]\n",
        "  indexed_2 = [TEXT.vocab.stoi[t] for t in tokenized_q2]\n",
        "\n",
        "  length_1 = [len(indexed_1)]\n",
        "  length_2 = [len(indexed_2)]\n",
        "\n",
        "  tensor_1 = torch.LongTensor(indexed_1).to(device).unsqueeze(1)\n",
        "  tensor_2 = torch.LongTensor(indexed_2).to(device).unsqueeze(1)\n",
        "  \n",
        "  length_tensor_1 = torch.LongTensor(length_1)\n",
        "  length_tensor_2 = torch.LongTensor(length_2)\n",
        "\n",
        "  prediction = torch.sigmoid(model(\n",
        "      tensor_1, length_tensor_1,\n",
        "      tensor_2, length_tensor_2\n",
        "      ))\n",
        "  return prediction.item()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ReNykvykiHn"
      },
      "source": [
        "### Getting questions for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZAPfDiIEcG9"
      },
      "source": [
        "dataframe = pd.read_csv(os.path.join(\n",
        "    base_path,\n",
        "    test_path\n",
        "))\n",
        "\n",
        "qns1 = dataframe.question1.values\n",
        "qns2 = dataframe.question2.values\n",
        "true_labels = dataframe.is_duplicate.values"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjPs4HPakhpV"
      },
      "source": [
        ""
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ0b8lZWkhm9"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "def tabulate(column_names, data, max_characters:int, title:str):\n",
        "  table = PrettyTable(column_names)\n",
        "  table.align[column_names[0]] = \"l\"\n",
        "  table.align[column_names[1]] = \"l\"\n",
        "  table.title = title\n",
        "  table._max_width = {column_names[0] :max_characters, column_names[1] :max_characters}\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L-s8DwykhkG",
        "outputId": "0f92cc50-be5c-4e0e-e909-e1bd60a31f75"
      },
      "source": [
        "\n",
        "for i, (q1, q2, label) in enumerate(zip(qns1, qns2, true_labels[:10])):\n",
        "  pred = predict_sentiment(duplicate_questions_model, q1, q2)\n",
        "  classes = [\"not duplicate\", \"duplicate\"]\n",
        "  probability = pred if pred >=0.5 else 1 - pred\n",
        "  table_headers =[\"KEY\", \"VALUE\"]\n",
        "  table_data = [\n",
        "        [\"Question 1\", q1],\n",
        "        [\"Question2\", q2],\n",
        "        [\"PREDICTED CLASS\",  round(pred)],\n",
        "        [\"PREDICTED CLASS NAME\",  classes[round(pred)]],\n",
        "        [\"REAL CLASS\",  label],\n",
        "        [\"REAL CLASS NAME\",  classes[label]],\n",
        "        [\"CONFIDENCE OVER OTHER CLASSES\", f'{ probability * 100:.2f}%'],\n",
        "             \n",
        "    ]\n",
        "  title = \"Duplicate Questions\"\n",
        "  tabulate(table_headers, table_data, 50, title=title)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------------------+\n",
            "|                                Duplicate Questions                                 |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "| KEY                           | VALUE                                              |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "| Question 1                    | Do you watch Korean dramas?                        |\n",
            "| Question2                     | Is it normal to watch Korean drama if you are a    |\n",
            "|                               | guy?                                               |\n",
            "| PREDICTED CLASS               | 0                                                  |\n",
            "| PREDICTED CLASS NAME          | not duplicate                                      |\n",
            "| REAL CLASS                    | 0                                                  |\n",
            "| REAL CLASS NAME               | not duplicate                                      |\n",
            "| CONFIDENCE OVER OTHER CLASSES | 56.08%                                             |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "+------------------------------------------------------------------------------------+\n",
            "|                                Duplicate Questions                                 |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "| KEY                           | VALUE                                              |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "| Question 1                    | What are some good home remedies for getting rid   |\n",
            "|                               | of stress bumps on the lips?                       |\n",
            "| Question2                     | How do I get rid of an acidic tummy and a sore     |\n",
            "|                               | mouth? Is there any home remedies?                 |\n",
            "| PREDICTED CLASS               | 0                                                  |\n",
            "| PREDICTED CLASS NAME          | not duplicate                                      |\n",
            "| REAL CLASS                    | 0                                                  |\n",
            "| REAL CLASS NAME               | not duplicate                                      |\n",
            "| CONFIDENCE OVER OTHER CLASSES | 94.64%                                             |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "+------------------------------------------------------------------------------------+\n",
            "|                                Duplicate Questions                                 |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "| KEY                           | VALUE                                              |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "| Question 1                    | “Everyone wants to go to Baghdad. Real men want to |\n",
            "|                               | go to Tehran.” What does this mean?                |\n",
            "| Question2                     | Why do you want to go back to college days?        |\n",
            "| PREDICTED CLASS               | 0                                                  |\n",
            "| PREDICTED CLASS NAME          | not duplicate                                      |\n",
            "| REAL CLASS                    | 0                                                  |\n",
            "| REAL CLASS NAME               | not duplicate                                      |\n",
            "| CONFIDENCE OVER OTHER CLASSES | 98.10%                                             |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "+-------------------------------------------------------------------+\n",
            "|                        Duplicate Questions                        |\n",
            "+-------------------------------+-----------------------------------+\n",
            "| KEY                           | VALUE                             |\n",
            "+-------------------------------+-----------------------------------+\n",
            "| Question 1                    | How can I ask my wife for sex?    |\n",
            "| Question2                     | Do I have to ask my wife for sex? |\n",
            "| PREDICTED CLASS               | 0                                 |\n",
            "| PREDICTED CLASS NAME          | not duplicate                     |\n",
            "| REAL CLASS                    | 0                                 |\n",
            "| REAL CLASS NAME               | not duplicate                     |\n",
            "| CONFIDENCE OVER OTHER CLASSES | 57.58%                            |\n",
            "+-------------------------------+-----------------------------------+\n",
            "+------------------------------------------------------------------------------------+\n",
            "|                                Duplicate Questions                                 |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "| KEY                           | VALUE                                              |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "| Question 1                    | How do you deal with having a bad reputation in    |\n",
            "|                               | college?                                           |\n",
            "| Question2                     | How can I deal with bad reputation in college?     |\n",
            "| PREDICTED CLASS               | 0                                                  |\n",
            "| PREDICTED CLASS NAME          | not duplicate                                      |\n",
            "| REAL CLASS                    | 0                                                  |\n",
            "| REAL CLASS NAME               | not duplicate                                      |\n",
            "| CONFIDENCE OVER OTHER CLASSES | 78.58%                                             |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "+------------------------------------------------------------------------------------+\n",
            "|                                Duplicate Questions                                 |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "| KEY                           | VALUE                                              |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "| Question 1                    | What credit card is the one that you pay the       |\n",
            "|                               | least?                                             |\n",
            "| Question2                     | What are the consiquences of not paying the credit |\n",
            "|                               | card?                                              |\n",
            "| PREDICTED CLASS               | 0                                                  |\n",
            "| PREDICTED CLASS NAME          | not duplicate                                      |\n",
            "| REAL CLASS                    | 0                                                  |\n",
            "| REAL CLASS NAME               | not duplicate                                      |\n",
            "| CONFIDENCE OVER OTHER CLASSES | 76.16%                                             |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "+----------------------------------------------------------------------+\n",
            "|                         Duplicate Questions                          |\n",
            "+-------------------------------+--------------------------------------+\n",
            "| KEY                           | VALUE                                |\n",
            "+-------------------------------+--------------------------------------+\n",
            "| Question 1                    | Does Elon Musk have a lack of focus? |\n",
            "| Question2                     | What is the origin of the Drama?     |\n",
            "| PREDICTED CLASS               | 0                                    |\n",
            "| PREDICTED CLASS NAME          | not duplicate                        |\n",
            "| REAL CLASS                    | 0                                    |\n",
            "| REAL CLASS NAME               | not duplicate                        |\n",
            "| CONFIDENCE OVER OTHER CLASSES | 53.05%                               |\n",
            "+-------------------------------+--------------------------------------+\n",
            "+------------------------------------------------------------------------------------+\n",
            "|                                Duplicate Questions                                 |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "| KEY                           | VALUE                                              |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "| Question 1                    | What universities does Investors Real estate       |\n",
            "|                               | recruit new grads from? What majors are they       |\n",
            "|                               | looking for?                                       |\n",
            "| Question2                     | What universities does Renaissance Real estate     |\n",
            "|                               | recruit new grads from? What majors are they       |\n",
            "|                               | looking for?                                       |\n",
            "| PREDICTED CLASS               | 0                                                  |\n",
            "| PREDICTED CLASS NAME          | not duplicate                                      |\n",
            "| REAL CLASS                    | 0                                                  |\n",
            "| REAL CLASS NAME               | not duplicate                                      |\n",
            "| CONFIDENCE OVER OTHER CLASSES | 100.00%                                            |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "+------------------------------------------------------------------------------------+\n",
            "|                                Duplicate Questions                                 |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "| KEY                           | VALUE                                              |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "| Question 1                    | Could God who is truly all powerful create a rock  |\n",
            "|                               | that he himself could not lift?                    |\n",
            "| Question2                     | If God is all powerful can he make a rock so heavy |\n",
            "|                               | even he cannot lift it?                            |\n",
            "| PREDICTED CLASS               | 1                                                  |\n",
            "| PREDICTED CLASS NAME          | duplicate                                          |\n",
            "| REAL CLASS                    | 1                                                  |\n",
            "| REAL CLASS NAME               | duplicate                                          |\n",
            "| CONFIDENCE OVER OTHER CLASSES | 93.23%                                             |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "+------------------------------------------------------------------------------------+\n",
            "|                                Duplicate Questions                                 |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "| KEY                           | VALUE                                              |\n",
            "+-------------------------------+----------------------------------------------------+\n",
            "| Question 1                    | How do I convey my mom (single mother) that i      |\n",
            "|                               | want/need to get married asap indirectly? Please   |\n",
            "|                               | help                                               |\n",
            "| Question2                     | How do I convey my parents that they need to let   |\n",
            "|                               | me take my own decisions?                          |\n",
            "| PREDICTED CLASS               | 0                                                  |\n",
            "| PREDICTED CLASS NAME          | not duplicate                                      |\n",
            "| REAL CLASS                    | 0                                                  |\n",
            "| REAL CLASS NAME               | not duplicate                                      |\n",
            "| CONFIDENCE OVER OTHER CLASSES | 99.67%                                             |\n",
            "+-------------------------------+----------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKQW_sudmrj0"
      },
      "source": [
        "### Conclusion.\n",
        "\n",
        "We have learned how to create a model that maps 2 inputs to one input. What's Next?\n",
        "\n",
        "### Next.\n",
        "In the next notebook we are going to do the same task with different the modified `FastText`. We are going to use this notbook as the base and then we expand it."
      ]
    }
  ]
}