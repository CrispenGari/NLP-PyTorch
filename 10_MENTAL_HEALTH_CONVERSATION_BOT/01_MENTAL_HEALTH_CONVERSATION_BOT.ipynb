{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "\n",
        "**project**: `Mental Health Conversation Bot (MHCB)`\n",
        "\n",
        "**date**: `2022-12-14`\n",
        "\n",
        "**decription`**: `This notebook represent the code for data preparation all the way to model traing of an AI chatbot that will behave and chat with human like a therapist.`\n",
        "\n",
        "**main**: `Natural Language Processing (NLP) pytorch`\n",
        "\n",
        "**programmer**: `crispengari`\n",
        "\n",
        "**architecture**: `BiDirectional Long Short Term Memory [BiLSTM] (torchtext)`\n",
        "\n",
        "**language:** `python` \n",
        "\n",
        "____\n",
        "\n",
        "\n",
        "### Problem Statement\n",
        "\"The world have been facing problem in shortage of labour, using chatbots intergated with modern deep learning in the field of artificial intelligence, bots can be created and behave like humans.\"\n",
        "\n",
        "In this project I will create a chatbot that will behave and converate with people like a therapist.\n",
        "\n",
        "### Data\n",
        "The dataset that we going to use in this notebook we be comming from [kaggle](https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data). We are going to generate our dataset based on the file called `intents.json` which is a file that contains `80` intents. We are going to save the prepared data into `.csv` files for three sets which are:\n",
        "\n",
        "1. train\n",
        "2. test\n",
        "3. validation\n",
        "\n",
        "\n",
        "### Model Architecture\n",
        "We are going to use `BiLSTM` in doing `multi-class` classifications of intents for our `Bot`. We are going to use the following notebook as reference:\n",
        "\n",
        "> [03_BiLSTM_CUSTOM_EMBEDDING_VECTORS](https://github.com/CrispenGari/torchtext/blob/main/sentiment-analyisis/03_BiLSTM_CUSTOM_EMBEDDING_VECTORS.ipynb)\n",
        "\n",
        "### Installing Helper Packages\n",
        "In the following code cell we are going to install the package called `helperfns` that provide us with some usefull helper functions for machine learning."
      ],
      "metadata": {
        "id": "_e0eOlNyp0Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install helperfns -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZEmuYO1pSCx",
        "outputId": "aa91a4c3-9e8c-411f-b656-01021e360418"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports\n",
        "\n",
        "In  the following code cell we are going to import all the packages that we are going to use throughout this `notebook`"
      ],
      "metadata": {
        "id": "-P5QiqDJtK37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "import torchtext\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "from torchtext import data\n",
        "from collections import Counter\n",
        "from torchtext import vocab\n",
        "\n",
        "from helperfns.tables import tabulate_data\n",
        "from helperfns.visualization import plot_complicated_confusion_matrix, plot_simple_confusion_matrix\n",
        "from helperfns.torch import models\n",
        "from helperfns.utils import hms_string\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive, files\n",
        "\n",
        "torchtext.__version__, torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXoDyT2RpR-0",
        "outputId": "91bd44a4-3fff-4240-cf09-36067d238760"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('0.14.0', '1.13.0+cu116')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seed\n",
        "In the following code cell we are going to set the seed to all random operations for reproducivity."
      ],
      "metadata": {
        "id": "cVqGu8UDTkOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "4g7FAa9ETrDA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Device\n",
        "In the following code cell we are going to get `gpu` device if possible"
      ],
      "metadata": {
        "id": "fx6BtsYvTrtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4mXa6L0Tx4O",
        "outputId": "eede4c93-5a52-4e10-8aa3-5f4b41fcd162"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "Our dataset that we are going to use will be comming from [`kaggle`](https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data) and will be loaded from google drive  where i uploaded it in a folder called `MHCB`. So in the following code cell we are going to mount our google drive to this colab instance."
      ],
      "metadata": {
        "id": "eDiHxRiMtxgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2XqK2PNlpR8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfaf2eb1-0ff4-492c-c4fc-9673d3753bf6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Path to the dataset.\n",
        "Now we can define the path as a variable to the location where our `intents.json` file is located in the following codecell."
      ],
      "metadata": {
        "id": "ciiieVigOGCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/drive/My Drive/NLP Data/MHCB\"\n",
        "\n",
        "assert os.path.exists(base_dir), f\"The path '{base_dir}' does not exists.\"\n",
        "\n",
        "intents_path = os.path.join(base_dir, 'intents.json')\n",
        "\n",
        "assert os.path.exists(intents_path), f\"The path '{intents_path}' does not exists.\""
      ],
      "metadata": {
        "id": "CEQ9QayBpR0L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we need to read the `intents.json` file and create a classification dataset from it."
      ],
      "metadata": {
        "id": "uhN_RheqQfGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(intents_path, \"r\") as f:\n",
        "  intents_data = json.load(f)"
      ],
      "metadata": {
        "id": "TmEiCSKDpRxq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of as single intent in that file looks as follows:\n",
        "\n",
        "```json\n",
        "{\n",
        "\"tag\": \"greeting\",\n",
        "\"patterns\": [\n",
        "  \"Hi\",\n",
        "  \"Hey\",\n",
        "  \"Is anyone there?\",\n",
        "  \"Hi there\",\n",
        "  \"Hello\",\n",
        "  \"Hey there\",\n",
        "  \"Howdy\",\n",
        "  \"Hola\",\n",
        "  \"Bonjour\",\n",
        "  \"Konnichiwa\",\n",
        "  \"Guten tag\",\n",
        "  \"Ola\"\n",
        "],\n",
        "\"responses\": [\n",
        "  \"Hello there. Tell me how are you feeling today?\",\n",
        "  \"Hi there. What brings you here today?\",\n",
        "  \"Hi there. How are you feeling today?\",\n",
        "  \"Great to see you. How do you feel currently?\",\n",
        "  \"Hello there. Glad to see you're back. What's going on in your world right now?\"\n",
        "]\n",
        "},\n",
        "```\n",
        "So what we are intrested in for model training is `patterns` and it's tag, so our target value that we are trying to predict is a `tag` given a certain `pattern`. So from this we are going to generate our data maped with all the `pattens` to their labels."
      ],
      "metadata": {
        "id": "z2rGLv9-Q637"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = list()\n",
        "for intent in intents_data.get('intents'):\n",
        "  label = intent.get('tag').lower()\n",
        "  for pattern in intent.get('patterns'):\n",
        "    feature = pattern.lower()\n",
        "    dataset.append((feature, label))\n",
        "\n",
        "print(\"Dataset size: {}\".format(len(dataset)))"
      ],
      "metadata": {
        "id": "OzuqHHnzpRuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee3c5b9-9bb5-4382-d1d7-184400e279d5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a small dataset that contains `232` examples, let's check the first `10` examples in the dataset:"
      ],
      "metadata": {
        "id": "ZmXJtTEsTAA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[:10]"
      ],
      "metadata": {
        "id": "mYtE32C2pRr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb72009e-690e-4986-d69f-1ec99e7a7ece"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hi', 'greeting'),\n",
              " ('hey', 'greeting'),\n",
              " ('is anyone there?', 'greeting'),\n",
              " ('hi there', 'greeting'),\n",
              " ('hello', 'greeting'),\n",
              " ('hey there', 'greeting'),\n",
              " ('howdy', 'greeting'),\n",
              " ('hola', 'greeting'),\n",
              " ('bonjour', 'greeting'),\n",
              " ('konnichiwa', 'greeting')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we are going to use the `random` module to shuffle our dataset and then check again the size first `10` examples before creating dataframes."
      ],
      "metadata": {
        "id": "miWuxr3nTTxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(dataset)"
      ],
      "metadata": {
        "id": "JjUTIfDIpRpE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OvbVPUrrnWRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "655a1748-0f09-444a-aa1a-b143b2ab920f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('financial problems', 'default'),\n",
              " ('that sounds awful. what do i do?', 'scared'),\n",
              " ('konnichiwa', 'greeting'),\n",
              " ('boyfriend', 'default'),\n",
              " ('support me please', 'help'),\n",
              " ('wrong response', 'wrong'),\n",
              " (\"i'm scared\", 'scared'),\n",
              " ('i feel ok', 'happy'),\n",
              " ('define depression', 'fact-3'),\n",
              " ('what does it mean to have a mental illness?', 'fact-8')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dataset[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So for our train data we are going to take all the examples in the `dataset` and then for the `validation` and `testing` set we are going to take a fraction of `40%` and `60%` from the dataset respectively."
      ],
      "metadata": {
        "id": "qn6nF_4JUVMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_df = pd.DataFrame(dataset, columns=[\"text\", \"label\" ])\n",
        "\n",
        "TEST_EXAMPLES = int(.6 * len(dataset))\n",
        "\n",
        "random.shuffle(dataset)\n",
        "test_df = pd.DataFrame(dataset[:TEST_EXAMPLES], columns=[\"text\", \"label\" ])\n",
        "val_df = pd.DataFrame(dataset[TEST_EXAMPLES: ], columns=[\"text\", \"label\" ])"
      ],
      "metadata": {
        "id": "fbwMyFBzT8y_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking our dataframes.\n",
        "\n",
        "\n",
        "1. train dataframe"
      ],
      "metadata": {
        "id": "ShVBIiWjU9t_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vtBv4dnjU7tQ",
        "outputId": "1ea976fb-48b6-42ca-f891-0839bfe50237"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               text     label\n",
              "0                financial problems   default\n",
              "1  that sounds awful. what do i do?    scared\n",
              "2                        konnichiwa  greeting\n",
              "3                         boyfriend   default\n",
              "4                 support me please      help"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-956573e5-2436-4795-b7cd-1e39ab3bc263\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>financial problems</td>\n",
              "      <td>default</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>that sounds awful. what do i do?</td>\n",
              "      <td>scared</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>konnichiwa</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>boyfriend</td>\n",
              "      <td>default</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>support me please</td>\n",
              "      <td>help</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-956573e5-2436-4795-b7cd-1e39ab3bc263')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-956573e5-2436-4795-b7cd-1e39ab3bc263 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-956573e5-2436-4795-b7cd-1e39ab3bc263');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. test dataframe"
      ],
      "metadata": {
        "id": "X1ktJffeVIBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZwWqjTLcVHHU",
        "outputId": "c0ca5b78-a156-4425-f226-890123c8dd12"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text             label\n",
              "0                                       nothing much  neutral-response\n",
              "1                                  what's your name?             about\n",
              "2                                        i am happy.             happy\n",
              "3  how can i find a mental health professional fo...           fact-14\n",
              "4                                         not really       no-approach"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ea375c1e-abe2-43b7-ac3c-c326e70667c6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nothing much</td>\n",
              "      <td>neutral-response</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>what's your name?</td>\n",
              "      <td>about</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i am happy.</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>how can i find a mental health professional fo...</td>\n",
              "      <td>fact-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>not really</td>\n",
              "      <td>no-approach</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea375c1e-abe2-43b7-ac3c-c326e70667c6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ea375c1e-abe2-43b7-ac3c-c326e70667c6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ea375c1e-abe2-43b7-ac3c-c326e70667c6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. validatation dataframe"
      ],
      "metadata": {
        "id": "UkqLes8-VNPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "DzNJKAAVVEeL",
        "outputId": "e24f8cf4-2abe-4c68-9ec9-23885d080305"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text               label\n",
              "0           tell me another fact about mental health  mental-health-fact\n",
              "1                                    my brother died               death\n",
              "2                                       nothing else                done\n",
              "3  what are the different types of mental health ...             fact-23\n",
              "4                                   i don't like you            hate-you"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95659d6d-dd51-4458-9af3-5727146dd20d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tell me another fact about mental health</td>\n",
              "      <td>mental-health-fact</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>my brother died</td>\n",
              "      <td>death</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>nothing else</td>\n",
              "      <td>done</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what are the different types of mental health ...</td>\n",
              "      <td>fact-23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i don't like you</td>\n",
              "      <td>hate-you</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95659d6d-dd51-4458-9af3-5727146dd20d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-95659d6d-dd51-4458-9af3-5727146dd20d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-95659d6d-dd51-4458-9af3-5727146dd20d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have text matched to labels, we can go ahead and save the `csv` files for these 3 different sets of data"
      ],
      "metadata": {
        "id": "cvjr1em9VYNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv(os.path.join(base_dir, \"train.csv\"),  index = False, header = True)\n",
        "test_df.to_csv(os.path.join(base_dir, \"test.csv\"),  index = False, header = True)\n",
        "val_df.to_csv(os.path.join(base_dir, \"val.csv\"),  index = False, header = True)\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQlpvRx4VWum",
        "outputId": "12ac7b13-699a-4831-9944-da0938d8b2b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code cell we are going to count the examples that are in each set of our whole dataset."
      ],
      "metadata": {
        "id": "6_0O8AsVV0ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"Set\", \"Example(s)\"]\n",
        "\n",
        "examples = [\n",
        "    ['training', len(train_df)],\n",
        "    ['validation', len(val_df)],\n",
        "    ['testing', len(test_df)],\n",
        "    ['total', len(train_df) +  len(test_df) + len(val_df)],\n",
        "]\n",
        "\n",
        "tabulate_data(columns, examples, \"Exmples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXeApTx7Vp-m",
        "outputId": "536277a0-f6ea-4772-fdc9-dbc299376f9f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+\n",
            "|         Exmples         |\n",
            "+------------+------------+\n",
            "| Set        | Example(s) |\n",
            "+------------+------------+\n",
            "| training   |        232 |\n",
            "| validation |         93 |\n",
            "| testing    |        139 |\n",
            "| total      |        464 |\n",
            "+------------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features and Labels\n",
        "Our fetures are the actual `text` in the dataframes which is the column named `text` and our labels will come from the column called `label`. In the following code cell we are going to read features and labels in a numpy arrays for each set."
      ],
      "metadata": {
        "id": "f5MGCJeBWMr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "train_texts = train_df.text.values\n",
        "train_labels = train_df.label.values\n",
        "\n",
        "# test\n",
        "test_texts = test_df.text.values\n",
        "test_labels = test_df.label.values\n",
        "\n",
        "# val\n",
        "val_texts = val_df.text.values\n",
        "val_labels = val_df.label.values"
      ],
      "metadata": {
        "id": "0OtJMI3tWDxP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Preprocessing\n",
        "In our text processing pipeline we need to do the following steps:\n",
        "\n",
        "1. tokenize sentences\n",
        "* this is the process of converting a sentence or text into senquence of word. For this process we are going to use a pre-trained model from spacy language model. You can read more about other tokenizers that you can use at [pytorch](https://pytorch.org/text/stable/data_utils.html).org.\n",
        "\n",
        "2. vocabulary\n",
        "We will to create a vocabulary based on our sentences that are in the train dataset. A `vocabulary` is esentially a `word` to `index` mapping that allows us to reference the word with their integer representation, since machine leaning models does not understand words. This vocabulary will be used during model training and also can be used at model inference.\n",
        "\n",
        "### Tokenizer\n",
        "In the following code cell we are going to geta a tokenier object that will convert a sentence into a sequence of word using the `spacy-en` language model. The reason we are using the english langauge model it's because our intents are in english."
      ],
      "metadata": {
        "id": "zn4H4nQPWh6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = data.utils.get_tokenizer('spacy', 'en')\n",
        "tokenizer(\"This is a boy.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKzRB6ERWb-Z",
        "outputId": "a9088a39-b5a4-4343-d5b1-f39c655d4864"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'boy', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary\n",
        "In the following code cell we are going to create a `vocabulary` object from torchtext. This vocabulary takes in an `` of words to their count. So we are going to use the `Counter` module from `collections` to generate these counts from our train features.\n",
        "\n",
        "We are going to specify the `min_freq` to `2` meaning that the words that does not appear at least 2 times will be converted to unknown. We are also going to specify the special tokens during creation of the vocabulary object."
      ],
      "metadata": {
        "id": "6eg16gaXXYXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter = Counter()\n",
        "for line in train_texts:\n",
        "    counter.update(tokenizer(line))\n",
        "\n",
        "#  our special tokens are (unknown, padding, start of sentence, end of sentence)\n",
        "vocabulary = vocab.vocab(counter, min_freq=2, specials=('<unk>', '<pad>', '<sos>', '<eos>'))"
      ],
      "metadata": {
        "id": "TGI6C9-8XETl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STOI - String To Integer\n",
        "This will be a dictionary that contains a string to integer mapping which will be our actual vocabulary. In the following code cell we are going to create object called `stoi` which is essentially a dictionary of word to index mapping. This dictionary will be used during training as well as during model inference."
      ],
      "metadata": {
        "id": "cTx0dEomYPXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = vocabulary.get_stoi()"
      ],
      "metadata": {
        "id": "P9qDNOS_XX-K"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Pipeline\n",
        "After our text has been tokenized we need a way of converting those words into numbers because machine leaning models understand numbers not words. That's where we the `text_pipeline` function comes into play. So this function takes in a sentence and tokenize it then converts each word to a number. Note that the word that does not exists in the vocabulay (`stoi`) will be converted to  an unkown `('<unk>')` token (0)."
      ],
      "metadata": {
        "id": "ERWg6ii7ZBR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_pipeline(x: str):\n",
        "  values = list()\n",
        "  tokens = tokenizer(x.lower()) # convert to lower case.\n",
        "  for token in tokens:\n",
        "    try:\n",
        "      v = stoi[token]\n",
        "    except KeyError as e:\n",
        "      v = stoi['<unk>']\n",
        "    values.append(v)\n",
        "  return values"
      ],
      "metadata": {
        "id": "76wRr0h0XX5U"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label pipeline\n",
        "Our labels for now are just textual. We also need to convert these labels into numbers. This is very simple what we need to do is to get all the uniqe labels and then create a `labels_vocab` which is a label to integer representation. which looks as follows:\n",
        "\n",
        "```json\n",
        "{\"default\": 0,\n",
        " \"scared\": 1,\n",
        " \"greeting\": 2,\n",
        " \"help\": 3,\n",
        " \"wrong\": 4,\n",
        " \"happy\": 5,\n",
        " \"fact-3\": 6,\n",
        " \"fact-8\": 7,\n",
        " \"problem\": 8,\n",
        " \"no-response\": 9,\n",
        " \"death\": 10,\n",
        " \"fact-2\": 11,\n",
        " \"user-advice\": 12,\n",
        " \"learn-mental-health\": 13,\n",
        " \"understand\": 14,\n",
        " \"suicide\": 15,\n",
        " \"fact-23\": 16,\n",
        " \"sad\": 17,\n",
        " \"stressed\": 18,\n",
        " \"fact-27\": 19,\n",
        " \"done\": 20,\n",
        " \"casual\": 21,\n",
        " \"fact-6\": 22,\n",
        " \"worthless\": 23,\n",
        " \"goodbye\": 24,\n",
        " \"hate-me\": 25,\n",
        " \"morning\": 26,\n",
        " \"name\": 27,\n",
        " \"depressed\": 28,\n",
        " \"thanks\": 29,\n",
        " \"hate-you\": 30,\n",
        " \"fact-26\": 31,\n",
        " \"neutral-response\": 32,\n",
        " \"creation\": 33,\n",
        " \"fact-10\": 34,\n",
        " \"fact-9\": 35,\n",
        " \"about\": 36,\n",
        " \"sleep\": 37,\n",
        " \"night\": 38,\n",
        " \"fact-20\": 39,\n",
        " \"learn-more\": 40,\n",
        " \"stupid\": 41,\n",
        " \"fact-17\": 42,\n",
        " \"jokes\": 43,\n",
        " \"location\": 44,\n",
        " \"afternoon\": 45,\n",
        " \"no-approach\": 46,\n",
        " \"fact-21\": 47,\n",
        " \"pandora-useful\": 48,\n",
        " \"repeat\": 49,\n",
        " \"fact-5\": 50,\n",
        " \"fact-29\": 51,\n",
        " \"fact-7\": 52,\n",
        " \"not-talking\": 53,\n",
        " \"user-agree\": 54,\n",
        " \"fact-1\": 55,\n",
        " \"something-else\": 56,\n",
        " \"fact-11\": 57,\n",
        " \"fact-32\": 58,\n",
        " \"fact-13\": 59,\n",
        " \"friends\": 60,\n",
        " \"meditation\": 61,\n",
        " \"fact-18\": 62,\n",
        " \"evening\": 63,\n",
        " \"user-meditation\": 64,\n",
        " \"fact-28\": 65,\n",
        " \"anxious\": 66,\n",
        " \"fact-14\": 67,\n",
        " \"fact-24\": 68,\n",
        " \"fact-12\": 69,\n",
        " \"fact-16\": 70,\n",
        " \"fact-25\": 71,\n",
        " \"fact-15\": 72,\n",
        " \"skill\": 73,\n",
        " \"fact-19\": 74,\n",
        " \"fact-22\": 75,\n",
        " \"ask\": 76,\n",
        " \"fact-31\": 77,\n",
        " \"fact-30\": 78,\n",
        " \"mental-health-fact\": 79}\n",
        "```\n",
        "\n",
        "> As you have noticed we have `80` labels which are tags that we need to be able to predict.\n",
        "\n",
        "The `label_pipeline` function will then takes in the label and then returns us an integer representation of that label."
      ],
      "metadata": {
        "id": "7T5oM8aIZqdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_dict = {k: v for v, k in enumerate(train_df.label.unique())}"
      ],
      "metadata": {
        "id": "-xdB9m-cXX0y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_pipeline = lambda x: labels_dict[x]"
      ],
      "metadata": {
        "id": "j8Ub0Wq4XXxd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our vocabularies for labels `labels_dict` and  features `stoi` we can then save thes files as they will be used suring model inference. We are going to save these files as `.json` files."
      ],
      "metadata": {
        "id": "0KSxxSUwbMLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(base_dir, \"vocab.json\"), 'w') as f:\n",
        "  f.write(json.dumps(stoi, indent=2))\n",
        "\n",
        "with open(os.path.join(base_dir, \"labels_dict.json\"), 'w') as f:\n",
        "  f.write(json.dumps(labels_dict, indent=2))\n",
        "\n",
        "print(\"Saved!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kpBXmdMbhOe",
        "outputId": "0688efda-476d-4517-fc66-1c3835ce3762"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained vectors\n",
        "In the following code cell we are going to download the predtrained word vectors. We are going to use the `GloVe.6B.100d`. These are pretrained vectors that were trained with about `~6B` words and have a vector representation of a word in `100` dimension for each word."
      ],
      "metadata": {
        "id": "D-s0S-pza939"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 100\n",
        "glove_vectors = vocab.GloVe('6B', dim=EMBEDDING_DIM)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtrb3ctoXXsW",
        "outputId": "fe60a729-4938-470d-b0c9-e2e2fc1ef0b2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:39, 5.40MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:15<00:00, 25822.32it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Embedding matrix\n",
        "Now that we have our glove vectors we need to costomize them so that they fit our use case. We are going to create an embedding matrix that suits the our vocabulary. So essentially this embedding matrix will be the word to vector mapping for all the words that arein our vocabulary."
      ],
      "metadata": {
        "id": "Kc_GWFnQcYmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(stoi)\n",
        "EMBEDDING_MATRIX= torch.zeros([VOCAB_SIZE, EMBEDDING_DIM])\n",
        "for i, word in enumerate(vocabulary.get_itos()):\n",
        "  EMBEDDING_MATRIX[i] = glove_vectors[word]"
      ],
      "metadata": {
        "id": "BeVKb-33a9dH"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the followig code cell we are going to check the embedding matrix for the word `\"the\"`."
      ],
      "metadata": {
        "id": "y9Tr8ACuciDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_MATRIX[stoi['the']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84DLGctra9aF",
        "outputId": "e9eb8c11-be4c-43f8-c735-b93912b5b87f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n",
              "        -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n",
              "         0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n",
              "         0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n",
              "         0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n",
              "        -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n",
              "        -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,\n",
              "         0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n",
              "         1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n",
              "        -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n",
              "         0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n",
              "         0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n",
              "        -0.5203, -0.1459,  0.8278,  0.2706])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Dataset for Training\n",
        "\n",
        "In the following code cell we are going to create a dataset class called `MHCBataset`. This dataset will takes in the labels and the text of a set."
      ],
      "metadata": {
        "id": "qzFJ0SZZcrhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHCBataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, labels, text):\n",
        "    super(MHCBataset, self).__init__()\n",
        "    self.labels = labels\n",
        "    self.text = text\n",
        "      \n",
        "  def __getitem__(self, index):\n",
        "    return self.labels[index], self.text[index]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.labels)"
      ],
      "metadata": {
        "id": "g3e35_Aza9W6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### collate_fn\n",
        "We are going to create a collate function called `tokenize_batch`. This function actually takes in a `batch` and does the preprocessing of the text and labels. This function will be passed to the `DataLoader` class to do the preprocessing of features and labels.\n",
        "\n",
        "`tokenize_batch` function:\n",
        "\n",
        "* this function takes in a batch in each set and convert the features and labels to integer representation. It goes ahead and `pad` and `truncate` the sequence to the same `length` and returns `labels` and `features`."
      ],
      "metadata": {
        "id": "AP49l5SIc8YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_batch(batch, max_len=100, padding=\"pre\"):\n",
        "  assert padding==\"pre\" or padding==\"post\", \"the padding can be either pre or post\"\n",
        "  labels_list, text_list = [], []\n",
        "  for _label, _text in batch:\n",
        "    labels_list.append(label_pipeline(_label))\n",
        "    text_holder = torch.zeros(max_len, dtype=torch.int32)\n",
        "    processed_text = torch.tensor(text_pipeline(_text.lower()), dtype=torch.int32)\n",
        "    pos = min(max_len, len(processed_text))\n",
        "    if padding == \"pre\":\n",
        "      text_holder[:pos] = processed_text[:pos]\n",
        "    else:\n",
        "      text_holder[-pos:] = processed_text[-pos:]\n",
        "    text_list.append(text_holder.unsqueeze(dim=0))\n",
        "  #  the labels will be torch long tensors since it is a multi-class classification.\n",
        "  return torch.LongTensor(labels_list), torch.cat(text_list, dim=0)"
      ],
      "metadata": {
        "id": "YzOXdOcka9UA"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datasets\n",
        "In the following code cell we are going to create the datasets for all our three sets using the `MHCBataset` class."
      ],
      "metadata": {
        "id": "BftA18_edart"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MHCBataset(train_labels, train_texts)\n",
        "test_dataset = MHCBataset(test_labels, test_texts)\n",
        "val_dataset = MHCBataset(val_labels, val_texts)"
      ],
      "metadata": {
        "id": "tPaNeJOVdVrt"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iterators\n",
        "In the following code cell we are going to create loaders using the `DataLoader` class from `torch.utils.data` for our `3` sets. We are going to use the `batch_size` of `128` and our `collate_function` is `tokenize_batch`. For the validation and testing dataset we are going to set the shuffle to `False` because there's no need fo us to shuffle these examples."
      ],
      "metadata": {
        "id": "jLrPPmq2dpKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=tokenize_batch)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=tokenize_batch)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=tokenize_batch)"
      ],
      "metadata": {
        "id": "aFhmjJ0qdiXk"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking a single Batch Data"
      ],
      "metadata": {
        "id": "khiKQFrJd58l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lbl, txt = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "Pfq_L5KNd-GT"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Labels in the first batch."
      ],
      "metadata": {
        "id": "6EZVZH0hd-C0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lbl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-6-3P76d9_s",
        "outputId": "61b12aac-e5ef-432c-e8d3-b7fb1c5e6fc2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([23, 15, 32, 28, 50, 15, 17, 20,  4,  3, 45, 29,  3,  2, 36, 17, 46, 64,\n",
              "         5, 21, 50, 36, 75, 24,  3, 21,  5, 67,  7, 17, 14, 24, 41,  2, 18,  2,\n",
              "        10, 12, 68, 70, 20, 14, 55,  4, 18, 66, 35,  2, 20, 14,  2, 29, 36, 10,\n",
              "        53, 73,  6, 36,  0,  0, 21, 22, 36, 17, 24, 37, 15,  2, 13, 62, 64, 20,\n",
              "        49, 26, 79,  1, 21,  2,  9, 65, 49, 19, 37, 50,  4, 34, 16, 24, 43,  2,\n",
              "        44, 53,  5, 13, 46, 21, 31, 21, 56, 21, 44, 55,  5, 10, 61, 10, 58, 33,\n",
              "        15,  4, 30, 53, 40,  3, 29,  8, 17, 61,  2, 17, 57,  5, 28, 25, 42, 14,\n",
              "         2,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first sentence in the batch."
      ],
      "metadata": {
        "id": "i-ho13LdeNC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12CNt4ffeM-b",
        "outputId": "5355fa25-3230-4433-9196-dae455a8f836"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 10,  98,  46,   9, 133,   7,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Creation\n",
        "Now that we have our loaders we can now create a model. The model that we are going to create is called `MHCBModel`.  As mentioned we are going to use `BiDirectional Long Short Term Memory (BiLSTM)` to build this model."
      ],
      "metadata": {
        "id": "WlFHj0dBeYlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHCBModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_size, hidden_size, output_size, num_layers\n",
        "               , bidirectional, dropout, pad_idx):\n",
        "    super(MHCBModel, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Sequential(\n",
        "        nn.Embedding(vocab_size, embedding_dim=embedding_size, padding_idx=pad_idx),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "    self.lstm = nn.Sequential(\n",
        "        nn.LSTM(\n",
        "          embedding_size, \n",
        "          hidden_size=hidden_size, \n",
        "          bidirectional=bidirectional, \n",
        "          num_layers=num_layers,\n",
        "          dropout=dropout\n",
        "        )\n",
        "    )\n",
        "    self.out = nn.Sequential(\n",
        "        nn.Linear(hidden_size * 2, out_features=128),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(128, out_features=output_size),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, text, text_lengths):\n",
        "    embedded = self.embedding(text)\n",
        "    # set batch_first=true since input shape has batch_size first and text_lengths to the device.\n",
        "    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False, batch_first=True)\n",
        "    packed_output, (h_0, c_0) = self.lstm(packed_embedded)\n",
        "    output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "    output = torch.cat((h_0[-2,:,:], h_0[-1,:,:]), dim = 1)\n",
        "    return self.out(output)"
      ],
      "metadata": {
        "id": "KUWtbBIceM7d"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Instance\n",
        "In the following code cell we are going to create a model instance."
      ],
      "metadata": {
        "id": "z509c1kIe4b5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(stoi) \n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = len(labels_dict)\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = stoi['<pad>'] \n",
        "mhcb_model = MHCBModel(\n",
        "              INPUT_DIM, \n",
        "              EMBEDDING_DIM, \n",
        "              HIDDEN_DIM, \n",
        "              OUTPUT_DIM, \n",
        "              N_LAYERS, \n",
        "              BIDIRECTIONAL, \n",
        "              DROPOUT, \n",
        "              PAD_IDX\n",
        ").to(device)\n",
        "mhcb_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F0hcAj4eMtn",
        "outputId": "ce07375c-7e5a-420f-8d99-cb12db2da9ca"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MHCBModel(\n",
              "  (embedding): Sequential(\n",
              "    (0): Embedding(154, 100, padding_idx=1)\n",
              "    (1): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (lstm): Sequential(\n",
              "    (0): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  )\n",
              "  (out): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
              "    (1): Dropout(p=0.5, inplace=False)\n",
              "    (2): Linear(in_features=128, out_features=80, bias=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting Model Parameters\n",
        "In the following code cell we are going to count the model parameters."
      ],
      "metadata": {
        "id": "5Q3sHK_rfLch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models.model_params(mhcb_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6aaOSVbd98d",
        "outputId": "6ef8d8b6-281d-4b2b-d22d-71ce8ae602a1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOTAL MODEL PARAMETERS: \t2,401,528\n",
            "TOTAL TRAINABLE PARAMETERS: \t2,401,528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Embedding Vectors\n",
        "In the following code cell we are going to load the pretained custom vectors in our embedding layer. We are going to load the embedding vectors tha suits our data using the `mhcb_model.embedding[0].weight.data.copy_(EMBEDDING_MATRIX)` as follows:"
      ],
      "metadata": {
        "id": "ixhvbZrnfa1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mhcb_model.embedding[0].weight.data.copy_(EMBEDDING_MATRIX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWmuwFOCd959",
        "outputId": "4f3ee659-3768-46d7-b542-b2fca11e2d7f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        ...,\n",
              "        [-0.1953,  0.2023,  0.6120,  ..., -0.1465, -0.7609,  0.5910],\n",
              "        [ 0.3760,  0.3190,  0.9101,  ..., -1.3099, -0.0878,  1.1415],\n",
              "        [-0.1320,  0.2808,  0.7735,  ..., -0.4347,  0.0964,  0.3194]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer and Criterion\n",
        "\n",
        "In the following code cell we are going to define the `optimizer` and `criterion`. For the `optimizer` we are going to use the `Adam` optimizer with default parameters and for the criterion we are going to use the `CrossEntropyLoss()` function since this is a `multi-class` classification."
      ],
      "metadata": {
        "id": "61W2JkF3f0V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(mhcb_model.parameters())\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "id": "bHJL3sJyd93Z"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code cell we are going to create our `categorical_accuracy` function, which is a function that calulates the the catecorical accuracy between the predicted labels and real labels."
      ],
      "metadata": {
        "id": "ifivhpn5gITZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "  top_pred = preds.argmax(1, keepdim = True)\n",
        "  correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "  acc = correct.float() / y.shape[0]\n",
        "  return acc"
      ],
      "metadata": {
        "id": "7LAsdm-DgEbF"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluate functions\n",
        "In the following code cell we are going to create our `train` and `evalute` functions:"
      ],
      "metadata": {
        "id": "JQZ49pfrgSPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "  epoch_loss,epoch_acc = 0, 0\n",
        "  model.train()\n",
        "  for batch in iterator:\n",
        "    y, X = batch\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    lengths = torch.tensor([len(i) for i in X])\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions = model(X, lengths).squeeze(1)\n",
        "    loss = criterion(predictions, y)\n",
        "    acc = categorical_accuracy(predictions, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "  epoch_loss,epoch_acc = 0, 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch in iterator:\n",
        "      y, X = batch\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      lengths = torch.tensor([len(i) for i in X])\n",
        "      predictions = model(X, lengths).squeeze(1)\n",
        "      loss = criterion(predictions, y)\n",
        "      acc = categorical_accuracy(predictions, y)\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "lkZUeIWJgPBx"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop\n",
        "In the following code cell we are going to run the training loop. We are going to save the model when the loss decreased."
      ],
      "metadata": {
        "id": "uSIas6DcgfMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 200\n",
        "MODEL_NAME = 'mhcb-model.pt'\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start = time.time()\n",
        "  train_loss, train_acc = train(mhcb_model, train_loader, optimizer, criterion)\n",
        "  valid_loss, valid_acc = evaluate(mhcb_model, val_loader, criterion)\n",
        "  title = f\"EPOCH: {epoch+1:02}/{N_EPOCHS:02} {'saving best model...' if valid_loss < best_valid_loss else 'not saving...'}\"\n",
        "  if valid_loss < best_valid_loss:\n",
        "      best_valid_loss = valid_loss\n",
        "      torch.save(mhcb_model.state_dict(), MODEL_NAME)\n",
        "  end = time.time()\n",
        "  data = [\n",
        "       [\"Training\", f'{train_loss:.3f}', f'{train_acc:.3f}', f\"{hms_string(end - start)}\" ],\n",
        "       [\"Validation\", f'{valid_loss:.3f}', f'{valid_acc:.3f}', \"\" ],       \n",
        "   ]\n",
        "  columns = [\"CATEGORY\", \"LOSS\", \"ACCURACY\", \"ETA\"]\n",
        "  tabulate_data(columns, data, title)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DLKNbVmgba4",
        "outputId": "b6515e9b-b514-4f5a-f599-0ab447c307eb"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------+\n",
            "|     EPOCH: 01/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.374 |    0.013 | 0:00:01.36 |\n",
            "| Validation | 4.362 |    0.011 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 02/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.362 |    0.026 | 0:00:00.20 |\n",
            "| Validation | 4.342 |    0.032 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 03/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.339 |    0.036 | 0:00:00.18 |\n",
            "| Validation | 4.307 |    0.032 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 04/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.305 |    0.047 | 0:00:00.17 |\n",
            "| Validation | 4.244 |    0.032 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 05/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.213 |    0.066 | 0:00:00.17 |\n",
            "| Validation | 4.166 |    0.054 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 06/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.214 |    0.066 | 0:00:00.17 |\n",
            "| Validation | 4.165 |    0.075 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 07/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.224 |    0.052 | 0:00:00.17 |\n",
            "| Validation | 4.140 |    0.086 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 08/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.169 |    0.074 | 0:00:00.17 |\n",
            "| Validation | 3.997 |    0.065 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 09/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.220 |    0.073 | 0:00:00.17 |\n",
            "| Validation | 4.026 |    0.086 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 10/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.138 |    0.078 | 0:00:00.16 |\n",
            "| Validation | 4.098 |    0.043 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 11/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.196 |    0.034 | 0:00:00.16 |\n",
            "| Validation | 4.082 |    0.043 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 12/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.154 |    0.069 | 0:00:00.15 |\n",
            "| Validation | 4.003 |    0.065 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 13/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.066 |    0.079 | 0:00:00.17 |\n",
            "| Validation | 3.909 |    0.065 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 14/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.057 |    0.087 | 0:00:00.18 |\n",
            "| Validation | 3.861 |    0.054 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 15/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.050 |    0.099 | 0:00:00.18 |\n",
            "| Validation | 3.851 |    0.097 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 16/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.041 |    0.063 | 0:00:00.15 |\n",
            "| Validation | 3.857 |    0.118 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 17/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.998 |    0.124 | 0:00:00.18 |\n",
            "| Validation | 3.829 |    0.108 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 18/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.014 |    0.079 | 0:00:00.18 |\n",
            "| Validation | 3.787 |    0.118 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 19/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.977 |    0.115 | 0:00:00.18 |\n",
            "| Validation | 3.743 |    0.129 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 20/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.996 |    0.102 | 0:00:00.18 |\n",
            "| Validation | 3.690 |    0.140 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 21/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.883 |    0.108 | 0:00:00.18 |\n",
            "| Validation | 3.640 |    0.151 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 22/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.858 |    0.140 | 0:00:00.18 |\n",
            "| Validation | 3.587 |    0.151 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 23/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 4.029 |    0.077 | 0:00:00.17 |\n",
            "| Validation | 3.584 |    0.204 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 24/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.879 |    0.098 | 0:00:00.15 |\n",
            "| Validation | 3.598 |    0.226 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 25/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.838 |    0.117 | 0:00:00.18 |\n",
            "| Validation | 3.574 |    0.194 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 26/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.815 |    0.115 | 0:00:00.18 |\n",
            "| Validation | 3.531 |    0.215 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 27/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.895 |    0.112 | 0:00:00.18 |\n",
            "| Validation | 3.479 |    0.215 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 28/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.802 |    0.121 | 0:00:00.16 |\n",
            "| Validation | 3.440 |    0.226 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 29/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.825 |    0.129 | 0:00:00.17 |\n",
            "| Validation | 3.428 |    0.237 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 30/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.633 |    0.154 | 0:00:00.18 |\n",
            "| Validation | 3.321 |    0.269 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 31/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.717 |    0.168 | 0:00:00.17 |\n",
            "| Validation | 3.249 |    0.290 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 32/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.700 |    0.162 | 0:00:00.18 |\n",
            "| Validation | 3.211 |    0.290 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 33/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.667 |    0.183 | 0:00:00.16 |\n",
            "| Validation | 3.226 |    0.290 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 34/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.722 |    0.132 | 0:00:00.17 |\n",
            "| Validation | 3.203 |    0.344 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 35/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.617 |    0.162 | 0:00:00.17 |\n",
            "| Validation | 3.172 |    0.344 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 36/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.691 |    0.167 | 0:00:00.18 |\n",
            "| Validation | 3.131 |    0.344 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 37/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.708 |    0.161 | 0:00:00.18 |\n",
            "| Validation | 3.129 |    0.323 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 38/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.723 |    0.155 | 0:00:00.17 |\n",
            "| Validation | 3.124 |    0.344 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 39/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.608 |    0.150 | 0:00:00.17 |\n",
            "| Validation | 3.079 |    0.344 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 40/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.692 |    0.131 | 0:00:00.18 |\n",
            "| Validation | 3.054 |    0.355 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 41/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.465 |    0.216 | 0:00:00.18 |\n",
            "| Validation | 2.992 |    0.409 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 42/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.565 |    0.169 | 0:00:00.18 |\n",
            "| Validation | 2.934 |    0.387 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 43/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.524 |    0.216 | 0:00:00.18 |\n",
            "| Validation | 2.929 |    0.398 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 44/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.654 |    0.162 | 0:00:00.18 |\n",
            "| Validation | 2.926 |    0.376 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 45/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.553 |    0.178 | 0:00:00.17 |\n",
            "| Validation | 2.879 |    0.409 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 46/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.541 |    0.168 | 0:00:00.18 |\n",
            "| Validation | 2.837 |    0.409 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 47/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.333 |    0.258 | 0:00:00.18 |\n",
            "| Validation | 2.820 |    0.387 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 48/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.292 |    0.249 | 0:00:00.16 |\n",
            "| Validation | 2.823 |    0.430 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 49/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.450 |    0.224 | 0:00:00.17 |\n",
            "| Validation | 2.816 |    0.409 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 50/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.369 |    0.238 | 0:00:00.18 |\n",
            "| Validation | 2.716 |    0.419 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 51/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.308 |    0.256 | 0:00:00.18 |\n",
            "| Validation | 2.638 |    0.441 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 52/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.456 |    0.185 | 0:00:00.18 |\n",
            "| Validation | 2.581 |    0.462 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 53/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.477 |    0.217 | 0:00:00.16 |\n",
            "| Validation | 2.625 |    0.430 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 54/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.492 |    0.212 | 0:00:00.16 |\n",
            "| Validation | 2.686 |    0.473 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 55/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.597 |    0.193 | 0:00:00.16 |\n",
            "| Validation | 2.722 |    0.570 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 56/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.427 |    0.241 | 0:00:00.17 |\n",
            "| Validation | 2.717 |    0.613 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 57/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.175 |    0.289 | 0:00:00.16 |\n",
            "| Validation | 2.642 |    0.570 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 58/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.456 |    0.244 | 0:00:00.18 |\n",
            "| Validation | 2.578 |    0.570 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 59/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.292 |    0.273 | 0:00:00.17 |\n",
            "| Validation | 2.518 |    0.602 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 60/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.345 |    0.240 | 0:00:00.17 |\n",
            "| Validation | 2.483 |    0.624 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 61/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.132 |    0.283 | 0:00:00.18 |\n",
            "| Validation | 2.452 |    0.581 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 62/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.263 |    0.278 | 0:00:00.18 |\n",
            "| Validation | 2.428 |    0.591 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 63/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.364 |    0.258 | 0:00:00.19 |\n",
            "| Validation | 2.383 |    0.624 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 64/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.127 |    0.291 | 0:00:00.18 |\n",
            "| Validation | 2.325 |    0.613 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 65/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.186 |    0.275 | 0:00:00.18 |\n",
            "| Validation | 2.278 |    0.634 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 66/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.198 |    0.255 | 0:00:00.18 |\n",
            "| Validation | 2.262 |    0.613 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 67/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.091 |    0.308 | 0:00:00.16 |\n",
            "| Validation | 2.263 |    0.613 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 68/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.162 |    0.275 | 0:00:00.15 |\n",
            "| Validation | 2.297 |    0.634 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 69/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.172 |    0.276 | 0:00:00.16 |\n",
            "| Validation | 2.300 |    0.645 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 70/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.065 |    0.321 | 0:00:00.18 |\n",
            "| Validation | 2.226 |    0.645 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 71/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.135 |    0.323 | 0:00:00.18 |\n",
            "| Validation | 2.193 |    0.656 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 72/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.052 |    0.319 | 0:00:00.18 |\n",
            "| Validation | 2.162 |    0.677 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 73/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.057 |    0.308 | 0:00:00.18 |\n",
            "| Validation | 2.119 |    0.634 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 74/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.002 |    0.313 | 0:00:00.18 |\n",
            "| Validation | 2.082 |    0.667 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 75/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.097 |    0.302 | 0:00:00.18 |\n",
            "| Validation | 2.067 |    0.667 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 76/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.891 |    0.332 | 0:00:00.18 |\n",
            "| Validation | 2.042 |    0.656 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 77/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.994 |    0.347 | 0:00:00.18 |\n",
            "| Validation | 2.020 |    0.720 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 78/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.119 |    0.296 | 0:00:00.19 |\n",
            "| Validation | 2.005 |    0.731 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 79/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.857 |    0.325 | 0:00:00.17 |\n",
            "| Validation | 1.963 |    0.710 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 80/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.871 |    0.377 | 0:00:00.18 |\n",
            "| Validation | 1.906 |    0.763 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 81/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.709 |    0.409 | 0:00:00.18 |\n",
            "| Validation | 1.880 |    0.774 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 82/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.823 |    0.381 | 0:00:00.17 |\n",
            "| Validation | 1.853 |    0.796 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 83/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.912 |    0.352 | 0:00:00.16 |\n",
            "| Validation | 1.867 |    0.796 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 84/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 3.002 |    0.353 | 0:00:00.18 |\n",
            "| Validation | 1.843 |    0.817 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 85/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.747 |    0.378 | 0:00:00.18 |\n",
            "| Validation | 1.807 |    0.828 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 86/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.963 |    0.337 | 0:00:00.17 |\n",
            "| Validation | 1.792 |    0.828 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 87/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.723 |    0.361 | 0:00:00.18 |\n",
            "| Validation | 1.773 |    0.828 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 88/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.743 |    0.391 | 0:00:00.18 |\n",
            "| Validation | 1.713 |    0.839 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 89/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.759 |    0.397 | 0:00:00.18 |\n",
            "| Validation | 1.675 |    0.785 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 90/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.521 |    0.447 | 0:00:00.18 |\n",
            "| Validation | 1.636 |    0.774 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 91/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.911 |    0.321 | 0:00:00.18 |\n",
            "| Validation | 1.597 |    0.806 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 92/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.868 |    0.374 | 0:00:00.18 |\n",
            "| Validation | 1.573 |    0.839 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 93/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.830 |    0.362 | 0:00:00.18 |\n",
            "| Validation | 1.571 |    0.785 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 94/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.604 |    0.396 | 0:00:00.18 |\n",
            "| Validation | 1.558 |    0.796 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 95/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.797 |    0.360 | 0:00:00.17 |\n",
            "| Validation | 1.525 |    0.817 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 96/200 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.935 |    0.321 | 0:00:00.16 |\n",
            "| Validation | 1.526 |    0.828 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 97/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.829 |    0.389 | 0:00:00.18 |\n",
            "| Validation | 1.512 |    0.806 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 98/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.701 |    0.366 | 0:00:00.18 |\n",
            "| Validation | 1.492 |    0.806 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 99/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.704 |    0.366 | 0:00:00.18 |\n",
            "| Validation | 1.488 |    0.828 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 100/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.713 |    0.384 | 0:00:00.18 |\n",
            "| Validation | 1.466 |    0.817 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 101/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.547 |    0.425 | 0:00:00.18 |\n",
            "| Validation | 1.429 |    0.828 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 102/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.792 |    0.364 | 0:00:00.18 |\n",
            "| Validation | 1.395 |    0.828 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 103/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.761 |    0.382 | 0:00:00.17 |\n",
            "| Validation | 1.369 |    0.828 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 104/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.661 |    0.406 | 0:00:00.15 |\n",
            "| Validation | 1.360 |    0.839 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 105/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.433 |    0.482 | 0:00:00.18 |\n",
            "| Validation | 1.336 |    0.849 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 106/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.606 |    0.405 | 0:00:00.18 |\n",
            "| Validation | 1.296 |    0.839 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 107/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.699 |    0.403 | 0:00:00.18 |\n",
            "| Validation | 1.291 |    0.828 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 108/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.507 |    0.460 | 0:00:00.19 |\n",
            "| Validation | 1.276 |    0.828 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 109/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.568 |    0.412 | 0:00:00.20 |\n",
            "| Validation | 1.262 |    0.849 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 110/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.599 |    0.407 | 0:00:00.18 |\n",
            "| Validation | 1.244 |    0.849 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 111/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.594 |    0.408 | 0:00:00.18 |\n",
            "| Validation | 1.205 |    0.860 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 112/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.554 |    0.428 | 0:00:00.18 |\n",
            "| Validation | 1.173 |    0.860 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 113/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.530 |    0.435 | 0:00:00.17 |\n",
            "| Validation | 1.175 |    0.849 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 114/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.703 |    0.388 | 0:00:00.17 |\n",
            "| Validation | 1.164 |    0.860 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 115/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.645 |    0.412 | 0:00:00.18 |\n",
            "| Validation | 1.130 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 116/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.669 |    0.389 | 0:00:00.18 |\n",
            "| Validation | 1.096 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 117/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.552 |    0.428 | 0:00:00.18 |\n",
            "| Validation | 1.087 |    0.882 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 118/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.759 |    0.387 | 0:00:00.16 |\n",
            "| Validation | 1.093 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 119/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.543 |    0.402 | 0:00:00.16 |\n",
            "| Validation | 1.122 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 120/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.460 |    0.431 | 0:00:00.16 |\n",
            "| Validation | 1.131 |    0.882 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 121/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.737 |    0.394 | 0:00:00.16 |\n",
            "| Validation | 1.117 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 122/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.377 |    0.474 | 0:00:00.18 |\n",
            "| Validation | 1.077 |    0.871 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 123/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.575 |    0.393 | 0:00:00.18 |\n",
            "| Validation | 1.045 |    0.871 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 124/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.577 |    0.420 | 0:00:00.18 |\n",
            "| Validation | 1.018 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 125/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.308 |    0.458 | 0:00:00.18 |\n",
            "| Validation | 0.990 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 126/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.330 |    0.483 | 0:00:00.18 |\n",
            "| Validation | 0.980 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 127/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.507 |    0.425 | 0:00:00.16 |\n",
            "| Validation | 0.998 |    0.882 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 128/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.356 |    0.474 | 0:00:00.16 |\n",
            "| Validation | 0.997 |    0.871 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 129/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.446 |    0.445 | 0:00:00.18 |\n",
            "| Validation | 0.958 |    0.871 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 130/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.455 |    0.424 | 0:00:00.18 |\n",
            "| Validation | 0.928 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 131/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.437 |    0.443 | 0:00:00.18 |\n",
            "| Validation | 0.922 |    0.871 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 132/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.334 |    0.473 | 0:00:00.18 |\n",
            "| Validation | 0.916 |    0.849 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 133/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.414 |    0.451 | 0:00:00.16 |\n",
            "| Validation | 0.922 |    0.882 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 134/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.443 |    0.428 | 0:00:00.16 |\n",
            "| Validation | 0.940 |    0.860 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 135/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.501 |    0.416 | 0:00:00.16 |\n",
            "| Validation | 0.954 |    0.871 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 136/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.412 |    0.460 | 0:00:00.16 |\n",
            "| Validation | 0.950 |    0.860 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 137/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.289 |    0.476 | 0:00:00.16 |\n",
            "| Validation | 0.917 |    0.860 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 138/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.338 |    0.472 | 0:00:00.18 |\n",
            "| Validation | 0.901 |    0.871 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 139/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.359 |    0.448 | 0:00:00.18 |\n",
            "| Validation | 0.884 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 140/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.486 |    0.446 | 0:00:00.18 |\n",
            "| Validation | 0.870 |    0.882 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 141/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.373 |    0.468 | 0:00:00.19 |\n",
            "| Validation | 0.832 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 142/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.433 |    0.427 | 0:00:00.18 |\n",
            "| Validation | 0.800 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 143/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.459 |    0.414 | 0:00:00.19 |\n",
            "| Validation | 0.786 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 144/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.276 |    0.489 | 0:00:00.16 |\n",
            "| Validation | 0.788 |    0.882 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 145/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.328 |    0.462 | 0:00:00.18 |\n",
            "| Validation | 0.780 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 146/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.331 |    0.452 | 0:00:00.18 |\n",
            "| Validation | 0.772 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 147/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.169 |    0.485 | 0:00:00.18 |\n",
            "| Validation | 0.764 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 148/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.376 |    0.454 | 0:00:00.19 |\n",
            "| Validation | 0.759 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 149/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.147 |    0.497 | 0:00:00.18 |\n",
            "| Validation | 0.750 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 150/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.502 |    0.408 | 0:00:00.18 |\n",
            "| Validation | 0.739 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 151/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.246 |    0.462 | 0:00:00.18 |\n",
            "| Validation | 0.726 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 152/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.342 |    0.470 | 0:00:00.18 |\n",
            "| Validation | 0.711 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 153/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.324 |    0.460 | 0:00:00.18 |\n",
            "| Validation | 0.702 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 154/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.331 |    0.447 | 0:00:00.18 |\n",
            "| Validation | 0.689 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 155/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.257 |    0.468 | 0:00:00.18 |\n",
            "| Validation | 0.678 |    0.914 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 156/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.295 |    0.469 | 0:00:00.18 |\n",
            "| Validation | 0.676 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 157/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.260 |    0.469 | 0:00:00.16 |\n",
            "| Validation | 0.677 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 158/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.272 |    0.468 | 0:00:00.18 |\n",
            "| Validation | 0.671 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 159/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.238 |    0.482 | 0:00:00.18 |\n",
            "| Validation | 0.664 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 160/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.306 |    0.450 | 0:00:00.18 |\n",
            "| Validation | 0.663 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 161/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.184 |    0.497 | 0:00:00.17 |\n",
            "| Validation | 0.666 |    0.882 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 162/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.403 |    0.433 | 0:00:00.18 |\n",
            "| Validation | 0.658 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 163/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.197 |    0.469 | 0:00:00.18 |\n",
            "| Validation | 0.649 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 164/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.506 |    0.385 | 0:00:00.16 |\n",
            "| Validation | 0.655 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 165/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.370 |    0.425 | 0:00:00.16 |\n",
            "| Validation | 0.657 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 166/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.184 |    0.505 | 0:00:00.18 |\n",
            "| Validation | 0.643 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 167/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.255 |    0.463 | 0:00:00.18 |\n",
            "| Validation | 0.639 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 168/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.241 |    0.460 | 0:00:00.18 |\n",
            "| Validation | 0.624 |    0.882 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 169/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.500 |    0.406 | 0:00:00.18 |\n",
            "| Validation | 0.613 |    0.882 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 170/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.366 |    0.448 | 0:00:00.18 |\n",
            "| Validation | 0.608 |    0.882 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 171/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.266 |    0.451 | 0:00:00.18 |\n",
            "| Validation | 0.590 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 172/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.113 |    0.503 | 0:00:00.18 |\n",
            "| Validation | 0.582 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 173/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.351 |    0.447 | 0:00:00.18 |\n",
            "| Validation | 0.581 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 174/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.333 |    0.451 | 0:00:00.16 |\n",
            "| Validation | 0.593 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 175/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.286 |    0.433 | 0:00:00.16 |\n",
            "| Validation | 0.613 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 176/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.189 |    0.478 | 0:00:00.16 |\n",
            "| Validation | 0.615 |    0.882 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 177/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.252 |    0.467 | 0:00:00.16 |\n",
            "| Validation | 0.594 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 178/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.446 |    0.402 | 0:00:00.16 |\n",
            "| Validation | 0.582 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 179/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.077 |    0.482 | 0:00:00.18 |\n",
            "| Validation | 0.567 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 180/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.346 |    0.448 | 0:00:00.18 |\n",
            "| Validation | 0.550 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 181/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.075 |    0.521 | 0:00:00.16 |\n",
            "| Validation | 0.554 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 182/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.229 |    0.481 | 0:00:00.16 |\n",
            "| Validation | 0.564 |    0.882 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 183/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.078 |    0.503 | 0:00:00.16 |\n",
            "| Validation | 0.574 |    0.882 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 184/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.441 |    0.407 | 0:00:00.16 |\n",
            "| Validation | 0.565 |    0.882 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 185/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.374 |    0.437 | 0:00:00.18 |\n",
            "| Validation | 0.545 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 186/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.021 |    0.516 | 0:00:00.19 |\n",
            "| Validation | 0.540 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 187/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.837 |    0.551 | 0:00:00.18 |\n",
            "| Validation | 0.526 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 188/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.548 |    0.378 | 0:00:00.18 |\n",
            "| Validation | 0.525 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 189/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.314 |    0.433 | 0:00:00.16 |\n",
            "| Validation | 0.534 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 190/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.346 |    0.440 | 0:00:00.16 |\n",
            "| Validation | 0.539 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 191/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.425 |    0.404 | 0:00:00.17 |\n",
            "| Validation | 0.544 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|        EPOCH: 192/200 not saving...        |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.260 |    0.430 | 0:00:00.16 |\n",
            "| Validation | 0.528 |    0.892 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 193/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.085 |    0.486 | 0:00:00.18 |\n",
            "| Validation | 0.520 |    0.903 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 194/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.381 |    0.410 | 0:00:00.18 |\n",
            "| Validation | 0.520 |    0.914 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 195/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.215 |    0.437 | 0:00:00.18 |\n",
            "| Validation | 0.516 |    0.914 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 196/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.198 |    0.476 | 0:00:00.18 |\n",
            "| Validation | 0.507 |    0.914 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 197/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.374 |    0.422 | 0:00:00.18 |\n",
            "| Validation | 0.504 |    0.925 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 198/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.164 |    0.481 | 0:00:00.18 |\n",
            "| Validation | 0.503 |    0.925 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 199/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.153 |    0.474 | 0:00:00.18 |\n",
            "| Validation | 0.497 |    0.925 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|    EPOCH: 200/200 saving best model...     |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 2.266 |    0.465 | 0:00:00.19 |\n",
            "| Validation | 0.493 |    0.914 |            |\n",
            "+------------+-------+----------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the best model.\n",
        "In the following code cell we are going to evaluate the best model using on the `test` data as follows:"
      ],
      "metadata": {
        "id": "Tt6w6iSNhBsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = [\"Set\", \"Loss\", \"Accuracy\", \"ETA (time)\"]\n",
        "mhcb_model.load_state_dict(torch.load(MODEL_NAME))\n",
        "test_loss, test_acc = evaluate(mhcb_model, test_loader, criterion)\n",
        "title = \"Model Evaluation Summary\"\n",
        "data_rows = [[\"Test\", f'{test_loss:.3f}', f'{test_acc * 100:.2f}%', \"\"]]\n",
        "\n",
        "tabulate_data(column_names, data_rows, title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjI79Ryhg0oz",
        "outputId": "f22f6817-4605-499c-a598-925f7c472c94"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------+\n",
            "|       Model Evaluation Summary       |\n",
            "+------+-------+----------+------------+\n",
            "| Set  |  Loss | Accuracy | ETA (time) |\n",
            "+------+-------+----------+------------+\n",
            "| Test | 0.553 |   91.94% |            |\n",
            "+------+-------+----------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Inference\n",
        "In the following code cell we are going to make predictions with the best model. We will have the function called `inference_preprocess_text` which is a function that process the text for inference."
      ],
      "metadata": {
        "id": "KwHVSqWqhRDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_preprocess_text(text, max_len=100, padding=\"pre\"):\n",
        "  assert padding==\"pre\" or padding==\"post\", \"the padding can be either pre or post\"\n",
        "  text_holder = torch.zeros(max_len, dtype=torch.int32) # fixed size tensor of max_len with  = 0\n",
        "  processed_text = torch.tensor(text_pipeline(text), dtype=torch.int32)\n",
        "  pos = min(max_len, len(processed_text))\n",
        "  if padding == \"pre\":\n",
        "    text_holder[:pos] = processed_text[:pos]\n",
        "  else:\n",
        "    text_holder[-pos:] = processed_text[-pos:]\n",
        "  text_list= text_holder.unsqueeze(dim=0)\n",
        "  return text_list"
      ],
      "metadata": {
        "id": "b92dGb5-hI_I"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting Tags\n",
        "In the following code cell we are going to create a function that predicts the `tags` given a certain `pattern` called `predict_tags`."
      ],
      "metadata": {
        "id": "zmvxwzw8hp35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Prediction:\n",
        "  def __init__(self, pattern: str, tag: str, tagId: int, confidence: float):\n",
        "    self.pattern = pattern\n",
        "    self.tag = tag\n",
        "    self.tagId = tagId\n",
        "    self.confidence = confidence\n",
        "\n",
        "  def __repr__(self) -> str:\n",
        "    return f\"<MHCB Preciction: {self.tag}>\"\n",
        "\n",
        "  def __str__(self) -> str:\n",
        "    return f\"<MHCB Preciction: {self.tag}>\"\n",
        "\n",
        "  def to_json(self):\n",
        "    return {\n",
        "        'pattern':  self.pattern,\n",
        "        'tag':  self.tag,\n",
        "        'tagId':  self.tagId,\n",
        "        'confidence':  self.confidence,\n",
        "    }"
      ],
      "metadata": {
        "id": "INOb_R2XhozJ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tag(model, sentence, device): \n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    tensor = inference_preprocess_text(sentence).to(device)\n",
        "    length = torch.tensor([len(t) for t in tensor])\n",
        "    probabilities = torch.softmax(model(tensor, length).squeeze(0), dim=0)\n",
        "    prediction = torch.argmax(probabilities)\n",
        "    prediction = prediction.detach().cpu().item()\n",
        "    tags = {v:k for k, v in labels_dict.items()}\n",
        "    tag = tags[prediction]\n",
        "   \n",
        "    return Prediction(\n",
        "        sentence.lower(), tag, int(prediction), float(round(probabilities[prediction].item(), 2))\n",
        "    )"
      ],
      "metadata": {
        "id": "O3Nk45oOijTX"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_tag(mhcb_model, \"nothing much\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FotmLIEkqmE",
        "outputId": "5dd29c82-bce2-40e8-934b-8d292538800f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MHCB Preciction: neutral-response>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the model.\n",
        "We are going to download the model"
      ],
      "metadata": {
        "id": "hZ1w-3vTlkXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "S0Yn_GM3llLp",
        "outputId": "178864b0-87b6-4e95-cccf-6e5fdda0ce5b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f5fe64b3-ae92-4f61-9488-7ce5be2883a7\", \"mhcb-model.pt\", 9609903)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U_JSLBqewxSU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}