{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_IMDB_FASTER_SENTIMENT_Review.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6SS2euwauZR"
      },
      "source": [
        "### IMDB Faster Sentiment Analyisis\n",
        "\n",
        "In this notebook we are going to use our own datset, load it using the `torchtext` predict positive or negative sentiments.\n",
        "\n",
        "We will be using the data that we created from the previous notebook to do sentiment classification on the movie reviews. This data was created and stored on my google drive @ [garidziracrispen@gmail.com](https://drive.google.com/drive/folders/1M5NseG3ni7_UcLYlCyTrhaqdrs0Pzf7d)\n",
        "\n",
        "\n",
        "The data was stored as clean text without `html` tags.\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbtouX3Qce7L"
      },
      "source": [
        "import torch\n",
        "from torchtext.legacy import data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import en_core_web_sm\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO001mo-c5sm"
      },
      "source": [
        "### Device Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64XFKQXYce-b"
      },
      "source": [
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.backends.cudnn.deteministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B072BhZAzaOd"
      },
      "source": [
        "### Mounting the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCvE4SKKzr1K",
        "outputId": "6bc991a8-88d7-4874-9f50-10eb0eadb1d1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeRP0xfJxaZo"
      },
      "source": [
        "### Generating Bigrams\n",
        "Accoding to the FastText paper we have to generate bigrams for each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUa0UiJyMo_w",
        "outputId": "cf738877-d0be-4ac8-de78-d24706ccaf9f"
      },
      "source": [
        "def generate_bigrams(x):\n",
        "  n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
        "  for n_gram in n_grams:\n",
        "      x.append(' '.join(n_gram))\n",
        "  return x\n",
        "generate_bigrams(['This', 'film', 'is', 'terrible'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'film', 'is', 'terrible', 'film is', 'is terrible', 'This film']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fck6PIdcfBL"
      },
      "source": [
        "TEXT = data.Field(tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm',\n",
        "                  preprocessing=generate_bigrams\n",
        "                  )\n",
        "LABEL = data.LabelField( dtype = torch.float)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHW-bUelcfDz"
      },
      "source": [
        "fields = {\n",
        "     \"review\": ('review', TEXT),\n",
        "    \"sentiment\": (\"sentiment\", LABEL)\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d59-h0Tjh0i5"
      },
      "source": [
        "### Creating dataset using the `TabularDataset`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXmEmynRcfGz"
      },
      "source": [
        "train_data, test_data, validation_data = data.TabularDataset.splits(\n",
        "    path=\"/content/drive/MyDrive/NLP Data/IMDB/\",\n",
        "    train=\"train.json\",\n",
        "    test=\"test.json\",\n",
        "    validation = \"validation.json\",\n",
        "    format=\"json\",\n",
        "    fields=fields\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEpWC3DXcfM7",
        "outputId": "181c7cc9-78be-41a1-e669-698615e7e4f3"
      },
      "source": [
        "print(vars(train_data[0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'review': ['I', 'really', 'liked', 'this', 'Summerslam', 'due', 'to', 'the', 'look', 'of', 'the', 'arena', ',', 'the', 'curtains', 'and', 'just', 'the', 'look', 'overall', 'was', 'interesting', 'to', 'me', 'for', 'some', 'reason', '.', 'Anyways', ',', 'this', 'could', 'have', 'been', 'one', 'of', 'the', 'best', 'Summerslam', \"'s\", 'ever', 'if', 'the', 'WWF', 'did', \"n't\", 'have', 'Lex', 'Luger', 'in', 'the', 'main', 'event', 'against', 'Yokozuna', ',', 'now', 'for', 'it', \"'s\", 'time', 'it', 'was', 'ok', 'to', 'have', 'a', 'huge', 'fat', 'man', 'vs', 'a', 'strong', 'man', 'but', 'I', \"'m\", 'glad', 'times', 'have', 'changed', '.', 'It', 'was', 'a', 'terrible', 'main', 'event', 'just', 'like', 'every', 'match', 'Luger', 'is', 'in', 'is', 'terrible', '.', 'Other', 'matches', 'on', 'the', 'card', 'were', 'Razor', 'Ramon', 'vs', 'Ted', 'Dibiase', ',', 'Steiner', 'Brothers', 'vs', 'Heavenly', 'Bodies', ',', 'Shawn', 'Michaels', 'vs', 'Curt', 'Hening', ',', 'this', 'was', 'the', 'event', 'where', 'Shawn', 'named', 'his', 'big', 'monster', 'of', 'a', 'body', 'guard', 'Diesel', ',', 'IRS', 'vs', '1', '-', '2', '-', '3', 'Kid', ',', 'Bret', 'Hart', 'first', 'takes', 'on', 'Doink', 'then', 'takes', 'on', 'Jerry', 'Lawler', 'and', 'stuff', 'with', 'the', 'Harts', 'and', 'Lawler', 'was', 'always', 'very', 'interesting', ',', 'then', 'Ludvig', 'Borga', 'destroyed', 'Marty', 'Jannetty', ',', 'Undertaker', 'took', 'on', 'Giant', 'Gonzalez', 'in', 'another', 'terrible', 'match', ',', 'The', 'Smoking', 'Gunns', 'and', 'Tatanka', 'took', 'on', 'Bam', 'Bam', 'Bigelow', 'and', 'the', 'Headshrinkers', ',', 'and', 'Yokozuna', 'defended', 'the', 'world', 'title', 'against', 'Lex', 'Luger', 'this', 'match', 'was', 'boring', 'and', 'it', 'has', 'a', 'terrible', 'ending', '.', 'However', 'it', 'deserves', '8/10', 'Marty Jannetty', '. It', 'takes on', 'a body', 'where Shawn', 'reason .', 'the main', 'it was', 'vs a', 'a strong', 'terrible main', 'on Giant', 'Tatanka took', 'Shawn Michaels', 'and the', 'was boring', '. However', 'the Headshrinkers', 'it deserves', 'for it', 'ok to', 'was always', 'terrible match', 'and Lawler', 'Other matches', 'Lawler was', 'it has', 'Hening ,', 'in the', 'world title', '- 2', \"n't have\", '3 Kid', 'Borga destroyed', 'for some', 'Yokozuna ,', 'this Summerslam', 'now for', 'Steiner Brothers', 'vs Curt', 'Brothers vs', 'main event', 'man but', 'best Summerslam', 'glad times', 'in is', 'time it', 'have a', 'some reason', 'look overall', 'huge fat', 'ever if', 'Bigelow and', 'changed .', 'of a', 'body guard', 'stuff with', 'It was', 'been one', 'due to', ', Bret', 'really liked', 'IRS vs', 'always very', 'Jannetty ,', 'against Lex', \"'s time\", 'Ted Dibiase', 'The Smoking', 'Luger in', 'However it', 'every match', 'Bret Hart', 'of the', 'Ludvig Borga', \"'s ever\", 'was ok', 'event just', 'this was', ', IRS', 'and Tatanka', 'if the', 'Gunns and', 'could have', ', Steiner', ', then', ', the', \"'m glad\", 'Harts and', 'have changed', 'took on', 'liked this', 'his big', ', Undertaker', 'Kid ,', 'Bodies ,', 'interesting ,', '2 -', ', this', 'Ramon vs', \"Summerslam 's\", 'Doink then', 'this could', 'Headshrinkers ,', 'Anyways ,', 'look of', 'but I', 'is terrible', 'man vs', 'the look', 'monster of', 'first takes', 'guard Diesel', 'a huge', 'ending .', 'title against', 'to me', 'the world', 'terrible .', 'times have', 'the Harts', ', and', 'terrible ending', 'and just', 'Curt Hening', '- 3', 'me for', 'fat man', 'boring and', 'was the', 'Undertaker took', 'I really', 'Bam Bigelow', 'Summerslam due', ', now', 'the curtains', 'arena ,', 'another terrible', 'Shawn named', 'one of', 'Jerry Lawler', 'Hart first', 'has a', 'was interesting', '. Anyways', 'on Bam', 'Giant Gonzalez', 'Dibiase ,', 'on the', 'and stuff', 'Luger this', 'and it', '. Other', 'matches on', 'deserves 8/10', 'the WWF', 'match ,', 'defended the', 'strong man', 'overall was', 'Lex Luger', 'big monster', 'vs 1', \"did n't\", 'Gonzalez in', 'destroyed Marty', 'then takes', 'Smoking Gunns', \"it 's\", 'Michaels vs', 'Yokozuna defended', 'WWF did', 'card were', 'like every', 'match was', 'vs Ted', 'event against', 'Heavenly Bodies', 'the best', 'curtains and', 'interesting to', 'were Razor', 'on Doink', 'Luger is', 'then Ludvig', 'Bam Bam', 'just like', 'just the', 'have Lex', 'vs Heavenly', 'event where', 'with the', 'very interesting', 'the event', 'this match', 'the arena', 'to have', 'in another', 'to the', 'against Yokozuna', 'a terrible', 'was a', 'named his', 'Diesel ,', '1 -', 'Razor Ramon', 'is in', 'match Luger', ', Shawn', ', The', 'Lawler and', 'on Jerry', \"I 'm\", 'have been', 'the card', 'and Yokozuna'], 'sentiment': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX5C56UxJtW3",
        "outputId": "4bd35b65-fbf2-435b-b432-ded5848793ea"
      },
      "source": [
        "from collections import Counter\n",
        "counter = Counter()\n",
        "for i in train_data:\n",
        "  counter[i.sentiment] += 1\n",
        "counter"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 18780, 1: 18720})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-ka7dFwjHpv"
      },
      "source": [
        "### Checking the data sizes for each sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht5bL08VcfQL",
        "outputId": "4a215f60-4d8e-40b5-ea17-1b569d301128"
      },
      "source": [
        "\n",
        "from prettytable import PrettyTable\n",
        "def tabulate(column_names, data):\n",
        "  table = PrettyTable(column_names)\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)\n",
        "column_names = [\"SUBSET\", \"EXAMPLE(s)\"]\n",
        "data_sets = [\n",
        "        [\"training\", len(train_data)],\n",
        "        ['validation', len(validation_data)],\n",
        "        ['test', len(test_data)]\n",
        "]\n",
        "tabulate(column_names, data_sets)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+------------+\n",
            "|   SUBSET   | EXAMPLE(s) |\n",
            "+------------+------------+\n",
            "|  training  |   37500    |\n",
            "| validation |    5000    |\n",
            "|    test    |    7500    |\n",
            "+------------+------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdRfrVESj1Qx"
      },
      "source": [
        "### Loading pretrained word Embedings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jfgi21_cfZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "057be7ec-61ec-4564-a195-961bbe0dd161"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(\n",
        "    train_data,\n",
        "    max_size = MAX_VOCAB_SIZE,\n",
        "    vectors = \"glove.6B.100d\",\n",
        "    unk_init = torch.Tensor.normal_\n",
        ")\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.37MB/s]                           \n",
            "100%|█████████▉| 398239/400000 [00:17<00:00, 23097.75it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrxkNOBWkRYu"
      },
      "source": [
        "### Creating Iterators - `Bucket Iterator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRah5w02kQHj"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, validation_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, validation_data, test_data),\n",
        "    device = device,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.review),\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K0RRtuek3T-"
      },
      "source": [
        "### Creating a Model\n",
        "In this notebook we are going to use a `bidirectional` LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE6-YqQpk3Dm"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xnkpV_-kzOB"
      },
      "source": [
        "class IMDBFastText(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
        "    super(IMDBFastText, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "    self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "  def forward(self, text):\n",
        "    #text = [sent len, batch size]\n",
        "    embedded = self.embedding(text)\n",
        "    #embedded = [sent len, batch size, emb dim]\n",
        "    embedded = embedded.permute(1, 0, 2)\n",
        "    #embedded = [batch size, sent len, emb dim]\n",
        "    pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
        "    #pooled = [batch size, embedding_dim]\n",
        "    return self.fc(pooled)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHwFdkMsnSnW"
      },
      "source": [
        "### Creating a model instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7oXh35UkzK9",
        "outputId": "9a16b0e5-c853-46c1-e8ff-ab94ba541d76"
      },
      "source": [
        "\n",
        "INPUT_DIM = len(TEXT.vocab) # # 25002\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # 0\n",
        "imdb_model = IMDBFastText(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            PAD_IDX)\n",
        "imdb_model"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IMDBFastText(\n",
              "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
              "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW_XcObanet-"
      },
      "source": [
        "### Counting number of trainable parameters in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9ee96uakzHg",
        "outputId": "fbb880a7-6e44-4691-fe4c-4ea6921eb94b"
      },
      "source": [
        "def count_trainable_params(model):\n",
        "  return sum([i.numel() for i in model.parameters() if i.requires_grad])\n",
        "\n",
        "print(f'The model has {count_trainable_params(imdb_model):,} trainable parameters')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,500,301 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYQEJMkokzEg"
      },
      "source": [
        "#### Loading pretrained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Mlsrd7oJtn"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbdF-1l6kzBO",
        "outputId": "ca463618-7703-492c-d887-304cf83e12e7"
      },
      "source": [
        "imdb_model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.1233,  0.3499,  0.6173],\n",
              "        [ 0.7262,  0.0912, -0.3891,  ...,  0.0821,  0.4440, -0.7240],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [-1.1343,  1.3227,  1.2282,  ...,  0.8923,  0.5572, -1.2730],\n",
              "        [-1.6437,  0.6558,  0.3809,  ..., -0.6630, -1.0708,  0.5194],\n",
              "        [ 0.4784,  0.6412, -0.1034,  ...,  0.6080,  0.2487, -2.1468]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "124pKMgaoc09"
      },
      "source": [
        "### Zeroing the <pad> and <unk> tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1WRrLudky92",
        "outputId": "af42a7ec-ef05-4016-b8f0-3740cd827668"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token] or TEXT.vocab.stoi[\"<unk>\"]\n",
        "imdb_model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "imdb_model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "print(imdb_model.embedding.weight.data)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [-1.1343,  1.3227,  1.2282,  ...,  0.8923,  0.5572, -1.2730],\n",
            "        [-1.6437,  0.6558,  0.3809,  ..., -0.6630, -1.0708,  0.5194],\n",
            "        [ 0.4784,  0.6412, -0.1034,  ...,  0.6080,  0.2487, -2.1468]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMGyQ5pyooSa"
      },
      "source": [
        "### Trainning the Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLnToh2ZooWG"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(imdb_model.parameters())"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5D6CNrHphNx"
      },
      "source": [
        "### Criterion and amazon_model to the Device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq4Qm_x7ooZv"
      },
      "source": [
        "imdb_model = imdb_model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_DaI4fepwew"
      },
      "source": [
        "### The Binary Accuracy Function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrHG4OjWoofv"
      },
      "source": [
        "def binary_accuracy(y_preds, y_true):\n",
        "  #round predictions to the closest integer\n",
        "  rounded_preds = torch.round(torch.sigmoid(y_preds))\n",
        "  correct = (rounded_preds == y_true).float() #convert into float for division \n",
        "  acc = correct.sum() / len(correct)\n",
        "  return acc"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuQMnQJWp7Qi"
      },
      "source": [
        "### Trainning and evaluation Functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWuc6c-zoojB"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text = batch.review\n",
        "        predictions = model(text).squeeze(1)\n",
        "        loss = criterion(predictions, batch.sentiment)\n",
        "        acc = binary_accuracy(predictions, batch.sentiment)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text = batch.review\n",
        "            predictions = model(text).squeeze(1)\n",
        "            loss = criterion(predictions, batch.sentiment)\n",
        "            acc = binary_accuracy(predictions, batch.sentiment)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhvBTP5OqKL-"
      },
      "source": [
        "We'll also create a function to tell us how long an epoch takes to compare training times between models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WhT7wkToonV"
      },
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LEZP7cWqPwG"
      },
      "source": [
        "### Trainning Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zSJTbCEoopE",
        "outputId": "d08a71f3-d1c4-4512-94e2-403638d4673a"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(imdb_model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(imdb_model, validation_iterator, criterion)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(imdb_model.state_dict(), 'best-model.pt')\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 2m 11s\n",
            "\tTrain Loss: 0.663 | Train Acc: 67.87%\n",
            "\t Val. Loss: 0.481 |  Val. Acc: 77.45%\n",
            "Epoch: 02 | Epoch Time: 2m 9s\n",
            "\tTrain Loss: 0.523 | Train Acc: 81.96%\n",
            "\t Val. Loss: 0.372 |  Val. Acc: 85.32%\n",
            "Epoch: 03 | Epoch Time: 2m 8s\n",
            "\tTrain Loss: 0.397 | Train Acc: 87.47%\n",
            "\t Val. Loss: 0.364 |  Val. Acc: 88.01%\n",
            "Epoch: 04 | Epoch Time: 2m 10s\n",
            "\tTrain Loss: 0.327 | Train Acc: 89.40%\n",
            "\t Val. Loss: 0.388 |  Val. Acc: 88.96%\n",
            "Epoch: 05 | Epoch Time: 2m 9s\n",
            "\tTrain Loss: 0.282 | Train Acc: 90.64%\n",
            "\t Val. Loss: 0.413 |  Val. Acc: 89.58%\n",
            "Epoch: 06 | Epoch Time: 2m 8s\n",
            "\tTrain Loss: 0.252 | Train Acc: 91.57%\n",
            "\t Val. Loss: 0.435 |  Val. Acc: 90.03%\n",
            "Epoch: 07 | Epoch Time: 2m 8s\n",
            "\tTrain Loss: 0.230 | Train Acc: 92.35%\n",
            "\t Val. Loss: 0.456 |  Val. Acc: 90.35%\n",
            "Epoch: 08 | Epoch Time: 2m 8s\n",
            "\tTrain Loss: 0.209 | Train Acc: 93.00%\n",
            "\t Val. Loss: 0.478 |  Val. Acc: 90.45%\n",
            "Epoch: 09 | Epoch Time: 2m 10s\n",
            "\tTrain Loss: 0.194 | Train Acc: 93.55%\n",
            "\t Val. Loss: 0.497 |  Val. Acc: 90.70%\n",
            "Epoch: 10 | Epoch Time: 2m 7s\n",
            "\tTrain Loss: 0.179 | Train Acc: 94.03%\n",
            "\t Val. Loss: 0.515 |  Val. Acc: 90.80%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm6C3rlIFJn3"
      },
      "source": [
        "### Evaluating the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0xDEYz1oovX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132de3c3-ea48-4f96-aed6-f2ce92538403"
      },
      "source": [
        "imdb_model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(imdb_model, test_iterator, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.398 | Test Acc: 86.71%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRd4xDQwFubf"
      },
      "source": [
        "### Model Inference - Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reP3lwspooz7"
      },
      "source": [
        "import spacy\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "sentiments = [\"NEG\", \"POS\"]\n",
        "def predict_sentiment(model, sent):\n",
        "  model.eval()\n",
        "  tokenized = [tok.text for tok in nlp.tokenizer(sent)]\n",
        "  indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "  length = [len(indexed)]\n",
        "  tensor = torch.LongTensor(indexed).to(device)\n",
        "  tensor = tensor.unsqueeze(1)\n",
        "  prediction = torch.sigmoid(model(tensor))\n",
        "\n",
        "  predicted_class = round(prediction.item())\n",
        "  confidence = prediction.item() if prediction.item() >= .5 else 1 - prediction.item()\n",
        "  print(f'PREDICTED CLASS:\\t{predicted_class}\\nCONFIDENCE:\\t\\t{confidence * 100:.2f}%\\nSENTIMENT:\\t\\t{sentiments[predicted_class]}')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVEc71MrIjZQ"
      },
      "source": [
        "### Negative review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAzsMzkCoo3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a8fc2c-6d16-47aa-8e21-4e8db572ca53"
      },
      "source": [
        "predict_sentiment(imdb_model, \"This movie is terrible\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTED CLASS:\t0\n",
            "CONFIDENCE:\t\t100.00%\n",
            "SENTIMENT:\t\tNEG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CKI6umcImtx"
      },
      "source": [
        "### Positive review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEPwGxu0kyp3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c508263-1728-4e93-ffbb-35217144bb97"
      },
      "source": [
        "predict_sentiment(imdb_model, \"Best movie of all time\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTED CLASS:\t1\n",
            "CONFIDENCE:\t\t100.00%\n",
            "SENTIMENT:\t\tPOS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IexvtkKvItpl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}