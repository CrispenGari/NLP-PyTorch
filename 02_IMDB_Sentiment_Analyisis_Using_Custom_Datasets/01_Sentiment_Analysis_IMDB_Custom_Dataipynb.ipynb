{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Sentiment_Analysis_IMDB_Custom_Dataipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6SS2euwauZR"
      },
      "source": [
        "### IMDB - Sentiment Analyisis\n",
        "\n",
        "In this notebook we are going to use our own datset, load it using the `torchtext` predict positive or negative sentiments.\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbtouX3Qce7L"
      },
      "source": [
        "import torch\n",
        "from torchtext.legacy import data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import en_core_web_sm\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO001mo-c5sm"
      },
      "source": [
        "### Device Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64XFKQXYce-b"
      },
      "source": [
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.backends.cudnn.deteministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrMhTAUvd65Q"
      },
      "source": [
        "### Data Prep.\n",
        "\n",
        "We have a IMDB ``csv`` file that we want to transform it's data from `csv` to `json`.\n",
        "\n",
        "```json\n",
        "{\"review\": \"This movie was borderline in crude humor....I utterly can not believe that these people can get away with this. Johnny Knoxville didn't cross the line...he was stomping all over it! This was better than the first...ALL THA WAY! The thing I found about the 1st movie was that the shenanigans were somewhat as if it was on the t.v. show. NOT THIS TIME!!! they completely made a 180 degree flip...the whole cast is so outstanding in what they do and not were the stunts crazy...but the music basically fit every situation...GOOD WORK!!!! When you go see this be sure to use the bathroom before going to the theater, maintain a strong stomach and rememba to not let your beverage spray out your nose....\", \"sentiment\": 1}\n",
        "```\n",
        "Here we are interested in two things:\n",
        "\n",
        "```\n",
        "\"review\" -> thats the review\n",
        "\"sentiment\" -> 0 negative and 1 for postive.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cUr9CIbtORS"
      },
      "source": [
        "### Read the csv file using pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "bTehgXVnELiO",
        "outputId": "cd4090c2-72f9-485d-cd58-e845b1a90621"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/NLP Data/IMDB/IMDB Dataset.csv')\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdjiPT-QEWLM"
      },
      "source": [
        "### Extract the sentiments and reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIijoF7TEzFl"
      },
      "source": [
        "reviews = df.review.values\n",
        "sentiments = [1 if sentiment== \"positive\" else 0 for sentiment in df.sentiment.values]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hd8gpuAtdMa"
      },
      "source": [
        "### A helper function that will help us to clean text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSs68PtuFCvm"
      },
      "source": [
        "import re\n",
        "def remove_html_tags(sent):\n",
        "  a = re.sub(r'<[a-zA-Z]+\\s/?>', ' ', sent)\n",
        "  b = re.sub(r'\\s+', ' ', a)\n",
        "  return b"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kh-kfy_ti8Y"
      },
      "source": [
        "### Clean the review text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x_RlSrkFCq1"
      },
      "source": [
        "reviews  = list(map(remove_html_tags, reviews))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgQfy1QDFCoW",
        "outputId": "54404c0b-1185-47b2-d898-239c9134182c"
      },
      "source": [
        "test_size = int(0.1 * len(reviews))\n",
        "valid_size = int(.15 * len(reviews))\n",
        "train_size = int((1 - 0.1 - 0.15) * len(reviews) )\n",
        "test_size, train_size, valid_size"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 37500, 7500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUEdFb1BtnvG"
      },
      "source": [
        "### Shuffle the data.\n",
        "\n",
        "Before shuffling, we don't want to split labels with their respective reviews and then randomly shuffle using numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hsb2p9XFCkZ"
      },
      "source": [
        "data = np.column_stack([reviews, sentiments])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r22RK5N1FCg7"
      },
      "source": [
        "np.random.shuffle(data)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AvZr9k-t5K0"
      },
      "source": [
        "### Using fancy indexing to distribute the sentiments and reviews with their respective size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA2ggRq5FCc8"
      },
      "source": [
        "train_reviews_sentiments = data[:train_size]\n",
        "validation_reviews_sentiments = data[train_size:train_size + valid_size]\n",
        "test_reviews_sentiments = data[train_size + valid_size:]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLcsT99Eu3as"
      },
      "source": [
        "### Checking the length of the training, validation, and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls3ILMkIFCRk",
        "outputId": "90787d5f-28ca-45f6-ecc7-7d6e8557fdf3"
      },
      "source": [
        "len(train_reviews_sentiments), len(validation_reviews_sentiments), len(test_reviews_sentiments)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(37500, 7500, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX_JYBvNqZfr"
      },
      "source": [
        "### Processing data to python dictioneries\n",
        "\n",
        "We will use these list of dictionery to create json files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFWb4W26P-YK"
      },
      "source": [
        "import json\n",
        "train_reviews_sentiments_list_dict = []\n",
        "for r, s in train_reviews_sentiments:\n",
        "  train_reviews_sentiments_list_dict.append({\n",
        "      \"review\": r,\n",
        "      \"sentiment\": int(s)\n",
        "  })\n",
        "\n",
        "test_reviews_sentiments_list_dict = []\n",
        "for r, s in test_reviews_sentiments:\n",
        "  test_reviews_sentiments_list_dict.append({\n",
        "      \"review\": r,\n",
        "      \"sentiment\": int(s)\n",
        "  })\n",
        "\n",
        "validation_reviews_sentiments_list_dict = []\n",
        "for r, s in validation_reviews_sentiments:\n",
        "  validation_reviews_sentiments_list_dict.append({\n",
        "      \"review\": r,\n",
        "      \"sentiment\": int(s)\n",
        "  })"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym3rFt0QQ0OB"
      },
      "source": [
        "### Saving  the preprocessed data to json files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m9yVs_JP-SO"
      },
      "source": [
        "import os\n",
        "base_path=\"/content/drive/MyDrive/NLP Data/IMDB\"\n",
        "test_path = 'test.json'\n",
        "train_path = 'train.json'\n",
        "valid_path = 'validation.json'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdolLTN0P-PU",
        "outputId": "fb68d5de-c8e7-4f4d-ebfb-8ec51f7d9534"
      },
      "source": [
        "file_object = open(os.path.join(base_path, train_path), 'w')\n",
        "for line in train_reviews_sentiments_list_dict:\n",
        "  file_object.write(json.dumps(line))\n",
        "  file_object.write('\\n')\n",
        "file_object.close()\n",
        "print(\"train.json created\")\n",
        "\n",
        "file_object = open(os.path.join(base_path, test_path), 'w')\n",
        "for line in test_reviews_sentiments_list_dict:\n",
        "  file_object.write(json.dumps(line))\n",
        "  file_object.write('\\n')\n",
        "file_object.close()\n",
        "print(\"test.json created\")\n",
        "\n",
        "file_object = open(os.path.join(base_path, valid_path), 'w')\n",
        "for line in validation_reviews_sentiments_list_dict:\n",
        "  file_object.write(json.dumps(line))\n",
        "  file_object.write('\\n')\n",
        "file_object.close()\n",
        "print(\"validation.json created\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.json created\n",
            "test.json created\n",
            "validation.json created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT_oyr1he3hn"
      },
      "source": [
        "### Preparing the `Fields`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClNef9lESbcg"
      },
      "source": [
        "from torchtext.legacy import data"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fck6PIdcfBL"
      },
      "source": [
        "TEXT = data.Field(tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm',\n",
        "                  include_lengths=True\n",
        "                  )\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nWx9kE1qRCj"
      },
      "source": [
        "### Creating the fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHW-bUelcfDz"
      },
      "source": [
        "fields = {\n",
        "     \"review\": ('review', TEXT),\n",
        "    \"sentiment\": (\"sentiment\", LABEL)\n",
        "}"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnYkeswrqNeB"
      },
      "source": [
        "### Creating a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXmEmynRcfGz"
      },
      "source": [
        "train_data, test_data, validation_data = data.TabularDataset.splits(\n",
        "    path=\"/content/drive/MyDrive/NLP Data/IMDB/\",\n",
        "    train=\"train.json\",\n",
        "    test=\"test.json\",\n",
        "    validation = \"validation.json\",\n",
        "    format=\"json\",\n",
        "    fields=fields\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEpWC3DXcfM7",
        "outputId": "3d396b64-5beb-4d79-a843-8151939ecdda"
      },
      "source": [
        "print(vars(train_data[0]))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'review': ['I', 'really', 'liked', 'this', 'Summerslam', 'due', 'to', 'the', 'look', 'of', 'the', 'arena', ',', 'the', 'curtains', 'and', 'just', 'the', 'look', 'overall', 'was', 'interesting', 'to', 'me', 'for', 'some', 'reason', '.', 'Anyways', ',', 'this', 'could', 'have', 'been', 'one', 'of', 'the', 'best', 'Summerslam', \"'s\", 'ever', 'if', 'the', 'WWF', 'did', \"n't\", 'have', 'Lex', 'Luger', 'in', 'the', 'main', 'event', 'against', 'Yokozuna', ',', 'now', 'for', 'it', \"'s\", 'time', 'it', 'was', 'ok', 'to', 'have', 'a', 'huge', 'fat', 'man', 'vs', 'a', 'strong', 'man', 'but', 'I', \"'m\", 'glad', 'times', 'have', 'changed', '.', 'It', 'was', 'a', 'terrible', 'main', 'event', 'just', 'like', 'every', 'match', 'Luger', 'is', 'in', 'is', 'terrible', '.', 'Other', 'matches', 'on', 'the', 'card', 'were', 'Razor', 'Ramon', 'vs', 'Ted', 'Dibiase', ',', 'Steiner', 'Brothers', 'vs', 'Heavenly', 'Bodies', ',', 'Shawn', 'Michaels', 'vs', 'Curt', 'Hening', ',', 'this', 'was', 'the', 'event', 'where', 'Shawn', 'named', 'his', 'big', 'monster', 'of', 'a', 'body', 'guard', 'Diesel', ',', 'IRS', 'vs', '1', '-', '2', '-', '3', 'Kid', ',', 'Bret', 'Hart', 'first', 'takes', 'on', 'Doink', 'then', 'takes', 'on', 'Jerry', 'Lawler', 'and', 'stuff', 'with', 'the', 'Harts', 'and', 'Lawler', 'was', 'always', 'very', 'interesting', ',', 'then', 'Ludvig', 'Borga', 'destroyed', 'Marty', 'Jannetty', ',', 'Undertaker', 'took', 'on', 'Giant', 'Gonzalez', 'in', 'another', 'terrible', 'match', ',', 'The', 'Smoking', 'Gunns', 'and', 'Tatanka', 'took', 'on', 'Bam', 'Bam', 'Bigelow', 'and', 'the', 'Headshrinkers', ',', 'and', 'Yokozuna', 'defended', 'the', 'world', 'title', 'against', 'Lex', 'Luger', 'this', 'match', 'was', 'boring', 'and', 'it', 'has', 'a', 'terrible', 'ending', '.', 'However', 'it', 'deserves', '8/10'], 'sentiment': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX5C56UxJtW3",
        "outputId": "16680c67-9e81-4198-8fdd-5870e1194f07"
      },
      "source": [
        "from collections import Counter\n",
        "counter = Counter()\n",
        "for i in train_data:\n",
        "  counter[i.sentiment] += 1\n",
        "counter"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 18780, 1: 18720})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUI-029EqFG_"
      },
      "source": [
        "#### Checking how many examples do we have for each set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3Wqp33nqA7K",
        "outputId": "ffae8d89-ddaa-42cc-c884-8d09eb38c6b1"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "def tabulate(column_names, data):\n",
        "  table = PrettyTable(column_names)\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)\n",
        "column_names = [\"SUBSET\", \"EXAMPLE(s)\"]\n",
        "data_sets = [\n",
        "        [\"training\", len(train_data)],\n",
        "        ['validation', len(validation_data)],\n",
        "        ['test', len(test_data)]\n",
        "]\n",
        "tabulate(column_names, data_sets)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+------------+\n",
            "|   SUBSET   | EXAMPLE(s) |\n",
            "+------------+------------+\n",
            "|  training  |   37500    |\n",
            "| validation |    5000    |\n",
            "|    test    |    7500    |\n",
            "+------------+------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdRfrVESj1Qx"
      },
      "source": [
        "### Loading pretrained word Embedings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jfgi21_cfZL"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(\n",
        "    train_data,\n",
        "    max_size = MAX_VOCAB_SIZE,\n",
        "    vectors = \"glove.6B.100d\",\n",
        "    unk_init = torch.Tensor.normal_\n",
        ")\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrxkNOBWkRYu"
      },
      "source": [
        "### Creating Iterators - `Bucket Iterator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRah5w02kQHj"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, validation_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, validation_data, test_data),\n",
        "    device = device,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.review),\n",
        ")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K0RRtuek3T-"
      },
      "source": [
        "### Creating a Model\n",
        "In this notebook we are going to use a `bidirectional` LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE6-YqQpk3Dm"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xnkpV_-kzOB"
      },
      "source": [
        "class IMDBLSTMRNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_size, hidden_size, output_size, num_layers\n",
        "               , bidirectional, dropout, pad_idx):\n",
        "    super(IMDBLSTMRNN, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim=embedding_size, padding_idx=pad_idx)\n",
        "    self.lstm = nn.LSTM(embedding_size, hidden_size=hidden_size, \n",
        "                        bidirectional=bidirectional, num_layers=num_layers,\n",
        "                        dropout=dropout)\n",
        "    self.fc = nn.Linear(hidden_size * 2, out_features=output_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, text, text_lengths):\n",
        "    embedded = self.dropout(self.embedding(text))\n",
        "    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False)\n",
        "    packed_output, (h_0, c_0) = self.lstm(packed_embedded)\n",
        "    output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "    h_0 = self.dropout(torch.cat((h_0[-2,:,:], h_0[-1,:,:]), dim = 1))\n",
        "    return self.fc(h_0)\n",
        "    "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHwFdkMsnSnW"
      },
      "source": [
        "### Creating a model instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7oXh35UkzK9",
        "outputId": "931a4864-9153-414b-a2fd-69909a55e2b4"
      },
      "source": [
        "\n",
        "INPUT_DIM = len(TEXT.vocab) # # 25002\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # 0\n",
        "imdb_model = IMDBLSTMRNN(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            N_LAYERS, \n",
        "            BIDIRECTIONAL, \n",
        "            DROPOUT, \n",
        "            PAD_IDX)\n",
        "imdb_model"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IMDBLSTMRNN(\n",
              "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
              "  (lstm): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW_XcObanet-"
      },
      "source": [
        "### Counting number of trainable parameters in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9ee96uakzHg",
        "outputId": "26b69052-1438-42f2-ec91-22ff4a72a4d1"
      },
      "source": [
        "def count_trainable_params(model):\n",
        "  return sum([i.numel() for i in model.parameters() if i.requires_grad])\n",
        "\n",
        "print(f'The model has {count_trainable_params(imdb_model):,} trainable parameters')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 4,810,857 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYQEJMkokzEg"
      },
      "source": [
        "#### Loading pretrained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Mlsrd7oJtn"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbdF-1l6kzBO",
        "outputId": "f0f84e64-08d6-4009-80b1-ce6edd4f5def"
      },
      "source": [
        "imdb_model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.1233,  0.3499,  0.6173],\n",
              "        [ 0.7262,  0.0912, -0.3891,  ...,  0.0821,  0.4440, -0.7240],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 0.1098,  0.5145,  0.3449,  ..., -0.5092,  0.6672,  0.3905],\n",
              "        [-0.5405,  0.3392,  0.0200,  ...,  0.1090, -1.0385,  0.4093],\n",
              "        [ 0.8393,  0.4943, -0.8756,  ...,  0.6801,  0.2226,  1.1534]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "124pKMgaoc09"
      },
      "source": [
        "### Zeroing the <pad> and <unk> tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1WRrLudky92",
        "outputId": "9d73016e-f507-4c85-8ee1-bdbd8bcfe896"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token] or TEXT.vocab.stoi[\"<unk>\"]\n",
        "imdb_model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "imdb_model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "print(imdb_model.embedding.weight.data)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [ 0.1098,  0.5145,  0.3449,  ..., -0.5092,  0.6672,  0.3905],\n",
            "        [-0.5405,  0.3392,  0.0200,  ...,  0.1090, -1.0385,  0.4093],\n",
            "        [ 0.8393,  0.4943, -0.8756,  ...,  0.6801,  0.2226,  1.1534]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMGyQ5pyooSa"
      },
      "source": [
        "### Trainning the Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLnToh2ZooWG"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(imdb_model.parameters())"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5D6CNrHphNx"
      },
      "source": [
        "### Criterion and amazon_model to the Device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq4Qm_x7ooZv"
      },
      "source": [
        "imdb_model = imdb_model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_DaI4fepwew"
      },
      "source": [
        "### The Binary Accuracy Function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrHG4OjWoofv"
      },
      "source": [
        "def binary_accuracy(y_preds, y_true):\n",
        "  #round predictions to the closest integer\n",
        "  rounded_preds = torch.round(torch.sigmoid(y_preds))\n",
        "  correct = (rounded_preds == y_true).float() #convert into float for division \n",
        "  acc = correct.sum() / len(correct)\n",
        "  return acc"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuQMnQJWp7Qi"
      },
      "source": [
        "### Trainning and evaluation Functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWuc6c-zoojB"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text, text_lengths = batch.review\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        loss = criterion(predictions, batch.sentiment)\n",
        "        acc = binary_accuracy(predictions, batch.sentiment)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, text_lengths = batch.review\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            loss = criterion(predictions, batch.sentiment)\n",
        "            acc = binary_accuracy(predictions, batch.sentiment)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhvBTP5OqKL-"
      },
      "source": [
        "We'll also create a function to tell us how long an epoch takes to compare training times between models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WhT7wkToonV"
      },
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LEZP7cWqPwG"
      },
      "source": [
        "### Trainning Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zSJTbCEoopE",
        "outputId": "8bcf9576-cec7-4b79-c138-5756a352ce6e"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(imdb_model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(imdb_model, validation_iterator, criterion)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(imdb_model.state_dict(), 'best-model.pt')\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 2m 37s\n",
            "\tTrain Loss: 0.680 | Train Acc: 56.30%\n",
            "\t Val. Loss: 0.633 |  Val. Acc: 64.48%\n",
            "Epoch: 02 | Epoch Time: 2m 39s\n",
            "\tTrain Loss: 0.559 | Train Acc: 70.85%\n",
            "\t Val. Loss: 0.484 |  Val. Acc: 77.31%\n",
            "Epoch: 03 | Epoch Time: 2m 39s\n",
            "\tTrain Loss: 0.326 | Train Acc: 86.28%\n",
            "\t Val. Loss: 0.254 |  Val. Acc: 89.85%\n",
            "Epoch: 04 | Epoch Time: 2m 39s\n",
            "\tTrain Loss: 0.266 | Train Acc: 89.15%\n",
            "\t Val. Loss: 0.271 |  Val. Acc: 88.69%\n",
            "Epoch: 05 | Epoch Time: 2m 38s\n",
            "\tTrain Loss: 0.222 | Train Acc: 91.31%\n",
            "\t Val. Loss: 0.252 |  Val. Acc: 89.22%\n",
            "Epoch: 06 | Epoch Time: 2m 39s\n",
            "\tTrain Loss: 0.195 | Train Acc: 92.47%\n",
            "\t Val. Loss: 0.224 |  Val. Acc: 91.36%\n",
            "Epoch: 07 | Epoch Time: 2m 39s\n",
            "\tTrain Loss: 0.178 | Train Acc: 93.15%\n",
            "\t Val. Loss: 0.233 |  Val. Acc: 91.40%\n",
            "Epoch: 08 | Epoch Time: 2m 39s\n",
            "\tTrain Loss: 0.158 | Train Acc: 94.05%\n",
            "\t Val. Loss: 0.224 |  Val. Acc: 91.93%\n",
            "Epoch: 09 | Epoch Time: 2m 39s\n",
            "\tTrain Loss: 0.140 | Train Acc: 94.83%\n",
            "\t Val. Loss: 0.230 |  Val. Acc: 91.71%\n",
            "Epoch: 10 | Epoch Time: 2m 39s\n",
            "\tTrain Loss: 0.129 | Train Acc: 95.21%\n",
            "\t Val. Loss: 0.239 |  Val. Acc: 91.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm6C3rlIFJn3"
      },
      "source": [
        "### Evaluating the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0xDEYz1oovX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57923c25-b9fb-497b-f688-751bd64195a3"
      },
      "source": [
        "imdb_model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(imdb_model, test_iterator, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.243 | Test Acc: 90.75%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRd4xDQwFubf"
      },
      "source": [
        "### Model Inference - Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reP3lwspooz7"
      },
      "source": [
        "import spacy\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "sentiments = [\"NEG\", \"POS\"]\n",
        "def predict_sentiment(model, sent):\n",
        "  model.eval()\n",
        "  tokenized = [tok.text for tok in nlp.tokenizer(sent)]\n",
        "  indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "  length = [len(indexed)]\n",
        "  tensor = torch.LongTensor(indexed).to(device)\n",
        "  tensor = tensor.unsqueeze(1)\n",
        "  length_tensor = torch.LongTensor(length)\n",
        "  prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "\n",
        "  predicted_class = round(prediction.item())\n",
        "  confidence = prediction.item() if prediction.item() >= .5 else 1 - prediction.item()\n",
        "  print(f'PREDICTED CLASS:\\t{predicted_class}\\nCONFIDENCE:\\t\\t{confidence * 100:.2f}%\\nSENTIMENT:\\t\\t{sentiments[predicted_class]}')"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVEc71MrIjZQ"
      },
      "source": [
        "### Negative review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAzsMzkCoo3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0cab599-2ca4-4fa3-ce33-e23561f836b1"
      },
      "source": [
        "predict_sentiment(imdb_model, \"This movie is boring\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTED CLASS:\t0\n",
            "CONFIDENCE:\t\t99.93%\n",
            "SENTIMENT:\t\tNEG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WN3HQv0yPpt",
        "outputId": "2f0e7d3d-ace4-4db9-c41e-3bf2f853458e"
      },
      "source": [
        "predict_sentiment(imdb_model, \"This movie is bad.\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTED CLASS:\t0\n",
            "CONFIDENCE:\t\t99.58%\n",
            "SENTIMENT:\t\tNEG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CKI6umcImtx"
      },
      "source": [
        "### Positive review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEPwGxu0kyp3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad23d454-7222-49fd-d0b9-6dad46936b51"
      },
      "source": [
        "predict_sentiment(imdb_model, \"This movie is the best.\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTED CLASS:\t1\n",
            "CONFIDENCE:\t\t98.83%\n",
            "SENTIMENT:\t\tPOS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jy89pjU4yc99",
        "outputId": "ba17e17e-2b6a-411c-a644-7c15527779d5"
      },
      "source": [
        "predict_sentiment(imdb_model, \"This movie is good.\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTED CLASS:\t1\n",
            "CONFIDENCE:\t\t93.05%\n",
            "SENTIMENT:\t\tPOS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR2Q512SyO9W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}