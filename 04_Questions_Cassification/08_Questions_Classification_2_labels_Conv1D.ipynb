{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08_Questions_Classification_2_labels_Conv1D.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwFkL0ZK1L-X"
      },
      "source": [
        "### Questions Classification Custom dataset and modified `Conv1D`.\n",
        "\n",
        "In the previous Notebook we did  question classification with two labels using  using Conv2d layers.In this notebook are are going to use [this notebook](https://github.com/CrispenGari/nlp-pytorch/blob/main/04_Questions_Cassification/04_Questions_Classification_Conv1D.ipynb) as the base notebook to perform the two label quetsions classifications on our toy dataset using `Conv1D` layers.\n",
        "\n",
        "\n",
        "The notebook will remain the same, Where there's a change i will highlight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl1NzpCf2rlk"
      },
      "source": [
        "### Data preparation using torchtext.\n",
        "\n",
        "In this notebook we are not going to prepare the data, because it has already been prepared for us in file. What we are going to do is to load the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5bfB7PC1HRJ",
        "outputId": "a157c0d6-a55c-4345-82a7-0b318244916c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpcZ-TG63VID"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qpE80WdY3TgV",
        "outputId": "61e4f391-3f5c-4e9e-8207-51750796bf0d"
      },
      "source": [
        "import time\n",
        "from prettytable import PrettyTable\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import torch, os, random\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.9.0+cu102'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAkhI0Ns3wiU"
      },
      "source": [
        "### Setting up the seeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwYV_hrz3sSB"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deteministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlPBQXKmGEFk"
      },
      "source": [
        "### File names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBBoZ8MqGsGR"
      },
      "source": [
        "train_path_json = 'train.json'\n",
        "test_path_json = 'test.json'\n",
        "val_path_json = 'val.json'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp6yR3gwyddd"
      },
      "source": [
        "### Conv1D\n",
        "\n",
        "The data preprocessing/processing will remain the same as from the prevous notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7YMbs8a43jD"
      },
      "source": [
        "### Creating the Fields.\n",
        "\n",
        "We don't need to pass arg `include_lengths` to `True` this time around."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_xUMByB5tGa"
      },
      "source": [
        "from torchtext.legacy import data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RaqW-b-4lJJ"
      },
      "source": [
        "TEXT = data.Field(\n",
        "   tokenize=\"spacy\",\n",
        "   batch_first=True,\n",
        "  tokenizer_language = 'en_core_web_sm',\n",
        ")\n",
        "LABEL_1 = data.LabelField()\n",
        "LABEL_2 = data.LabelField()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OugU18S_7tAS"
      },
      "source": [
        "fields = {\n",
        "  \"Questions\": ('text', TEXT),\n",
        "  \"Category1\": ('label_1', LABEL_1),\n",
        "  \"Category2\":('label_2', LABEL_2),\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiNOBqFO8pyo"
      },
      "source": [
        "### Creating the dataset.\n",
        "\n",
        "We ar going to use the `TabularDataset.split()` to create the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAgiWOoh8mWO"
      },
      "source": [
        "files_path = '/content/drive/MyDrive/NLP Data/questions-classification/pytorch'\n",
        "train_data, val_data, test_data = data.TabularDataset.splits(\n",
        "   files_path,\n",
        "   train=train_path_json,\n",
        "   test= test_path_json,\n",
        "   validation= val_path_json,\n",
        "   format = \"json\",\n",
        "   fields=fields\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f4YEu-fQ1WK",
        "outputId": "026621eb-95ce-40fa-e0b6-c0d469a64c10"
      },
      "source": [
        "len(train_data), len(test_data), len(val_data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5179, 28, 245)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ1oWIQW9KWn",
        "outputId": "a6d71c3d-0e20-4828-b8ab-0c0b996df190"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['What', 'is', 'the', 'name', 'of', 'Miss', 'India', '1994', '?'], 'label_1': 'HUM', 'label_2': 'ind'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t-0JC1U-l73"
      },
      "source": [
        "### Building the Vocabulary and Loading the `pretrained` word vectors.\n",
        "\n",
        "We are going to use the `glove.6B.100d` word vectors which was trained with 6 billion words and each word is a 100 dimesional vector.\n",
        "\n",
        "**Note** We should only build the vocabulary on the `train` dataset only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4p3m1_--WYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4b7d06b-8c2d-4162-cab5-10d2a1e743b5"
      },
      "source": [
        "MAX_VOCAB_SIZE = 100_000_000\n",
        "\n",
        "TEXT.build_vocab(\n",
        "    train_data,\n",
        "     max_size = MAX_VOCAB_SIZE,\n",
        "    vectors = \"glove.6B.100d\",\n",
        "    unk_init = torch.Tensor.normal_\n",
        ")\n",
        "LABEL_1.build_vocab(train_data)\n",
        "LABEL_2.build_vocab(train_data)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:42, 5.31MB/s]                           \n",
            "100%|█████████▉| 398040/400000 [00:20<00:00, 20066.01it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF6qioSVHWMw"
      },
      "source": [
        "### Checking the labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4faA0_LHZA3",
        "outputId": "57119d79-f224-430f-8a24-0bea35800c0e"
      },
      "source": [
        "print(\"************************ FIRST LABELS ********************************\")\n",
        "print(LABEL_1.vocab.stoi)\n",
        "print(\"************************ SECOND LABELS ********************************\")\n",
        "print(LABEL_2.vocab.stoi)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "************************ FIRST LABELS ********************************\n",
            "defaultdict(None, {'ENTY': 0, 'HUM': 1, 'DESC': 2, 'NUM': 3, 'LOC': 4, 'ABBR': 5})\n",
            "************************ SECOND LABELS ********************************\n",
            "defaultdict(None, {'ind': 0, 'other': 1, 'def': 2, 'count': 3, 'desc': 4, 'manner': 5, 'cremat': 6, 'date': 7, 'gr': 8, 'reason': 9, 'country': 10, 'city': 11, 'animal': 12, 'food': 13, 'dismed': 14, 'termeq': 15, 'period': 16, 'money': 17, 'exp': 18, 'state': 19, 'sport': 20, 'event': 21, 'product': 22, 'substance': 23, 'techmeth': 24, 'color': 25, 'dist': 26, 'perc': 27, 'veh': 28, 'word': 29, 'title': 30, 'mount': 31, 'body': 32, 'abb': 33, 'lang': 34, 'volsize': 35, 'plant': 36, 'symbol': 37, 'instru': 38, 'weight': 39, 'code': 40, 'letter': 41, 'speed': 42, 'temp': 43, 'ord': 44, 'currency': 45, 'religion': 46})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFukx38W_aNv"
      },
      "source": [
        "### Device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYPGTIZ7_Zet",
        "outputId": "85d0e60f-e097-4eb4-da2c-5e3897890606"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaCMz4yZ_n5W"
      },
      "source": [
        "### Creating iterators.\n",
        "\n",
        "We are going to use our favorite iterator known as the `BucketIterator` to create iterators for all the sets that we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPDrPFKQ_mf1"
      },
      "source": [
        "sort_key = lambda x: len(x.text)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    device = device,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = sort_key,\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spZX8zodA91F"
      },
      "source": [
        "### Creating the Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReThGRspA7uO"
      },
      "source": [
        "class QuestionsConv1DNet(nn.Module):\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               embedding_size,\n",
        "               n_filters,\n",
        "               filter_sizes,\n",
        "               output_size_1,\n",
        "               output_size_2,\n",
        "               pad_idx,\n",
        "               dropout=.5\n",
        "               ):\n",
        "    super(QuestionsConv1DNet, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(\n",
        "        vocab_size, embedding_size, padding_idx=pad_idx\n",
        "    )\n",
        "    self.convs = nn.ModuleList([\n",
        "        nn.Conv1d(\n",
        "            in_channels=embedding_size,\n",
        "            out_channels = n_filters,\n",
        "            kernel_size=fs\n",
        "        ) for fs in filter_sizes \n",
        "    ])\n",
        "\n",
        "    self.fc_1 = nn.Linear(len(filter_sizes) * n_filters,\n",
        "                          output_size_1)\n",
        "    self.fc_2 = nn.Linear(len(filter_sizes) * n_filters,\n",
        "                          output_size_2)\n",
        "    \n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, text):\n",
        "    embedded = self.embedding(text).permute(0, 2, 1)\n",
        "    conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "    pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2)\n",
        "            for conv in conved\n",
        "            ]\n",
        "    cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "    return self.fc_1(cat), self.fc_2(cat)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2IVtv8EDhJd"
      },
      "source": [
        "### Creating the model instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbJdW058DgTl",
        "outputId": "b0f7a7e7-82cd-46ee-a79c-83eeb4379c03"
      },
      "source": [
        "\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5]\n",
        "OUTPUT_DIM_1 =  len(LABEL_1.vocab)\n",
        "OUTPUT_DIM_2 =  len(LABEL_2.vocab)\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] \n",
        "DROPOUT = 0.5\n",
        "\n",
        "questions_model = QuestionsConv1DNet(\n",
        "            INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            N_FILTERS,\n",
        "            FILTER_SIZES,\n",
        "            OUTPUT_DIM_1, \n",
        "            OUTPUT_DIM_2,\n",
        "            pad_idx = PAD_IDX,\n",
        "            dropout=DROPOUT\n",
        "            ).to(device)\n",
        "questions_model"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuestionsConv1DNet(\n",
              "  (embedding): Embedding(9053, 100, padding_idx=1)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "    (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "    (2): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "    (3): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "    (4): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "    (5): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "    (6): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "    (7): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "    (8): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "    (9): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "    (10): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "  )\n",
              "  (fc_1): Linear(in_features=1100, out_features=6, bias=True)\n",
              "  (fc_2): Linear(in_features=1100, out_features=47, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acQD-B6zEE72"
      },
      "source": [
        "### Model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQaztp4ID-HI",
        "outputId": "9baedf52-bfda-4050-c053-5d3d60296905"
      },
      "source": [
        "\n",
        "def count_trainable_params(model):\n",
        "  return sum(p.numel() for p in model.parameters()), sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "n_params, trainable_params = count_trainable_params(questions_model)\n",
        "print(f\"Total number of paramaters: {n_params:,}\\nTotal tainable parameters: {trainable_params:,}\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of paramaters: 1,394,753\n",
            "Total tainable parameters: 1,394,753\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8gjXny7EUNb"
      },
      "source": [
        "### Loading pretrained vextors to the embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULe__PiKEQS8"
      },
      "source": [
        "pretrained_embeddings  = TEXT.vocab.vectors"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeXa9ycREfuV",
        "outputId": "724a05b8-7a49-425a-fc66-13310acbf5e2"
      },
      "source": [
        "questions_model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.1233,  0.3499,  0.6173],\n",
              "        [ 0.7262,  0.0912, -0.3891,  ...,  0.0821,  0.4440, -0.7240],\n",
              "        [ 0.1638,  0.6046,  1.0789,  ..., -0.3140,  0.1844,  0.3624],\n",
              "        ...,\n",
              "        [ 0.0091,  0.2810,  0.7356,  ..., -0.7508,  0.8967, -0.7631],\n",
              "        [ 0.2906,  0.3217,  0.2419,  ..., -0.9444, -0.3790,  0.6196],\n",
              "        [-0.3898, -0.5949,  0.2729,  ..., -1.0948,  0.8617, -0.4429]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q27hKnrpEoKF"
      },
      "source": [
        "### Zeroing the `<pad>` and `<unk>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_qvO2teEmvh",
        "outputId": "fdd116a0-2480-4897-a29b-d38689183aad"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token] or TEXT.vocab.stoi[\"<unk>\"]\n",
        "questions_model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "questions_model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "questions_model.embedding.weight.data"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.1638,  0.6046,  1.0789,  ..., -0.3140,  0.1844,  0.3624],\n",
              "        ...,\n",
              "        [ 0.0091,  0.2810,  0.7356,  ..., -0.7508,  0.8967, -0.7631],\n",
              "        [ 0.2906,  0.3217,  0.2419,  ..., -0.9444, -0.3790,  0.6196],\n",
              "        [-0.3898, -0.5949,  0.2729,  ..., -1.0948,  0.8617, -0.4429]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm2RklTLE-Dv"
      },
      "source": [
        "### Loss and optimizer.\n",
        "For the loss we are going to create 2 loss functions. We are going to use the `Adam` as our optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7UCyAlYE1nA"
      },
      "source": [
        "optimizer = torch.optim.Adam(questions_model.parameters())\n",
        "criterion_1 = nn.CrossEntropyLoss().to(device)\n",
        "criterion_2 = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi2tk-kSFTVq"
      },
      "source": [
        "### Accuracy function.\n",
        "We are going to create the `categorical_accuracy()` function that will calculate the categorical accuracy for predicted labels and actual labels.\n",
        "\n",
        "**Note**: this function will remain the same we are just going to reuse it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRCfQZvmFPBS"
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "  top_pred = preds.argmax(1, keepdim = True)\n",
        "  correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "  return correct.float() / y.shape[0]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gf_nyXVFvOG"
      },
      "source": [
        "### Training and Evaluation functions.\n",
        "\n",
        "In the train and evaluate function we are going to change a lot of things. I will highlight the changes using comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80QEfQeaFuVZ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion_1, criterion_2):\n",
        "    \"\"\"\n",
        "    Losses and accuracy should be of different labels\n",
        "    \"\"\"\n",
        "    epoch_loss_1 = 0\n",
        "    epoch_acc_1 = 0\n",
        "    epoch_loss_2 = 0\n",
        "    epoch_acc_2 = 0\n",
        "\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text = batch.text\n",
        "        \"\"\"\n",
        "        The model returns two predictions for different labels.\n",
        "        \"\"\"\n",
        "        predictions_1, predictions_2 = model(text)\n",
        "        predictions_1 = predictions_1.squeeze(1)\n",
        "        predictions_2 = predictions_2.squeeze(1)\n",
        "\n",
        "        \"\"\"\n",
        "        Get the loss for each label\n",
        "        \"\"\"\n",
        "        loss_1 = criterion_1(predictions_1, batch.label_1) # we are using label 1 to calculate the loss for the first label\n",
        "        loss_2 = criterion_2(predictions_2, batch.label_2) # we are using label 2 to calculate the loss for the first label\n",
        "\n",
        "        acc_1 = categorical_accuracy(predictions_1, batch.label_1) # accuracy for the first label\n",
        "        acc_2 = categorical_accuracy(predictions_2, batch.label_2) # accuracy for the first label\n",
        "        \n",
        "        \"\"\"\n",
        "        We have to sum the loss before back propagation\n",
        "        \"\"\"\n",
        "        loss = loss_1 + loss_2\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \"\"\"\n",
        "        ********* METRICS ************\n",
        "        \"\"\"\n",
        "        epoch_loss_1 += loss_1.item()\n",
        "        epoch_loss_2 += loss_2.item()\n",
        "        epoch_acc_1 += acc_1.item()\n",
        "        epoch_acc_2 += acc_2.item()\n",
        "    return epoch_loss_1 / len(iterator), epoch_loss_2 / len(iterator), epoch_acc_1 / len(iterator), epoch_acc_2/ len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion_1, criterion_2):\n",
        "    \"\"\"\n",
        "    Losses and accuracy should be of different labels\n",
        "    \"\"\"\n",
        "    epoch_loss_1 = 0\n",
        "    epoch_acc_1 = 0\n",
        "    epoch_loss_2 = 0\n",
        "    epoch_acc_2 = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch in iterator:\n",
        "          text = batch.text\n",
        "          \"\"\"\n",
        "          The model returns two predictions for different labels.\n",
        "          \"\"\"\n",
        "          predictions_1, predictions_2 = model(text)\n",
        "          predictions_1 = predictions_1.squeeze(1)\n",
        "          predictions_2 = predictions_2.squeeze(1)\n",
        "          \"\"\"\n",
        "          Get the loss for each label\n",
        "          \"\"\"\n",
        "          loss_1 = criterion_1(predictions_1, batch.label_1) # we are using label 1 to calculate the loss for the first label\n",
        "          loss_2 = criterion_2(predictions_2, batch.label_2) # we are using label 2 to calculate the loss for the first label\n",
        "\n",
        "          acc_1 = categorical_accuracy(predictions_1, batch.label_1) # accuracy for the first label\n",
        "          acc_2 = categorical_accuracy(predictions_2, batch.label_2) # accuracy for the first label\n",
        "          \"\"\"\n",
        "          ********* METRICS ************\n",
        "          \"\"\"\n",
        "          epoch_loss_1 += loss_1.item()\n",
        "          epoch_loss_2 += loss_2.item()\n",
        "          epoch_acc_1 += acc_1.item()\n",
        "          epoch_acc_2 += acc_2.item()\n",
        "    return epoch_loss_1 / len(iterator), epoch_loss_2 / len(iterator), epoch_acc_1 / len(iterator), epoch_acc_2/ len(iterator)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdtkgFFJG1WS"
      },
      "source": [
        "### Training loop.\n",
        "We are going to create helper functions that will help us to visualize our training.\n",
        "\n",
        "1. Time to string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMH6iA1AG0tj"
      },
      "source": [
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp5OjGiPHR6h"
      },
      "source": [
        "2. tabulate training epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtCTm84HHQb5"
      },
      "source": [
        "def visualize_training(start, end, train_loss_1, train_loss_2, train_accuracy_1, train_accuracy_2, \n",
        "                       val_loss_1, val_loss_2, val_accuracy_1, val_accuracy_2, title):\n",
        "  data = [\n",
        "       [\"Training\", f'{train_loss_1:.3f}',  f'{train_loss_2:.3f}', f'{train_accuracy_1:.3f}', f'{train_accuracy_2:.3f}', f\"{hms_string(end - start)}\" ],\n",
        "       [\"Validation\", f'{val_loss_1:.3f}', f'{val_loss_2:.3f}', f'{val_accuracy_1:.3f}', f'{val_accuracy_2:.3f}', \"\" ],       \n",
        "  ]\n",
        "  table = PrettyTable([\"CATEGORY\", \"LOSS_1\", \"LOSS_2\", \"ACCURACY_1\", \"ACCURACY_2\", \"ETA\"])\n",
        "  table.align[\"CATEGORY\"] = 'l'\n",
        "  table.align[\"ETA\"] = 'r'\n",
        "  table.align[\"LOSS_1\"] = 'r'\n",
        "  table.align[\"ACCURACY_1\"] = 'r'\n",
        "  table.align[\"LOSS_2\"] = 'r'\n",
        "  table.align[\"ACCURACY_2\"] = 'r'\n",
        "  table.title = title\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcu_WMc2HXqb",
        "outputId": "620322dc-cb6b-4b53-ab81-63bf1de275c7"
      },
      "source": [
        "N_EPOCHS = 30\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss_1, train_loss_2, train_acc_1, train_acc_2 = train(questions_model, train_iter, \n",
        "                                                                 optimizer, criterion_1, criterion_2)\n",
        "    \n",
        "    valid_loss_1, valid_loss_2, valid_acc_1, valid_acc_2 = evaluate(questions_model, val_iter, \n",
        "                                                                    criterion_1, criterion_2)\n",
        "    title = f\"EPOCH: {epoch+1:02}/{N_EPOCHS:02} {'saving best model...' if valid_loss_2 < best_valid_loss else 'not saving...'}\"\n",
        "    \"\"\"\n",
        "    We are going to check for the validation loss of the second label with 47 \n",
        "    classes feel free to check on the loss you want during model saving\n",
        "    \"\"\"\n",
        "    if valid_loss_2 < best_valid_loss:\n",
        "        best_valid_loss = valid_loss_2\n",
        "        torch.save(questions_model.state_dict(), 'best-model.pt')\n",
        "    end = time.time()\n",
        "    visualize_training(start, end, train_loss_1, train_loss_2, train_acc_1, train_acc_2, \n",
        "                       valid_loss_1, valid_loss_2, valid_acc_1, valid_acc_2, title)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------+\n",
            "|                  EPOCH: 03/30 saving best model...                  |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.549 |  1.341 |      0.809 |      0.644 | 0:00:00.95 |\n",
            "| Validation |  0.634 |  1.432 |      0.780 |      0.634 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                  EPOCH: 04/30 saving best model...                  |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.399 |  1.017 |      0.867 |      0.732 | 0:00:00.95 |\n",
            "| Validation |  0.549 |  1.245 |      0.794 |      0.691 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                  EPOCH: 05/30 saving best model...                  |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.290 |  0.761 |      0.906 |      0.807 | 0:00:00.94 |\n",
            "| Validation |  0.505 |  1.090 |      0.814 |      0.712 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                  EPOCH: 06/30 saving best model...                  |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.221 |  0.566 |      0.935 |      0.863 | 0:00:00.95 |\n",
            "| Validation |  0.491 |  1.011 |      0.823 |      0.731 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                  EPOCH: 07/30 saving best model...                  |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.158 |  0.425 |      0.955 |      0.899 | 0:00:00.96 |\n",
            "| Validation |  0.460 |  0.936 |      0.830 |      0.739 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                  EPOCH: 08/30 saving best model...                  |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.121 |  0.324 |      0.966 |      0.933 | 0:00:00.95 |\n",
            "| Validation |  0.446 |  0.891 |      0.831 |      0.752 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                      EPOCH: 09/30 not saving...                     |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.087 |  0.244 |      0.981 |      0.950 | 0:00:00.93 |\n",
            "| Validation |  0.431 |  0.896 |      0.851 |      0.759 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                  EPOCH: 10/30 saving best model...                  |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.069 |  0.190 |      0.983 |      0.965 | 0:00:00.96 |\n",
            "| Validation |  0.471 |  0.856 |      0.843 |      0.756 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                  EPOCH: 11/30 saving best model...                  |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.052 |  0.143 |      0.990 |      0.978 | 0:00:00.94 |\n",
            "| Validation |  0.452 |  0.853 |      0.847 |      0.755 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                  EPOCH: 12/30 saving best model...                  |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.042 |  0.109 |      0.993 |      0.986 | 0:00:00.95 |\n",
            "| Validation |  0.461 |  0.844 |      0.847 |      0.784 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                  EPOCH: 13/30 saving best model...                  |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.032 |  0.089 |      0.996 |      0.988 | 0:00:00.94 |\n",
            "| Validation |  0.448 |  0.842 |      0.851 |      0.759 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                      EPOCH: 14/30 not saving...                     |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.027 |  0.070 |      0.997 |      0.993 | 0:00:00.95 |\n",
            "| Validation |  0.458 |  0.846 |      0.847 |      0.759 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                  EPOCH: 15/30 saving best model...                  |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.023 |  0.061 |      0.997 |      0.993 | 0:00:00.95 |\n",
            "| Validation |  0.468 |  0.831 |      0.855 |      0.783 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                  EPOCH: 16/30 saving best model...                  |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.020 |  0.049 |      0.998 |      0.996 | 0:00:00.96 |\n",
            "| Validation |  0.454 |  0.819 |      0.847 |      0.787 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                      EPOCH: 17/30 not saving...                     |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.018 |  0.042 |      0.998 |      0.996 | 0:00:00.93 |\n",
            "| Validation |  0.463 |  0.836 |      0.830 |      0.770 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                      EPOCH: 18/30 not saving...                     |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.013 |  0.037 |      0.998 |      0.997 | 0:00:00.94 |\n",
            "| Validation |  0.462 |  0.834 |      0.850 |      0.788 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                      EPOCH: 19/30 not saving...                     |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.015 |  0.033 |      0.998 |      0.997 | 0:00:00.94 |\n",
            "| Validation |  0.462 |  0.850 |      0.859 |      0.771 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                      EPOCH: 20/30 not saving...                     |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.013 |  0.030 |      0.999 |      0.998 | 0:00:00.94 |\n",
            "| Validation |  0.469 |  0.849 |      0.850 |      0.783 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                      EPOCH: 21/30 not saving...                     |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.011 |  0.025 |      0.999 |      0.998 | 0:00:00.94 |\n",
            "| Validation |  0.492 |  0.869 |      0.842 |      0.779 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                      EPOCH: 22/30 not saving...                     |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.010 |  0.024 |      0.998 |      0.998 | 0:00:00.94 |\n",
            "| Validation |  0.477 |  0.833 |      0.847 |      0.771 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                      EPOCH: 23/30 not saving...                     |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.008 |  0.020 |      0.999 |      0.999 | 0:00:00.95 |\n",
            "| Validation |  0.482 |  0.870 |      0.863 |      0.771 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                      EPOCH: 24/30 not saving...                     |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.008 |  0.019 |      0.999 |      0.999 | 0:00:00.94 |\n",
            "| Validation |  0.507 |  0.881 |      0.854 |      0.783 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                      EPOCH: 25/30 not saving...                     |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.009 |  0.019 |      0.998 |      0.998 | 0:00:00.94 |\n",
            "| Validation |  0.489 |  0.835 |      0.854 |      0.779 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                      EPOCH: 26/30 not saving...                     |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.007 |  0.015 |      0.999 |      0.999 | 0:00:00.92 |\n",
            "| Validation |  0.507 |  0.880 |      0.858 |      0.779 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                      EPOCH: 27/30 not saving...                     |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.006 |  0.014 |      1.000 |      0.999 | 0:00:00.94 |\n",
            "| Validation |  0.519 |  0.896 |      0.850 |      0.763 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                      EPOCH: 28/30 not saving...                     |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.005 |  0.014 |      0.999 |      0.999 | 0:00:00.94 |\n",
            "| Validation |  0.523 |  0.882 |      0.851 |      0.775 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                      EPOCH: 29/30 not saving...                     |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.006 |  0.012 |      0.999 |      0.999 | 0:00:00.93 |\n",
            "| Validation |  0.542 |  0.866 |      0.842 |      0.783 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "+---------------------------------------------------------------------+\n",
            "|                      EPOCH: 30/30 not saving...                     |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| CATEGORY   | LOSS_1 | LOSS_2 | ACCURACY_1 | ACCURACY_2 |        ETA |\n",
            "+------------+--------+--------+------------+------------+------------+\n",
            "| Training   |  0.005 |  0.013 |      1.000 |      0.999 | 0:00:00.95 |\n",
            "| Validation |  0.507 |  0.915 |      0.863 |      0.788 |            |\n",
            "+------------+--------+--------+------------+------------+------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZNkikbDIB2z"
      },
      "source": [
        "### Model Evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8fWTBruHw1p",
        "outputId": "f0c1afe8-a365-4494-c68c-1cd283741cd8"
      },
      "source": [
        "questions_model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "test_loss_1, test_loss_2, test_acc_1, test_acc_2 = evaluate(questions_model, test_iter, criterion_1, criterion_2)\n",
        "print(f'Test Loss 1: {test_loss_1:.3f} | Test Loss 2: {test_loss_2:.3f}  | Test Acc 1: {test_acc_1*100:.2f}% | Test Acc 2: {test_acc_2*100:.2f}%')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss 1: 0.843 | Test Loss 2: 1.459  | Test Acc 1: 75.00% | Test Acc 2: 60.71%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4zEBLYsLki5"
      },
      "source": [
        "### Model Inference.\n",
        "\n",
        "We are now ready to make predictions with our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyNLifHaJBKS"
      },
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clNBiJ9NPkRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d62a2f7b-1232-4c0f-8a06-4e24d831ef11"
      },
      "source": [
        "reversed_labels_1 = dict([(v, k) for (k, v) in LABEL_1.vocab.stoi.items()])\n",
        "reversed_labels_2 = dict([(v, k) for (k, v) in LABEL_2.vocab.stoi.items()])\n",
        "\n",
        "reversed_labels_1, reversed_labels_2"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({0: 'ENTY', 1: 'HUM', 2: 'DESC', 3: 'NUM', 4: 'LOC', 5: 'ABBR'},\n",
              " {0: 'ind',\n",
              "  1: 'other',\n",
              "  2: 'def',\n",
              "  3: 'count',\n",
              "  4: 'desc',\n",
              "  5: 'manner',\n",
              "  6: 'cremat',\n",
              "  7: 'date',\n",
              "  8: 'gr',\n",
              "  9: 'reason',\n",
              "  10: 'country',\n",
              "  11: 'city',\n",
              "  12: 'animal',\n",
              "  13: 'food',\n",
              "  14: 'dismed',\n",
              "  15: 'termeq',\n",
              "  16: 'period',\n",
              "  17: 'money',\n",
              "  18: 'exp',\n",
              "  19: 'state',\n",
              "  20: 'sport',\n",
              "  21: 'event',\n",
              "  22: 'product',\n",
              "  23: 'substance',\n",
              "  24: 'techmeth',\n",
              "  25: 'color',\n",
              "  26: 'dist',\n",
              "  27: 'perc',\n",
              "  28: 'veh',\n",
              "  29: 'word',\n",
              "  30: 'title',\n",
              "  31: 'mount',\n",
              "  32: 'body',\n",
              "  33: 'abb',\n",
              "  34: 'lang',\n",
              "  35: 'volsize',\n",
              "  36: 'plant',\n",
              "  37: 'symbol',\n",
              "  38: 'instru',\n",
              "  39: 'weight',\n",
              "  40: 'code',\n",
              "  41: 'letter',\n",
              "  42: 'speed',\n",
              "  43: 'temp',\n",
              "  44: 'ord',\n",
              "  45: 'currency',\n",
              "  46: 'religion'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9A6ZGTILzWD"
      },
      "source": [
        "def tabulate(column_names, data, title=\"QUESTIONS PREDICTIONS TABLE\"):\n",
        "  table = PrettyTable(column_names)\n",
        "  table.align[column_names[0]] = \"l\"\n",
        "  table.align[column_names[1]] = \"l\"\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)\n",
        "\n",
        "def predict_question_type(model, sentence, min_len = 5, actual_class_1=0, actual_class_2=0):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "     \n",
        "      if len(tokenized) < min_len:\n",
        "          tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
        "      indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "      tensor = torch.LongTensor(indexed).to(device).unsqueeze(0)\n",
        "      probabilities_1, probabilities_2 = model(tensor)\n",
        "      prediction_1 = torch.argmax(probabilities_1, dim=1).item()\n",
        "      prediction_2 = torch.argmax(probabilities_2, dim=1).item()\n",
        "      table_headers =[\"KEY\", \"VALUE\"]\n",
        "      table_data = [\n",
        "          [\"PREDICTED CLASS 1\",  prediction_1],\n",
        "          [\"ACTUAL CLASS 1\", actual_class_1],\n",
        "          [\"PREDICTED CLASS 2\",  prediction_2],\n",
        "          [\"ACTUAL CLASS 2\", actual_class_2],\n",
        "          [\"PREDICTED CLASS NAME 1\",  reversed_labels_1[prediction_1]],    \n",
        "          [\"PREDICTED CLASS NAME 2\",  reversed_labels_2[prediction_2]],    \n",
        "      ]\n",
        "      tabulate(table_headers, table_data)\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W93zzSlQMO4p"
      },
      "source": [
        "###  Entity and Other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcuHlaPwML4Y",
        "outputId": "5cdb82fb-8a7a-4271-f1fc-4b10d45f962e"
      },
      "source": [
        "predict_question_type(questions_model, \"What kind of weapons were used in Medieval warfare ?\", \n",
        "                      actual_class_1=LABEL_1.vocab.stoi[\"ENTY\"], actual_class_2=LABEL_2.vocab.stoi[\"other\"]\n",
        "                      )"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------+-------+\n",
            "| KEY                    | VALUE |\n",
            "+------------------------+-------+\n",
            "| PREDICTED CLASS 1      | 0     |\n",
            "| ACTUAL CLASS 1         | 0     |\n",
            "| PREDICTED CLASS 2      | 1     |\n",
            "| ACTUAL CLASS 2         | 1     |\n",
            "| PREDICTED CLASS NAME 1 | ENTY  |\n",
            "| PREDICTED CLASS NAME 2 | other |\n",
            "+------------------------+-------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5Eqjo0MMP8L"
      },
      "source": [
        "### Human and IND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnhFchDiMQQn",
        "outputId": "3e19032b-3483-4edc-f0b8-25df9193eb44"
      },
      "source": [
        "predict_question_type(questions_model, \"Whose video is titled Shape Up with Arnold ?\", \n",
        "                      actual_class_1=LABEL_1.vocab.stoi[\"HUM\"], actual_class_2=LABEL_2.vocab.stoi[\"ind\"]\n",
        "                      )"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------+-------+\n",
            "| KEY                    | VALUE |\n",
            "+------------------------+-------+\n",
            "| PREDICTED CLASS 1      | 1     |\n",
            "| ACTUAL CLASS 1         | 1     |\n",
            "| PREDICTED CLASS 2      | 0     |\n",
            "| ACTUAL CLASS 2         | 0     |\n",
            "| PREDICTED CLASS NAME 1 | HUM   |\n",
            "| PREDICTED CLASS NAME 2 | ind   |\n",
            "+------------------------+-------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHlRIuOWvvXU"
      },
      "source": [
        "### Description and DESC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbuo0D9iv1pi",
        "outputId": "a214748d-b698-41a2-d602-6ac48f2d6a57"
      },
      "source": [
        "predict_question_type(questions_model, \"What 's the Olympic motto ?\", \n",
        "                      actual_class_1=LABEL_1.vocab.stoi[\"DESC\"], actual_class_2=LABEL_2.vocab.stoi[\"desc\"]\n",
        "                      )"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------+-------+\n",
            "| KEY                    | VALUE |\n",
            "+------------------------+-------+\n",
            "| PREDICTED CLASS 1      | 0     |\n",
            "| ACTUAL CLASS 1         | 2     |\n",
            "| PREDICTED CLASS 2      | 1     |\n",
            "| ACTUAL CLASS 2         | 4     |\n",
            "| PREDICTED CLASS NAME 1 | ENTY  |\n",
            "| PREDICTED CLASS NAME 2 | other |\n",
            "+------------------------+-------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErOffK_DwNQA"
      },
      "source": [
        "### Location and STATE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63HWwLTmwHkd",
        "outputId": "d44e7a10-14b6-4709-f67c-6e9f795a0f0f"
      },
      "source": [
        "predict_question_type(questions_model, \"What state full of milk and honey was the destination in The Grapes of Wrath ?\", \n",
        "                      actual_class_1=LABEL_1.vocab.stoi[\"LOC\"], actual_class_2=LABEL_2.vocab.stoi[\"state\"]\n",
        "                      )"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------+-------+\n",
            "| KEY                    | VALUE |\n",
            "+------------------------+-------+\n",
            "| PREDICTED CLASS 1      | 4     |\n",
            "| ACTUAL CLASS 1         | 4     |\n",
            "| PREDICTED CLASS 2      | 11    |\n",
            "| ACTUAL CLASS 2         | 19    |\n",
            "| PREDICTED CLASS NAME 1 | LOC   |\n",
            "| PREDICTED CLASS NAME 2 | city  |\n",
            "+------------------------+-------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Sa3_lq4VHSn"
      },
      "source": [
        "### Conclusion \n",
        "\n",
        "Congrats you have made it to the end of this amaizing simple series see you next time!!!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgz4_uzJwiim"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}